{
 "cells": [
  {
   "cell_type": "code",
   "id": "b3292da9556a2a68",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import math\n",
    "import os\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyabc\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from fitmulticell import model as morpheus_model\n",
    "from fitmulticell.sumstat import SummaryStatistics\n",
    "from tqdm import tqdm\n",
    "\n",
    "from summary_stats import reduced_coordinates_to_sumstat, reduce_to_coordinates, compute_mean_summary_stats, cut_region\n",
    "\n",
    "# get the job array id and number of processors\n",
    "job_array_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0))\n",
    "n_procs = int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",
    "print(job_array_id)\n",
    "on_cluster = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if on_cluster:\n",
    "    gp = '/home/jarruda_hpc/CellMigration/synth_data_params_bayesflow'\n",
    "else:\n",
    "    gp = os.getcwd()\n",
    "\n",
    "# defining the mapping of parameter inside the model xml file. the dictionary name is for \n",
    "# parameter name, and the value are the mapping values, to get the map value for parameter \n",
    "# check here: https://fitmulticell.readthedocs.io/en/latest/example/minimal.html#Inference-problem-definition\n",
    "\n",
    "par_map = {\n",
    "    'gradient_strength': './CellTypes/CellType/Constant[@symbol=\"gradient_strength\"]',\n",
    "    'move.strength': './CellTypes/CellType/Constant[@symbol=\"move.strength\"]',  # 0.21359842373064927*0.01\n",
    "    #'move.duration.mean': './CellTypes/CellType/Constant[@symbol=\"move.duration.mean\"]',\n",
    "    #'move.duration.median': './CellTypes/CellType/Constant[@symbol=\"move.duration.median\"]',\n",
    "    'move.duration.scale': './CellTypes/CellType/Constant[@symbol=\"move.duration.scale\"]',\n",
    "    'cell_nodes': './Global/Constant[@symbol=\"cell_nodes\"]',\n",
    "}\n",
    "\n",
    "model_path = gp + \"/cell_movement_v23.xml\"  # time step is 30sec, move.dir completely normalized, init move.dir rand in all directions\n",
    "# defining the summary statistics function\n",
    "max_sequence_length = 120\n",
    "min_sequence_length = 0\n",
    "only_longest_traj_per_cell = True  # mainly to keep the data batchable\n",
    "sumstat = SummaryStatistics(sum_stat_calculator=partial(reduce_to_coordinates,\n",
    "                                                        minimal_length=min_sequence_length,\n",
    "                                                        maximal_length=max_sequence_length,\n",
    "                                                        only_longest_traj_per_cell=only_longest_traj_per_cell))                    \n",
    "\n",
    "if on_cluster:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"lin\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "    # todo: remember also change tiff path in model.xml!\n",
    "else:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "\n",
    "# parameter values used to generate the synthetic data\n",
    "obs_pars = {\n",
    "    'gradient_strength': 500000.,  # strength of the gradient of chemotaxis  # todo: unit?\n",
    "    'move.strength': 1.,  # strength of directed motion  # todo: unit?\n",
    "    'move.duration.scale': 30.,  # median of exponential distribution (seconds)\n",
    "    'cell_nodes': 30.,  # volume of the cell   # todo: unit?\n",
    "}\n",
    "\n",
    "# define parameters' limits\n",
    "obs_pars_log = {key: math.log10(val) for key, val in obs_pars.items()}\n",
    "limits = {'gradient_strength': (math.log10(10 ** 4), math.log10(10 ** 12)), \n",
    "          'move.strength': (0, math.log10(10 ** 5)),\n",
    "          'move.duration.scale': (math.log10((10 ** -2) * 30), math.log10((10 ** 2) * 30)),\n",
    "          'cell_nodes': (math.log10(10 ** 0), math.log10(10 ** 2))}\n",
    "\n",
    "prior = pyabc.Distribution(**{key: pyabc.RV(\"uniform\", lb, ub - lb)\n",
    "                              for key, (lb, ub) in limits.items()})\n",
    "param_names = list(obs_pars.keys())\n",
    "print(obs_pars)"
   ],
   "id": "74390dfb0e802a77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import bayesflow as bf\n",
    "from bayesflow.amortizers import AmortizedPosterior\n",
    "from bayesflow.networks import InvertibleNetwork\n",
    "from bayesflow.helper_networks import MultiConv1D\n",
    "from bayesflow.simulation import GenerativeModel, Prior, Simulator\n",
    "from bayesflow.trainers import Trainer\n",
    "from bayesflow import default_settings as defaults\n",
    "from bayesflow.computational_utilities import maximum_mean_discrepancy\n",
    "from tensorflow.keras.layers import Dense, GRU, LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "\n",
    "def prior_fun(batch_size: int) -> np.ndarray:\n",
    "    samples = []\n",
    "    for _ in range(batch_size):\n",
    "        samples.append(list(prior.rvs().values()))\n",
    "    return np.array(samples)\n",
    "\n",
    "\n",
    "def generate_population_data(param_batch: np.ndarray, cells_in_population: int, max_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate population data\n",
    "    :param param_batch:  batch of parameters\n",
    "    :param cells_in_population:  number of cells in a population (50)\n",
    "    :param max_length:  maximum length of the sequence\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_batch = []\n",
    "    for params in param_batch:\n",
    "        params_dict = {key: p for key, p in zip(obs_pars.keys(), params)}\n",
    "        sim = model.sample(params_dict)\n",
    "        data_batch.append(sim)  # generates a cell population in one experiment\n",
    "\n",
    "    data_batch_transformed = np.ones((param_batch.shape[0], cells_in_population, max_length, 2)) * np.nan\n",
    "    # each cell is of different length, each with x and y coordinates, make a tensor out of it\n",
    "    n_cells_not_visible = 0\n",
    "    for p_id, population_sim in enumerate(data_batch):\n",
    "        if len(population_sim) == 0:\n",
    "            # no cells were visible in the simulation\n",
    "            n_cells_not_visible += 1\n",
    "            continue\n",
    "        for c_id, cell_sim in enumerate(population_sim):\n",
    "            # pre-pad the data with zeros, but first write zeros as nans to compute the mean and std\n",
    "            data_batch_transformed[p_id, c_id, -len(cell_sim['x']):, 0] = cell_sim['x']\n",
    "            data_batch_transformed[p_id, c_id, -len(cell_sim['y']):, 1] = cell_sim['y']\n",
    "\n",
    "    if n_cells_not_visible > 0:\n",
    "        print(f'Simulation with no cells visible: {n_cells_not_visible}/{len(data_batch)}')\n",
    "    return data_batch_transformed"
   ],
   "id": "c606f75e3a4c0257",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "presimulate = False\n",
    "presimulation_path = 'presimulations'\n",
    "n_val_data = 100\n",
    "cells_in_population = 50\n",
    "n_params = len(obs_pars)\n",
    "batch_size = 32\n",
    "iterations_per_epoch = 100\n",
    "# 4000 batches to be generated, 40 epochs until the batch is used again\n",
    "epochs = 500\n",
    "\n",
    "# check if gpu is available\n",
    "print('gpu:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "bayesflow_prior = Prior(batch_prior_fun=prior_fun, param_names=param_names)\n",
    "bayes_simulator = Simulator(batch_simulator_fun=partial(generate_population_data,\n",
    "                                                        cells_in_population=cells_in_population,\n",
    "                                                        max_length=max_sequence_length))\n",
    "generative_model = GenerativeModel(prior=bayesflow_prior, simulator=bayes_simulator,\n",
    "                                   skip_test=True,  # once is enough, simulation takes time\n",
    "                                   name=\"Normalizing Flow Generative Model\")"
   ],
   "id": "cecd0d3e2713e759",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if presimulate:\n",
    "    print('presimulating')\n",
    "    from time import sleep\n",
    "    sleep(job_array_id)\n",
    "    \n",
    "    # we create on batch per job and save it in a folder\n",
    "    epoch_id = job_array_id // iterations_per_epoch\n",
    "    generative_model.presimulate_and_save(batch_size=batch_size, \n",
    "                                          folder_path=presimulation_path+f'/epoch_{epoch_id}',\n",
    "                                          iterations_per_epoch=1,\n",
    "                                          epochs=1,\n",
    "                                          extend_from=job_array_id,\n",
    "                                          disable_user_input=True)\n",
    "    print('Done!')"
   ],
   "id": "958a416f9ac30668",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def custom_loader(file_path):\n",
    "    \"\"\"Uses pickle to load, but each path is folder with multiple files, each one batch\"\"\"\n",
    "    # load all files in folder\n",
    "    loaded_presimulations = []\n",
    "    for file in os.listdir(file_path):\n",
    "        with open(os.path.join(file_path, file), 'rb') as f:\n",
    "            test = pickle.load(f)[0]\n",
    "            assert isinstance(test, dict)\n",
    "            loaded_presimulations.append(test)\n",
    "    # shuffle list, so iterations are random, only batches stay the same\n",
    "    np.random.shuffle(loaded_presimulations)\n",
    "    return loaded_presimulations"
   ],
   "id": "52a1c6ecd20c7991",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.exists(os.path.join(gp, 'validation_data.pickle')):\n",
    "    with open(os.path.join(gp, 'validation_data.pickle'), 'rb') as f:\n",
    "        valid_data = pickle.load(f)\n",
    "else:\n",
    "    print('Generating validation data')\n",
    "    valid_data = generative_model(n_val_data)\n",
    "    # save the data\n",
    "    with open(os.path.join(gp, 'validation_data.pickle'), 'wb') as f:\n",
    "        pickle.dump(valid_data, f)\n",
    "\n",
    "x_mean = np.nanmean(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "x_std = np.nanstd(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "p_mean = np.mean(valid_data['prior_draws'], axis=0)\n",
    "p_std = np.std(valid_data['prior_draws'], axis=0)\n",
    "print('Mean and std of data:', x_mean, x_std)\n",
    "print('Mean and std of parameters:', p_mean, p_std)\n",
    "\n",
    "\n",
    "# compute the mean of the summary statistics\n",
    "summary_stats_list_ = [reduced_coordinates_to_sumstat(t) for t in valid_data['sim_data']]\n",
    "(_, _, _, _, _, ad_averg, MSD_averg, \n",
    " TA_averg, VEL_averg, WT_averg) = compute_mean_summary_stats(summary_stats_list_, remove_nan=False)\n",
    "direct_conditions_ = np.stack([ad_averg, MSD_averg, TA_averg, VEL_averg, WT_averg]).T\n",
    "# replace inf with -1\n",
    "direct_conditions_[np.isinf(direct_conditions_)] = np.nan\n",
    "        \n",
    "summary_valid_max = np.nanmax(direct_conditions_, axis=0)\n",
    "summary_valid_min = np.nanmin(direct_conditions_, axis=0)"
   ],
   "id": "df1829ce49462436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def configurator(forward_dict: dict, remove_nans: bool = False, manual_summary: bool = False) -> dict:\n",
    "    out_dict = {}\n",
    "\n",
    "    # Extract data\n",
    "    x = forward_dict[\"sim_data\"]\n",
    "    \n",
    "    if remove_nans:\n",
    "        # check if simulation with only nan values in a row\n",
    "        non_nan_populations = np.isnan(x).sum(axis=(1,2,3))-np.prod(x.shape[1:]) != 0\n",
    "        #print(x.shape[0]-non_nan_populations.sum(), 'samples with only nan values in a row')\n",
    "        x = x[non_nan_populations]\n",
    "    \n",
    "    # compute manual summary statistics\n",
    "    if manual_summary:\n",
    "        summary_stats_list = [reduced_coordinates_to_sumstat(t) for t in x]\n",
    "        # compute the mean of the summary statistics\n",
    "        (_, _, _, _, _, \n",
    "         ad_averg, MSD_averg, \n",
    "         TA_averg, VEL_averg, WT_averg) = compute_mean_summary_stats(summary_stats_list, remove_nan=False)\n",
    "        direct_conditions = np.stack([ad_averg, MSD_averg, TA_averg, VEL_averg, WT_averg]).T   \n",
    "        # normalize statistics\n",
    "        direct_conditions = (direct_conditions - summary_valid_min) / (summary_valid_max - summary_valid_min)\n",
    "        # replace nan or inf with -1\n",
    "        direct_conditions[np.isinf(direct_conditions)] = -1\n",
    "        direct_conditions[np.isnan(direct_conditions)] = -1\n",
    "        print(direct_conditions.shape, x.shape, ad_averg.shape, MSD_averg.shape, \n",
    "         TA_averg.shape, VEL_averg.shape, WT_averg.shape)\n",
    "        out_dict['direct_conditions'] = direct_conditions.astype(np.float32)\n",
    "    \n",
    "    # Normalize data\n",
    "    x = (x - x_mean) / x_std\n",
    "    \n",
    "    # Check for NaN values in the first entry of the last axis\n",
    "    # If nan_mask is False (no NaNs), set to 1; otherwise, set to 0\n",
    "    nan_mask = np.isnan(x[..., 0])\n",
    "    new_dim = np.where(nan_mask, 0, 1)\n",
    "    new_dim_expanded = np.expand_dims(new_dim, axis=-1)\n",
    "    x = np.concatenate((x, new_dim_expanded), axis=-1)\n",
    "\n",
    "    # Normalize data\n",
    "    x[np.isnan(x)] = 0  # replace nan with 0, pre-padding (since we have nans in the data at the end)\n",
    "    out_dict['summary_conditions'] = x.astype(np.float32)\n",
    "\n",
    "    # Extract params\n",
    "    if 'parameters' in forward_dict.keys():\n",
    "        forward_dict[\"prior_draws\"] = forward_dict[\"parameters\"]\n",
    "    if 'prior_draws' in forward_dict.keys():\n",
    "        params = forward_dict[\"prior_draws\"]\n",
    "        if remove_nans:\n",
    "            params = params[non_nan_populations]\n",
    "        params = (params - p_mean) / p_std\n",
    "        out_dict['parameters'] = params.astype(np.float32)\n",
    "    return out_dict"
   ],
   "id": "330709997cc7e561",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# define the network\n",
    "class GroupSummaryNetwork(tf.keras.Model):\n",
    "    \"\"\"Network to summarize the data of groups of cells.  Each group is passed through a series of convolutional layers\n",
    "    followed by an LSTM layer. The output of the LSTM layer is then pooled across the groups and dense layer applied\n",
    "    to obtain a summary of fixed dimensionality. The network is invariant to the order of the groups.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            summary_dim, \n",
    "            num_conv_layers=2, \n",
    "            rnn_units=128, \n",
    "            use_lstm=True, \n",
    "            bidirectional=False,\n",
    "            conv_settings=None, \n",
    "            use_attention=False,\n",
    "            **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        if conv_settings is None:\n",
    "            conv_settings = defaults.DEFAULT_SETTING_MULTI_CONV\n",
    "\n",
    "        self.conv = Sequential([MultiConv1D(conv_settings) for _ in range(num_conv_layers)])\n",
    "        self.use_attention = use_attention\n",
    "        \n",
    "        if use_lstm:\n",
    "            self.rnn = Bidirectional(LSTM(rnn_units, return_sequences=use_attention)) if bidirectional else LSTM(rnn_units, return_sequences=use_attention)\n",
    "        else:\n",
    "            self.rnn = GRU(LSTM(rnn_units, return_sequences=use_attention)) if bidirectional else LSTM(rnn_units, return_sequences=use_attention)\n",
    "\n",
    "        if self.use_attention:\n",
    "            self.attention = tf.keras.layers.Attention()\n",
    "        self.out_layer = Dense(summary_dim, activation=\"linear\")\n",
    "        self.summary_dim = summary_dim\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        \"\"\"Performs a forward pass through the network by first passing `x` through the same rnn network for\n",
    "        each household and then pooling the outputs across households.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : tf.Tensor\n",
    "            Input of shape (batch_size, n_groups, n_time_steps, n_features)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        out : tf.Tensor\n",
    "            Output of shape (batch_size, summary_dim)\n",
    "        \"\"\"\n",
    "        # iterate over groups\n",
    "        out_list = []  # list to store outputs of LSTM for each group\n",
    "        for g_i in range(x.shape[1]):\n",
    "            out = self.conv(x[:, g_i], **kwargs)  # (batch_size, n_time_steps, n_filters) -> default: filters=32\n",
    "            out = self.rnn(out, **kwargs)  # (batch_size, lstm_units) \n",
    "            # if attention is used, return full sequence (batch_size, n_time_steps, lstm_units)\n",
    "            # bidirectional LSTM returns 2*lstm_units\n",
    "            out_list.append(out)\n",
    "        if self.use_attention:\n",
    "            # learn a query vector to attend over the groups, some groups might be more important\n",
    "            # this should be invariant to the order of the groups (depends on the learned attention mechanism)\n",
    "            out = tf.stack(out_list, axis=1)  # (batch_size, n_groups, n_time_steps, lstm_units)\n",
    "            query = tf.reduce_mean(out, axis=2)  # (batch_size, n_groups, lstm_units)\n",
    "            # Reshape query to match the required shape for attention\n",
    "            query = tf.expand_dims(query, axis=2)  # (batch_size, n_groups, 1, lstm_units)\n",
    "            out = self.attention([query, out], **kwargs)  # (batch_size, n_groups, 1, lstm_units)\n",
    "            out = tf.reduce_max(out, axis=1)  # (batch_size, 1, lstm_units)\n",
    "            out = tf.squeeze(out, axis=1)  # Remove the extra dimension (batch_size, lstm_units)\n",
    "        else:\n",
    "            # max pooling over groups, this totally invariants to the order of the groups\n",
    "            out = tf.reduce_max(out_list, axis=0)  # (batch_size, lstm_units)\n",
    "        # apply dense layer\n",
    "        out = self.out_layer(out, **kwargs)  # (batch_size, summary_dim)\n",
    "        return out\n",
    "\n",
    "job_array_id = 6\n",
    "\n",
    "num_coupling_layers = 6\n",
    "num_dense = 3\n",
    "use_attention = False\n",
    "use_bidirectional = False\n",
    "summary_loss = None\n",
    "use_manual_summary = False\n",
    "if job_array_id == 0:\n",
    "    checkpoint_path = 'amortizer-cell-migration-conv-6'\n",
    "    map_idx_sim = 21\n",
    "elif job_array_id == 1:\n",
    "    checkpoint_path = 'amortizer-cell-migration-attention-6-bid'\n",
    "    use_attention = True\n",
    "    use_bidirectional = True\n",
    "    map_idx_sim = 77\n",
    "elif job_array_id == 2:\n",
    "    checkpoint_path = 'amortizer-cell-migration-conv-7'\n",
    "    num_coupling_layers = 7\n",
    "    map_idx_sim = 51\n",
    "elif job_array_id == 3:\n",
    "    checkpoint_path = 'amortizer-cell-migration-attention-7'\n",
    "    num_coupling_layers = 7\n",
    "    use_attention = True\n",
    "    map_idx_sim = 64\n",
    "elif job_array_id == 4:\n",
    "    checkpoint_path = 'amortizer-cell-migration-attention-7-bid'\n",
    "    num_coupling_layers = 7\n",
    "    use_attention = True\n",
    "    use_bidirectional = True\n",
    "    map_idx_sim = 38\n",
    "elif job_array_id == 5:\n",
    "    checkpoint_path = 'amortizer-cell-migration-attention-7-bid-MMD'\n",
    "    num_coupling_layers = 7\n",
    "    use_attention = True\n",
    "    use_bidirectional = True\n",
    "    summary_loss = 'MMD'\n",
    "    map_idx_sim = 77\n",
    "elif job_array_id == 6:\n",
    "    checkpoint_path = 'amortizer-cell-migration-attention-7-bid-MMD-manual'\n",
    "    num_coupling_layers = 7\n",
    "    use_attention = True\n",
    "    use_bidirectional = True\n",
    "    summary_loss = 'MMD'\n",
    "    map_idx_sim = 57\n",
    "    use_manual_summary = True\n",
    "# elif checkpoint_path == 'amortizer-cell-migration-conv-7-nan':\n",
    "#     num_coupling_layers = 7\n",
    "#     num_dense = 3\n",
    "#     config_remove_nans = True\n",
    "#     map_idx_sim = 98\n",
    "# elif checkpoint_path == 'amortizer-cell-migration-attention-7-nan':\n",
    "#     num_coupling_layers = 7\n",
    "#     num_dense = 3\n",
    "#     use_attention = True\n",
    "#     config_remove_nans = True\n",
    "#     map_idx_sim = 37\n",
    "# elif checkpoint_path == 'amortizer-cell-migration-attention-7-bid-nan':\n",
    "#     num_coupling_layers = 7\n",
    "#     num_dense = 3\n",
    "#     use_attention = True\n",
    "#     use_bidirectional = True\n",
    "#     config_remove_nans = True\n",
    "#     map_idx_sim = 19\n",
    "else:\n",
    "    raise ValueError('Checkpoint path not found')\n",
    "os.makedirs(f\"../results/{checkpoint_path}\", exist_ok=True)\n",
    "print(checkpoint_path)\n",
    "\n",
    "summary_net = GroupSummaryNetwork(summary_dim=n_params * 2,\n",
    "                                  rnn_units=2 ** int(np.ceil(np.log2(max_sequence_length))),\n",
    "                                  use_attention=use_attention,\n",
    "                                  bidirectional=use_bidirectional)\n",
    "inference_net = InvertibleNetwork(num_params=n_params,\n",
    "                                  num_coupling_layers=num_coupling_layers,\n",
    "                                  coupling_design='spline',\n",
    "                                  coupling_settings={\n",
    "                                      \"num_dense\": num_dense,\n",
    "                                      \"dense_args\": dict(\n",
    "                                          activation='relu',\n",
    "                                          kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                      ),\n",
    "                                      \"dropout_prob\": 0.2,\n",
    "                                      \"bins\": 16,\n",
    "                                  })\n",
    "\n",
    "amortizer = AmortizedPosterior(inference_net=inference_net, summary_net=summary_net,\n",
    "                               summary_loss_fun=summary_loss)"
   ],
   "id": "a7c0835963566e42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# build the trainer with networks and generative model\n",
    "max_to_keep = 17\n",
    "trainer = Trainer(amortizer=amortizer,\n",
    "                  configurator=partial(configurator, \n",
    "                                       manual_summary=use_manual_summary),\n",
    "                  generative_model=generative_model,\n",
    "                  checkpoint_path=checkpoint_path,\n",
    "                  skip_checks=True,  # once is enough, simulation takes time\n",
    "                  max_to_keep=max_to_keep)\n",
    "\n",
    "# check if file exist\n",
    "if os.path.exists(checkpoint_path):\n",
    "    trainer.load_pretrained_network()\n",
    "    history = trainer.loss_history.get_plottable()\n",
    "else:\n",
    "    trainer._setup_optimizer(optimizer=None,\n",
    "                         epochs=epochs,\n",
    "                         iterations_per_epoch=iterations_per_epoch)\n",
    "    \n",
    "    history = trainer.train_from_presimulation(presimulation_path=presimulation_path,\n",
    "                                               optimizer=trainer.optimizer,\n",
    "                                               max_epochs=epochs,\n",
    "                                               early_stopping=True,\n",
    "                                               early_stopping_args={'patience': max_to_keep-2},\n",
    "                                               custom_loader=custom_loader,\n",
    "                                               validation_sims=valid_data)\n",
    "    print('Training done!')"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bf.diagnostics.plot_losses(history['train_losses'], history['val_losses'], moving_average=True, fig_size=(10, 6));",
   "id": "aa6d79b3faa0a57a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check if training converged\n",
    "if np.isnan(history['val_losses'].iloc[-1]).any():\n",
    "    print('Training failed with NaN loss at the end')\n",
    "    if np.isnan(history['val_losses'].iloc[-max_to_keep:]).all():\n",
    "        print('Training failed with NaN loss for all latest checkpoints')\n",
    "\n",
    "# Find the checkpoint with the lowest validation loss out of the last max_to_keep\n",
    "recent_losses = history['val_losses'].iloc[-max_to_keep:]\n",
    "best_valid_epoch = recent_losses['Loss'].idxmin() + 1  # checkpoints are 1-based indexed\n",
    "new_checkpoint = trainer.manager.latest_checkpoint.rsplit('-', 1)[0] + f'-{best_valid_epoch}'    \n",
    "trainer.checkpoint.restore(new_checkpoint)\n",
    "print(\"Networks loaded from {}\".format(new_checkpoint))"
   ],
   "id": "eefc6362c875b3d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Diagnostic plots",
   "id": "c72ff040811c2643"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from tarp import get_tarp_coverage\n",
    "from matplotlib import pyplot as plt"
   ],
   "id": "2ffb89761c2c5bcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "valid_data_config = trainer.configurator(valid_data)",
   "id": "e6a730d966e83d26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_samples = amortizer.sample(valid_data_config, n_samples=100)\n",
    "posterior_samples = posterior_samples * p_std + p_mean\n",
    "prior_draws = valid_data_config[\"parameters\"] * p_std + p_mean"
   ],
   "id": "f6ad80516d7191c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bf.diagnostics.plot_sbc_histograms(posterior_samples, prior_draws, num_bins=10, param_names=param_names);",
   "id": "597be28509b8855e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.plot_sbc_ecdf(posterior_samples, prior_draws, difference=True, param_names=param_names)\n",
    "plt.savefig(f'../results/{checkpoint_path}/sbc_ecdf.png')"
   ],
   "id": "bf28748d802ce799",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_samples = amortizer.sample(valid_data_config, n_samples=1000)\n",
    "posterior_samples = posterior_samples * p_std + p_mean"
   ],
   "id": "9672ea6487623555",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sample_id = 0\n",
    "fig = bf.diagnostics.plot_posterior_2d(posterior_draws=posterior_samples[sample_id], prior_draws=prior_draws[:posterior_samples.shape[1]],\n",
    "                                 param_names=param_names)\n",
    "# from figure get axis\n",
    "ax = fig.get_axes()\n",
    "for i, a in enumerate(ax):\n",
    "    # plot only on the diagonal\n",
    "    if i == i // len(param_names) * (len(param_names)+1):\n",
    "        a.axvline(prior_draws[sample_id][i // len(param_names)], color='b', label='True parameter')\n",
    "plt.show()"
   ],
   "id": "290c267aeceb50e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.plot_recovery(posterior_samples, prior_draws, param_names=param_names)\n",
    "plt.savefig(f'../results/{checkpoint_path}/recovery.png')"
   ],
   "id": "a8a443ecb123ad2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bf.diagnostics.plot_z_score_contraction(posterior_samples, prior_draws, param_names=param_names);",
   "id": "69522e989ab5747d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "posterior_samples_reshaped = posterior_samples.reshape(posterior_samples.shape[1], posterior_samples.shape[0], posterior_samples.shape[2])",
   "id": "5480764514697d11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ecp_bootstrap, alpha = get_tarp_coverage(posterior_samples_reshaped, prior_draws, references='random', #posterior_samples_reshaped[0, :, :]\n",
    "                                         metric='euclidean', norm=True, bootstrap=True)"
   ],
   "id": "134fd5229cfe0037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "k_sigma = [1, 2, 3]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "ax.plot([0, 1], [0, 1], ls='--', color='k', label=\"Ideal case\")\n",
    "ax.plot(alpha, ecp_bootstrap.mean(axis=0), label='TARP')\n",
    "for k in k_sigma:\n",
    "    ax.fill_between(alpha, \n",
    "                    ecp_bootstrap.mean(axis=0) - k * ecp_bootstrap.std(axis=0), \n",
    "                    ecp_bootstrap.mean(axis=0) + k * ecp_bootstrap.std(axis=0), \n",
    "                    alpha = 0.7)\n",
    "ax.legend()\n",
    "ax.set_ylabel(\"Expected Coverage\")\n",
    "ax.set_xlabel(\"Credibility Level\")\n",
    "plt.show()"
   ],
   "id": "a8c1d2d8a1cc04e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test on synthetic data",
   "id": "de8f91657ceebed0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# simulate test data\n",
    "test_params = np.log10(list(obs_pars.values()))\n",
    "if not os.path.exists('test_sim.npy'):\n",
    "    test_sim_full = bayes_simulator(test_params[np.newaxis])\n",
    "    test_sim = test_sim_full['sim_data']\n",
    "    np.save('test_sim.npy', test_sim)\n",
    "else:\n",
    "    test_sim = np.load('test_sim.npy')\n",
    "    test_sim_full = {'sim_data': test_sim}\n",
    "test_sim.shape"
   ],
   "id": "d18447b62e9c4750",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_posterior_samples = amortizer.sample(trainer.configurator(test_sim_full), n_samples=100)\n",
    "test_posterior_samples = test_posterior_samples * p_std + p_mean\n",
    "\n",
    "# compute the log posterior of the test data\n",
    "input_dict = {\n",
    "    'sim_data': np.repeat(test_sim, repeats=100, axis=0),\n",
    "    'parameters': test_posterior_samples\n",
    "}\n",
    "log_prob = amortizer.log_posterior(trainer.configurator(input_dict))\n",
    "    \n",
    "# get the MAP\n",
    "map_idx = np.argmax(log_prob)"
   ],
   "id": "731774aebbbce43c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.plot_posterior_2d(posterior_draws=test_posterior_samples,\n",
    "                                       prior_draws=prior_draws[:test_posterior_samples.shape[0]],\n",
    "                                       param_names=param_names)\n",
    "# from figure get axis\n",
    "ax = fig.get_axes()\n",
    "for i, a in enumerate(ax):\n",
    "    # plot only on the diagonal\n",
    "    if i == i // len(param_names) * (len(param_names)+1):\n",
    "        a.axvline(test_params[i // len(param_names)], color='b', label='True parameter')\n",
    "plt.savefig(f'../results/{checkpoint_path}/posterior_vs_prior.png')\n",
    "plt.show()"
   ],
   "id": "dc88096666755040",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get posterior samples and simulate\n",
    "if not os.path.exists(checkpoint_path+'/posterior_sim.npy'):\n",
    "    # simulate the data\n",
    "    posterior_sim = bayes_simulator(test_posterior_samples)['sim_data']\n",
    "    np.save(checkpoint_path+'/posterior_sim.npy', posterior_sim)\n",
    "    \n",
    "    print('map_sim', map_idx, log_prob[map_idx], test_posterior_samples[map_idx])\n",
    "else:\n",
    "    posterior_sim = np.load(checkpoint_path+'/posterior_sim.npy')\n",
    "    map_sim = posterior_sim[map_idx_sim]"
   ],
   "id": "ab4d1e0a7e2402",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compute the summary statistics\n",
    "synthetic_summary_stats_list = [reduced_coordinates_to_sumstat(t) for t in test_sim]  # should be only one population\n",
    "simulation_synth_summary_stats_list = [reduced_coordinates_to_sumstat(pop_sim) for pop_sim in posterior_sim]\n",
    "\n",
    "# compute the mean of the summary statistics\n",
    "ad_mean_synth, MSD_mean_synth, TA_mean_synth, VEL_mean_synth, WT_mean_synth, ad_mean_synth_averg, MSD_mean_synth_averg, TA_mean_synth_averg, VEL_mean_synth_averg, WT_mean_synth_averg = compute_mean_summary_stats(synthetic_summary_stats_list)\n",
    "ad_mean_synth_sim, MSD_mean_synth_sim, TA_mean_synth_sim, VEL_mean_synth_sim, WT_mean_synth_sim, ad_mean_synth_sim_averg, MSD_mean_synth_sim_averg, TA_mean_synth_sim_averg, VEL_mean_synth_sim_averg, WT_mean_synth_sim_averg = compute_mean_summary_stats(simulation_synth_summary_stats_list)"
   ],
   "id": "6cd5eb40ca2b15bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the summary statistics\n",
    "fig, ax = plt.subplots(nrows=2, ncols=5, sharex=True, sharey='col', tight_layout=True, figsize=(12, 5))\n",
    "# Perform the Kolmogorov-Smirnov test\n",
    "ks_statistic, p_value = stats.ks_2samp(ad_mean_synth_sim[map_idx_sim], ad_mean_synth[0])\n",
    "print(f\"Angle Degree KS Statistic: {ks_statistic}\")\n",
    "print(f\"Angle Degree P-value: {p_value}, {p_value < 0.05}: different distributions\")\n",
    "ax[0, 0].violinplot([ad_mean_synth_sim[map_idx_sim], ad_mean_synth[0]], showmeans=True)\n",
    "if p_value < 0.05:\n",
    "    ax[0, 0].set_title(f'Angle Degree\\n(Different)')\n",
    "else:\n",
    "    ax[0, 0].set_title(f'Angle Degree\\n(Same)')\n",
    "ax[0, 0].set_ylabel(f'Angle Degree (degrees)\\nMean per Cell')\n",
    "ax[1, 0].violinplot([ad_mean_synth_sim_averg, ad_mean_synth_averg], showmeans=True)\n",
    "ax[1, 0].set_ylabel(f'Angle Degree (degrees)\\nPopulation Mean')\n",
    "ax[1, 0].set_xticks([1, 2], ['Simulation', 'Synthetic'])\n",
    "\n",
    "ks_statistic, p_value = stats.ks_2samp(MSD_mean_synth_sim_averg[map_idx_sim], MSD_mean_synth[0])\n",
    "print(f\"MSD KS Statistic: {ks_statistic}\")\n",
    "print(f\"MSD P-value: {p_value}, {p_value < 0.05}: different distributions\")\n",
    "ax[0, 1].violinplot([MSD_mean_synth_sim[map_idx_sim], MSD_mean_synth[0]], showmeans=True)\n",
    "if p_value < 0.05:\n",
    "    ax[0, 1].set_title(f'Mean Squared Displacement\\n(Different)')\n",
    "else:\n",
    "    ax[0, 1].set_title(f'Mean Squared Displacement\\n(Same)')\n",
    "ax[0, 1].set_ylabel(f'MSD\\nMean per Cell')\n",
    "ax[1, 1].violinplot([MSD_mean_synth_sim_averg, MSD_mean_synth_averg], showmeans=True)\n",
    "ax[1, 1].set_ylabel(f'MSD\\nPopulation Mean')\n",
    "ax[1, 1].set_xticks([1, 2], ['Simulation', 'Synthetic'])\n",
    "\n",
    "ks_statistic, p_value = stats.ks_2samp(TA_mean_synth_sim[map_idx_sim], TA_mean_synth[0])\n",
    "print(f\"Turning Angle KS Statistic: {ks_statistic}\")\n",
    "print(f\"Turning Angle P-value: {p_value}, {p_value < 0.05}: different distributions\")\n",
    "ax[0, 2].violinplot([TA_mean_synth_sim[map_idx_sim], TA_mean_synth[0]], showmeans=True)\n",
    "if p_value < 0.05:\n",
    "    ax[0, 2].set_title(f'Turning Angle\\n(Different)')\n",
    "else:\n",
    "    ax[0, 2].set_title(f'Turning Angle\\n(Same)')\n",
    "ax[0, 2].set_ylabel(f'Turning Angle (radians)\\nMean per Cell')\n",
    "ax[1, 2].violinplot([TA_mean_synth_sim_averg, TA_mean_synth_averg], showmeans=True)\n",
    "ax[1, 2].set_ylabel(f'Turning Angle (radians)\\nPopulation Mean')\n",
    "ax[1, 2].set_xticks([1, 2], ['Simulation', 'Synthetic'])\n",
    "\n",
    "ks_statistic, p_value = stats.ks_2samp(VEL_mean_synth_sim[map_idx_sim], VEL_mean_synth[0])\n",
    "print(f\"Velocity KS Statistic: {ks_statistic}\")\n",
    "print(f\"Velocity P-value: {p_value}, {p_value < 0.05}: different distributions\")\n",
    "ax[0, 3].violinplot([VEL_mean_synth_sim[map_idx_sim], VEL_mean_synth[0]], showmeans=True)\n",
    "if p_value < 0.05:\n",
    "    ax[0, 3].set_title(f'Velocity\\n(Different)')\n",
    "else:\n",
    "    ax[0, 3].set_title(f'Velocity\\n(Same)')\n",
    "ax[0, 3].set_ylabel(f'Velocity\\nMean per Cell')\n",
    "ax[1, 3].violinplot([VEL_mean_synth_sim_averg, VEL_mean_synth_averg], showmeans=True)\n",
    "ax[1, 3].set_ylabel(f'Velocity\\nPopulation Mean')\n",
    "ax[1, 3].set_xticks([1, 2], ['Simulation', 'Synthetic'])\n",
    "\n",
    "ks_statistic, p_value = stats.ks_2samp(WT_mean_synth_sim[map_idx_sim], WT_mean_synth[0])\n",
    "print(f\"Waiting Time KS Statistic: {ks_statistic}\")\n",
    "print(f\"Waiting Time P-value: {p_value}, {p_value < 0.05}: different distributions\")\n",
    "ax[0, 4].violinplot([WT_mean_synth_sim[map_idx_sim], WT_mean_synth[0]], showmeans=True)\n",
    "if p_value < 0.05:\n",
    "    ax[0, 4].set_title(f'Waiting Time\\n(Different)')\n",
    "else:\n",
    "    ax[0, 4].set_title(f'Waiting Time\\n(Same)')\n",
    "ax[0, 4].set_ylabel(f'Waiting Time (sec)\\nMean per Cell')\n",
    "ax[1, 4].violinplot([WT_mean_synth_sim_averg, WT_mean_synth_averg], showmeans=True)\n",
    "ax[1, 4].set_ylabel(f'Waiting Time (sec)\\nPopulation Mean')\n",
    "ax[1, 4].set_xticks([1, 2], ['Simulation', 'Synthetic'])\n",
    "plt.savefig(f'../results/{checkpoint_path}/Summary Stats.png')\n",
    "plt.show()\n",
    "\n",
    "# Wasserstein distance\n",
    "wasserstein_distance = stats.wasserstein_distance(ad_mean_synth_sim[map_idx_sim], ad_mean_synth[0])\n",
    "wasserstein_distance += stats.wasserstein_distance(MSD_mean_synth_sim[map_idx_sim], MSD_mean_synth[0])\n",
    "wasserstein_distance += stats.wasserstein_distance(TA_mean_synth_sim[map_idx_sim], TA_mean_synth[0])\n",
    "wasserstein_distance += stats.wasserstein_distance(VEL_mean_synth_sim[map_idx_sim], VEL_mean_synth[0])\n",
    "wasserstein_distance += stats.wasserstein_distance(WT_mean_synth_sim[map_idx_sim], WT_mean_synth[0])\n",
    "print(f\"Wasserstein distance: {wasserstein_distance}\")"
   ],
   "id": "c24f7e652f32d23b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the simulations or the MAP\n",
    "plt.plot(map_sim[0, :, 0], map_sim[0, :, 1], 'b', label='MAP Simulated Trajectories', alpha=1)\n",
    "for cell_id in range(1, cells_in_population):           \n",
    "    plt.plot(map_sim[cell_id, :, 0], map_sim[cell_id, :, 1], 'b')\n",
    "    \n",
    "# plot the synthetic data\n",
    "plt.plot(test_sim[0][0, :, 0], test_sim[0][0, :, 1], 'r', label='Synthetic Trajectories', alpha=1)\n",
    "for cell_id in range(1, cells_in_population):           \n",
    "    plt.plot(test_sim[0][cell_id, :, 0], test_sim[0][cell_id, :, 1], 'r')\n",
    "    \n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.savefig(f'../results/{checkpoint_path}/Simulations.png')\n",
    "plt.show()"
   ],
   "id": "8f78c2c056e6a37a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if summary_loss is not None:\n",
    "    test_data_config = trainer.configurator(test_sim_full)\n",
    "    summary_statistics = trainer.amortizer.summary_net(valid_data_config[\"summary_conditions\"])\n",
    "    summary_statistics_obs = trainer.amortizer.summary_net(test_data_config[\"summary_conditions\"])\n",
    "    mmd = np.sqrt(maximum_mean_discrepancy(summary_statistics, summary_statistics_obs))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    colors = cm.viridis(np.linspace(0.1, 0.9, 2))\n",
    "    ax.scatter(\n",
    "        summary_statistics_obs[:, 0], summary_statistics_obs[:, 1], \n",
    "        color=colors[0], label=r\"Observed: $h_{\\psi}(x_{obs})$\" + f\", MMD: {mmd:.3f}\"\n",
    "    )\n",
    "    ax.scatter(summary_statistics[:, 0], summary_statistics[:, 1], \n",
    "               color=colors[1], label=r\"Well-specified: $h_{\\psi}(x)$\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.2)\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "    sns.despine(ax=ax)\n",
    "    fig.savefig(f'../results/{checkpoint_path}/Synthetic Summary Latent Space.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    MMD_sampling_distribution, MMD_observed = trainer.mmd_hypothesis_test(\n",
    "        observed_data=test_data_config, \n",
    "        reference_data=valid_data_config,  # if not provided, will use the generative model\n",
    "        num_null_samples=500,\n",
    "        bootstrap=True  # if True, use the reference data as null samples\n",
    "    )\n",
    "    fig = bf.diagnostics.plot_mmd_hypothesis_test(MMD_sampling_distribution, MMD_observed)\n",
    "    fig.savefig(f'../results/{checkpoint_path}/Synthetic MMD.png', bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "id": "420c2b14ccdeee35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Parameter Identifiability",
   "id": "28887ab9c5c5dc9f"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# multiprocess the simulations\n",
    "def simulate_data(params):\n",
    "    return bayes_simulator(np.log10(params[np.newaxis]))['sim_data']"
   ],
   "id": "3518fab6c3643598"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "test_params_1 = np.array(list(obs_pars.values())).copy()\n",
    "test_params_1[1] = 1e-1\n",
    "\n",
    "test_params_2 = np.array(list(obs_pars.values())).copy()\n",
    "test_params_2[1] = 1e-1\n",
    "\n",
    "test_params_3 = np.array(list(obs_pars.values())).copy()\n",
    "test_params_3[1] = 1\n",
    "\n",
    "test_params_4 = np.array(list(obs_pars.values())).copy()\n",
    "test_params_4[1] = 1"
   ],
   "id": "e47295272142037a"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "test_sim_1 = simulate_data(test_params_1)\n",
    "test_sim_2 = simulate_data(test_params_2)\n",
    "#test_sim_3 = simulate_data(test_params_3)\n",
    "#test_sim_4 = simulate_data(test_params_4)\n",
    "\n",
    "np.abs(test_sim_1[0] - test_sim_2[0]).sum()#, np.abs(test_sim_3[0] - test_sim_4[0]).sum()"
   ],
   "id": "da26e46db26c1c12"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "fig, ax = plt.subplots(2, 2, sharex=True, sharey=True, figsize=(12, 5))\n",
    "test_sims = [test_sim_1[0], test_sim_2[0], test_sim_3[0], test_sim_4[0]]\n",
    "\n",
    "for i, test_sim in enumerate(test_sims):\n",
    "    ax[i//2, i%2].plot(test_sim[0, :, 0], test_sim[0, :, 1], 'r', label='Test Population', alpha=1)\n",
    "    for cell_id in range(1, test_sim.shape[0]):\n",
    "        ax[i//2, i%2].plot(test_sim[cell_id, :, 0], test_sim[cell_id, :, 1], 'r', alpha=1)\n",
    "\n",
    "ax[1, 0].set_xlabel('x')\n",
    "ax[1, 1].set_xlabel('x')\n",
    "#ax[0, 0].set_ylabel(f'mean waiting time {test_params_1[1]}')\n",
    "#ax[0, 1].set_ylabel(f'mean waiting time {test_params_2[1]}')\n",
    "#ax[1, 0].set_ylabel(f'mean waiting time {test_params_3[1]}')\n",
    "#ax[1, 1].set_ylabel(f'mean waiting time {test_params_4[1]}')\n",
    "ax[0, 0].set_ylabel(f'move strength {test_params_1[1]}')\n",
    "ax[0, 1].set_ylabel(f'move strength {test_params_2[1]}')\n",
    "ax[1, 0].set_ylabel(f'move strength {test_params_3[1]}')\n",
    "ax[1, 1].set_ylabel(f'move strength {test_params_4[1]}')\n",
    "plt.show()"
   ],
   "id": "905e9b7b92fdbb1c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Real Data",
   "id": "748dffc6b156ef8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "wasserstein_distance_dict = {0: np.nan, 1: np.nan}\n",
    "samples_dict = {0: np.nan, 1: np.nan}"
   ],
   "id": "fb88dab9c4baf068",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load real data\n",
    "real_data_id = [0, 1][0]\n",
    "if real_data_id == 0:\n",
    "    real_data_df = pd.read_csv('../real_data/37C-1_crop.csv')\n",
    "    y_size = 739.79  # microns\n",
    "    x_size = 279.74  # microns\n",
    "    x_offset, y_offset = -7.5, -2  # correction such that the pillars are in the middle\n",
    "    y_lin_shift = 0\n",
    "else:\n",
    "    # more important data set, closer to the truth\n",
    "    real_data_df = pd.read_csv('../real_data/37C_ctrl2.csv')\n",
    "    y_size = 882.94  # microns\n",
    "    x_size = 287.03  # microns\n",
    "    x_offset, y_offset = -2, -7.5 #3, 7.5  # correction such that the pillars are in the middle\n",
    "    y_lin_shift = -0.04 # 0.05  # correction for the tilt of the data\n",
    "    \n",
    "# define the window\n",
    "# simulation coordinates, length of the gap: 1351-1145 = 206\n",
    "# in real data: 270\n",
    "# 270/206 = 1.31\n",
    "# morpheus size of image: 1173x2500\n",
    "factor = 1.31\n",
    "# reconstruction of the window assuming that it is centered\n",
    "window_x1, window_x2 = (1173-y_size/factor)/2., 1173-(1173-y_size/factor)/2.\n",
    "window_y1, window_y2 = (2500-x_size/factor)/2., 2500-(2500-x_size/factor)/2.\n",
    "\n",
    "# remove first three rows\n",
    "real_data_df = real_data_df.iloc[3:]\n",
    "# only keep positions and time\n",
    "real_data_df = real_data_df[['TRACK_ID', 'POSITION_X', 'POSITION_Y', 'POSITION_T']]\n",
    "# convert to numeric\n",
    "real_data_df = real_data_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# scale to morpheus coordinates\n",
    "real_data_scaled_df = real_data_df.copy()\n",
    "real_data_scaled_df['x'] = real_data_scaled_df['POSITION_X'] / factor + window_x1\n",
    "real_data_scaled_df['y'] = real_data_scaled_df['POSITION_Y'] / factor + window_y1\n",
    "# real cells are moving downwards, but in simulations they are going upwards\n",
    "real_data_scaled_df['y'] = 2500 - real_data_scaled_df['y']\n",
    "# data does not fit optimally, so we need to shift it\n",
    "real_data_scaled_df['x'] += x_offset\n",
    "real_data_scaled_df['y'] += y_offset\n",
    "# data is tilted\n",
    "real_data_scaled_df['x'] = real_data_scaled_df['x'] + (window_y2-real_data_scaled_df['y']) * y_lin_shift\n",
    "# time\n",
    "real_data_scaled_df['t'] = real_data_scaled_df['POSITION_T']"
   ],
   "id": "fa8abce2ebdc9339",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "real_data = []\n",
    "cut_region_real_data = True\n",
    "sequence_lengths_real = []\n",
    "# each cell is of different length, each with x and y coordinates, make a tensor out of it\n",
    "for s_id, sample in enumerate(real_data_scaled_df.TRACK_ID.unique()):\n",
    "    cell = real_data_scaled_df[real_data_scaled_df.TRACK_ID == sample]\n",
    "    # order by time\n",
    "    cell = cell.sort_values('POSITION_T', ascending=True)\n",
    "    sequence_lengths_real.append(len(cell['y']))\n",
    "    if cut_region_real_data:\n",
    "        cell = cut_region(cell, x_min=316.5, x_max=856.5, y_min=1145, y_max=1351, return_longest=True)\n",
    "        if cell is None:\n",
    "            continue\n",
    "        cell = cell[0]\n",
    "    # pre-pad the data with zeros, but first write zeros as nans to compute the mean and std\n",
    "    track = np.ones((max_sequence_length+1, 2)) * np.nan\n",
    "    track[-len(cell['x'][:max_sequence_length]):, 0] = cell['x'][:max_sequence_length]\n",
    "    track[-len(cell['y'][:max_sequence_length]):, 1] = cell['y'][:max_sequence_length]\n",
    "    real_data.append(track[1:])  # remove the first time point, same as in the simulation (often nan anyway)\n",
    "  \n",
    "real_data = np.stack(real_data)  \n",
    "# order real_data by number of nans, to get data_dim cells with the longest trajectories\n",
    "real_data_full = real_data.copy()\n",
    "real_data = real_data[:cells_in_population*(real_data_full.shape[0] // cells_in_population)]\n",
    "print(real_data.shape)"
   ],
   "id": "52a039b01673630d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "max(sequence_lengths_real)\n",
    "plt.hist(sequence_lengths_real, bins=20)\n",
    "plt.show()"
   ],
   "id": "4db50338856e9eb1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Zelldichte macht was aus\n",
    "\n",
    "Wo wollen Zellen hin? welche parameter beeinflussen das ganze? Zelldichte?\n",
    "\n",
    "1,5mm\n",
    "\n",
    "1 (nicht so gut, extrema)\n",
    "739.79x279.74  microns\n",
    "20231x768 pixel\n",
    "\n",
    "\n",
    "2 (wesentlich mehr der Wahrheit)\n",
    "882.94x287.03 microns\n",
    "2424x788 pixel\n"
   ],
   "id": "17248751d4533848"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.plot(real_data_full[0, :, 0], real_data_full[0, :, 1], 'r', label='Real Cell Trajectories')\n",
    "for cell_id in range(1, real_data_full.shape[0]):           \n",
    "    plt.plot(real_data_full[cell_id, :, 0], real_data_full[cell_id, :, 1], 'r')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.plot([415/1.31, 415/1.31], [1145, 1351], 'black', label='visible window (in simulations)')  # left vertical line\n",
    "plt.plot([856.5, 856.5], [1145, 1351], 'black')  # right vertical line\n",
    "plt.plot([415/1.31, 856.5], [1145, 1145], 'black')  # lower horizontal line\n",
    "plt.plot([415/1.31, 856.5], [1351, 1351], 'black')  # upper horizontal line\n",
    "\n",
    "plt.plot([window_x1, window_x1], [window_y1, window_y2], color='grey', label='actual window')  # left vertical line\n",
    "plt.plot([window_x2, window_x2], [window_y1, window_y2], color='grey')  # right vertical line\n",
    "plt.plot([window_x1, window_x2], [window_y1, window_y1], color='grey')  # lower horizontal line\n",
    "plt.plot([window_x1, window_x2], [window_y2, window_y2], color='grey')  # upper horizontal line\n",
    "tiff_im = plt.imread('Cell_migration_grid_v3_final2_invers.tiff')\n",
    "plt.imshow(tiff_im, origin='lower')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "id": "22026478fa4f460d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# batch the real data\n",
    "batches = [real_data[i * cells_in_population:(i + 1) * cells_in_population] \n",
    "           for i in range(len(real_data) // cells_in_population)]\n",
    "real_posterior_samples_full = []\n",
    "map_idx_full = []\n",
    "n_samples = 100\n",
    "\n",
    "for b in tqdm(batches):\n",
    "    real_posterior_samples = amortizer.sample(trainer.configurator({'sim_data': b[np.newaxis]}), \n",
    "                                          n_samples=n_samples)\n",
    "    real_posterior_samples = real_posterior_samples * p_std + p_mean\n",
    "    real_posterior_samples_full.append(real_posterior_samples)\n",
    "    \n",
    "    # compute the log posterior of the test data\n",
    "    input_dict = {\n",
    "        'sim_data': np.repeat(b[np.newaxis], repeats=n_samples, axis=0),\n",
    "        'parameters': real_posterior_samples\n",
    "    }\n",
    "    log_prob = amortizer.log_posterior(trainer.configurator(input_dict))\n",
    "    # get the MAP\n",
    "    map_idx = np.argmax(log_prob)\n",
    "    map_idx_full.append(map_idx)\n",
    "     \n",
    "real_posterior_samples = np.concatenate(real_posterior_samples_full)\n",
    "map_idxs = np.array(map_idx_full)\n",
    "samples_dict[real_data_id] = real_posterior_samples"
   ],
   "id": "aeefa13b15ae4ce6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.plot_posterior_2d(posterior_draws=real_posterior_samples,\n",
    "                                       prior_draws=prior_draws[:real_posterior_samples.shape[0]],\n",
    "                                       param_names=param_names)\n",
    "plt.savefig(f'../results/{checkpoint_path}/Real_{real_data_id} posterior_vs_prior {len(batches)}.png')\n",
    "plt.show()"
   ],
   "id": "6d0ff48279b4aebe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get posterior samples and simulate\n",
    "map_sims_real = []\n",
    "for i, map_idx in tqdm(enumerate(map_idxs), total=len(map_idxs)):\n",
    "    if not os.path.exists(checkpoint_path+f'/posterior_sim_real_{real_data_id}_{i}.npy'):\n",
    "        # simulate the data\n",
    "        map_sim_real = bayes_simulator(real_posterior_samples[map_idx + i*n_samples][np.newaxis])['sim_data'][0]\n",
    "        np.save(checkpoint_path+f'/posterior_sim_real_{real_data_id}_{i}.npy', map_sim_real)\n",
    "    else:\n",
    "        # load the data\n",
    "        map_sim_real = np.load(checkpoint_path+f'/posterior_sim_real_{real_data_id}_{i}.npy')\n",
    "    map_sims_real.append(map_sim_real)\n",
    "map_sim_real = np.concatenate(map_sims_real)"
   ],
   "id": "987f7031336e533b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compute the summary statistics (invariant to translation, but not to scaling, we want them in microns)\n",
    "real_summary_stats_list = [reduced_coordinates_to_sumstat(real_data * factor)]  # should be only one population\n",
    "simulation_summary_stats_list = [reduced_coordinates_to_sumstat(map_sim_real * factor)]\n",
    "\n",
    "# compute the mean of the summary statistics\n",
    "ad_mean_real, MSD_mean_real, TA_mean_real, VEL_mean_real, WT_mean_real, _, _, _, _, _ = compute_mean_summary_stats(real_summary_stats_list)\n",
    "ad_mean_real_sim, MSD_mean_real_sim, TA_mean_real_sim, VEL_mean_real_sim, WT_mean_real_sim, _, _, _, _, _ = compute_mean_summary_stats(simulation_summary_stats_list)"
   ],
   "id": "3a211ed9763c0e65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, sharex=True, sharey=True, tight_layout=True, figsize=(10, 5))\n",
    "tiff_im = plt.imread('Cell_migration_grid_v3_final2_invers.tiff')\n",
    "\n",
    "# plot the simulations or the MAP\n",
    "#ax[1].imshow(tiff_im)\n",
    "ax[1].plot(map_sim_real[0, :, 0], map_sim_real[0, :, 1], 'b', label='MAP Simulated Trajectories')\n",
    "for cell_id in range(1, map_sim_real.shape[0]):           \n",
    "    ax[1].plot(map_sim_real[cell_id, :, 0], map_sim_real[cell_id, :, 1], 'b')\n",
    "\n",
    "# plot the synthetic data\n",
    "#ax[0].imshow(tiff_im)\n",
    "ax[0].plot(real_data_full[0, :, 0], real_data_full[0, :, 1], 'r', label='Real Cell Trajectories')\n",
    "for cell_id in range(1, real_data.shape[0]):          \n",
    "    ax[0].plot(real_data_full[cell_id, :, 0], real_data_full[cell_id, :, 1], 'r')\n",
    "if real_data.shape[0] < real_data_full.shape[0]:\n",
    "    ax[0].plot(real_data_full[real_data.shape[0], :, 0], real_data_full[real_data.shape[0], :, 1], 'r', \n",
    "               label='(not used for inference)', alpha=0.5)\n",
    "    for cell_id in range(real_data.shape[0]+1, real_data_full.shape[0]):           \n",
    "        ax[0].plot(real_data_full[cell_id, :, 0], real_data_full[cell_id, :, 1], 'r', alpha=0.5)\n",
    "    \n",
    "ax[0].set_ylabel('y')\n",
    "for a in ax:\n",
    "    a.legend()\n",
    "    a.set_xlabel('x')\n",
    "plt.savefig(f'../results/{checkpoint_path}/Real_{real_data_id} vs Simulations {len(batches)}.png')\n",
    "plt.show()"
   ],
   "id": "84e6f43b4f9f1c00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Wasserstein distance\n",
    "wasserstein_distance = stats.wasserstein_distance(ad_mean_real_sim[0], ad_mean_real[0])\n",
    "wasserstein_distance += stats.wasserstein_distance(MSD_mean_real_sim[0], MSD_mean_real[0])\n",
    "wasserstein_distance += stats.wasserstein_distance(TA_mean_real_sim[0], TA_mean_real[0])\n",
    "wasserstein_distance += stats.wasserstein_distance(VEL_mean_real_sim[0], VEL_mean_real[0])\n",
    "print(f\"Wasserstein distance: {wasserstein_distance}\")\n",
    "wasserstein_distance += stats.wasserstein_distance(WT_mean_real_sim[0], WT_mean_real[0])\n",
    "print(f\"Wasserstein distance (with waiting time): {wasserstein_distance}\")\n",
    "\n",
    "wasserstein_distance_dict[real_data_id] = wasserstein_distance\n",
    "print(f\"Wasserstein distance both datasets: {np.sum(list(wasserstein_distance_dict.values()))}\")"
   ],
   "id": "2ff1f254e3036d9a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the summary statistics\n",
    "fig, ax = plt.subplots(nrows=1, ncols=5, sharex=True, sharey='col', tight_layout=True, figsize=(10, 4))\n",
    "# Perform the Kolmogorov-Smirnov test\n",
    "ks_statistic, p_value = stats.ks_2samp(ad_mean_real_sim[0], ad_mean_real[0])\n",
    "print(f\"Angle Degree KS Statistic: {ks_statistic}\")\n",
    "print(f\"Angle Degree P-value: {p_value}, {p_value < 0.05}: different distributions\")\n",
    "ax[0].violinplot([ad_mean_real_sim[0], ad_mean_real[0]], showmeans=True)\n",
    "if p_value < 0.05:\n",
    "    ax[0].set_title(f'Angle Degree\\n(Different)')\n",
    "else:\n",
    "    ax[0].set_title(f'Angle Degree\\n(Same)')\n",
    "ax[0].set_ylabel(f'Angle Degree (degrees)\\nMean per Cell')\n",
    "ax[0].set_xticks([1, 2], ['Simulation', 'Real'])\n",
    "\n",
    "ks_statistic, p_value = stats.ks_2samp(MSD_mean_real_sim[0], MSD_mean_real[0])\n",
    "print(f\"MSD KS Statistic: {ks_statistic}\")\n",
    "print(f\"MSD P-value: {p_value}, {p_value < 0.05}: different distributions\")\n",
    "ax[1].violinplot([MSD_mean_real_sim[0], MSD_mean_real[0]], showmeans=True)\n",
    "if p_value < 0.05:\n",
    "    ax[1].set_title(f'Mean Squared Displacement\\n(Different)')\n",
    "else:\n",
    "    ax[1].set_title(f'Mean Squared Displacement\\n(Same)')\n",
    "ax[1].set_ylabel(f'MSD\\nMean per Cell')\n",
    "ax[1].set_xticks([1, 2], ['Simulation', 'Real'])\n",
    "\n",
    "ks_statistic, p_value = stats.ks_2samp(TA_mean_real_sim[0], TA_mean_real[0])\n",
    "print(f\"Turning Angle KS Statistic: {ks_statistic}\")\n",
    "print(f\"Turning Angle P-value: {p_value}, {p_value < 0.05}: different distributions\")\n",
    "ax[2].violinplot([TA_mean_real_sim[0], TA_mean_real[0]], showmeans=True)\n",
    "if p_value < 0.05:\n",
    "    ax[2].set_title(f'Turning Angle\\n(Different)')\n",
    "else:\n",
    "    ax[2].set_title(f'Turning Angle\\n(Same)')\n",
    "ax[2].set_ylabel(f'Turning Angle (radians)\\nMean per Cell')\n",
    "ax[2].set_xticks([1, 2], ['Simulation', 'Real'])\n",
    "\n",
    "ks_statistic, p_value = stats.ks_2samp(VEL_mean_real_sim[0], VEL_mean_real[0])\n",
    "print(f\"Velocity KS Statistic: {ks_statistic}\")\n",
    "print(f\"Velocity P-value: {p_value}, {p_value < 0.05}: different distributions\")\n",
    "ax[3].violinplot([VEL_mean_real_sim[0], VEL_mean_real[0]], showmeans=True)\n",
    "if p_value < 0.05:\n",
    "    ax[3].set_title(f'Velocity\\n(Different)')\n",
    "else:\n",
    "    ax[3].set_title(f'Velocity\\n(Same)')\n",
    "ax[3].set_ylabel(f'Velocity\\nMean per Cell')\n",
    "ax[3].set_xticks([1, 2], ['Simulation', 'Real'])\n",
    "\n",
    "ks_statistic, p_value = stats.ks_2samp(WT_mean_real_sim[0], WT_mean_real[0])\n",
    "print(f\"Waiting Time KS Statistic: {ks_statistic}\")\n",
    "print(f\"Waiting Time P-value: {p_value}, {p_value < 0.05}: different distributions\")\n",
    "ax[4].violinplot([WT_mean_real_sim[0], WT_mean_real[0]], showmeans=True)\n",
    "if p_value < 0.05:\n",
    "    ax[4].set_title(f'Waiting Time\\n(Different)')\n",
    "else:\n",
    "    ax[4].set_title(f'Waiting Time\\n(Same)')\n",
    "ax[4].set_ylabel(f'Waiting Time\\nMean per Cell')\n",
    "ax[4].set_xticks([1, 2], ['Simulation', 'Real'])\n",
    "plt.savefig(f'../results/{checkpoint_path}/Real_{real_data_id} Summary Stats {len(batches)}.png')\n",
    "plt.show()"
   ],
   "id": "40545d190ce2e2b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if summary_loss is not None:\n",
    "    real_data_config = trainer.configurator({'sim_data': np.stack(batches)})\n",
    "    summary_statistics = trainer.amortizer.summary_net(valid_data_config[\"summary_conditions\"])\n",
    "    summary_statistics_obs = trainer.amortizer.summary_net(real_data_config[\"summary_conditions\"])\n",
    "    mmd = np.sqrt(maximum_mean_discrepancy(summary_statistics, summary_statistics_obs))\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 1, figsize=(6, 4))\n",
    "    colors = cm.viridis(np.linspace(0.1, 0.9, 2))\n",
    "    ax.scatter(\n",
    "        summary_statistics_obs[:, 0], summary_statistics_obs[:, 1], \n",
    "        color=colors[0], label=r\"Observed: $h_{\\psi}(x_{obs})$\" + f\", MMD: {mmd:.3f}\"\n",
    "    )\n",
    "    ax.scatter(summary_statistics[:, 0], summary_statistics[:, 1], \n",
    "               color=colors[1], label=r\"Well-specified: $h_{\\psi}(x)$\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.2)\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "    sns.despine(ax=ax)\n",
    "    fig.savefig(f'../results/{checkpoint_path}/Real_{real_data_id} Summary Latent Space {len(batches)}.png',\n",
    "                bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    MMD_sampling_distribution, MMD_observed = trainer.mmd_hypothesis_test(\n",
    "        observed_data=real_data_config, \n",
    "        reference_data=valid_data_config,  # if not provided, will use the generative model\n",
    "        num_null_samples=500,\n",
    "        bootstrap=True  # if True, use the reference data as null samples\n",
    "    )\n",
    "    fig = bf.diagnostics.plot_mmd_hypothesis_test(MMD_sampling_distribution, MMD_observed)\n",
    "    fig.savefig(f'../results/{checkpoint_path}/Real_{real_data_id} MMD {len(batches)}.png',\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "id": "1d4036aaff03efd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "samples_dict[0][np.newaxis].shape, samples_dict[1][np.newaxis].shape",
   "id": "4bc8ab053542fa65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.plot_posterior_2d(posterior_draws=samples_dict[0],\n",
    "                                       prior_draws=samples_dict[1],\n",
    "                                       post_alpha=0.7,\n",
    "                                       #post_color='red',\n",
    "                                       prior_alpha=0.9, # other posterior samples\n",
    "                                       prior_color='blue',\n",
    "                                       param_names=param_names)\n",
    "plt.savefig(f'../results/{checkpoint_path}/Real posterior_vs_posterior.png')\n",
    "plt.show()"
   ],
   "id": "6fd4801fa63c53c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "8522eceb33f5a581",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
