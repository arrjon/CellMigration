{
 "cells": [
  {
   "cell_type": "code",
   "id": "b3292da9556a2a68",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pyabc\n",
    "from fitmulticell import model as morpheus_model\n",
    "from fitmulticell.sumstat import SummaryStatistics\n",
    "from matplotlib import pyplot as plt\n",
    "from tarp import get_tarp_coverage\n",
    "from tqdm import tqdm\n",
    "\n",
    "from synth_data_params_bayesflow.load_bayesflow_model import load_model, custom_loader\n",
    "from synth_data_params_bayesflow.plotting_routines import plot_compare_summary_stats, plot_trajectory, \\\n",
    "    plot_autocorrelation\n",
    "from synth_data_params_bayesflow.summary_stats import reduced_coordinates_to_sumstat, reduce_to_coordinates, \\\n",
    "    compute_mean_summary_stats\n",
    "\n",
    "# get the job array id and number of processors\n",
    "job_array_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0))\n",
    "n_procs = int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",
    "print(job_array_id)\n",
    "on_cluster = False"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if on_cluster:\n",
    "    gp = '/home/jarruda_hpc/CellMigration/synth_data_params_bayesflow'\n",
    "else:\n",
    "    gp = os.getcwd()\n",
    "\n",
    "# defining the mapping of parameter inside the model xml file. the dictionary name is for \n",
    "# parameter name, and the value are the mapping values, to get the map value for parameter \n",
    "# check here: https://fitmulticell.readthedocs.io/en/latest/example/minimal.html#Inference-problem-definition\n",
    "\n",
    "par_map = {\n",
    "    'gradient_strength': './CellTypes/CellType/Constant[@symbol=\"gradient_strength\"]',\n",
    "    'move.strength': './CellTypes/CellType/Constant[@symbol=\"move.strength\"]',\n",
    "    'move.duration.mean': './CellTypes/CellType/Constant[@symbol=\"move.duration.mean\"]',\n",
    "    'cell_nodes_real': './Global/Constant[@symbol=\"cell_nodes_real\"]',\n",
    "}\n",
    "\n",
    "model_path = gp + \"/cell_movement_v24.xml\"  # time step is 30sec, move.dir completely normalized, init move.dir rand in all directions\n",
    "# defining the summary statistics function\n",
    "max_sequence_length = 120\n",
    "min_sequence_length = 0\n",
    "only_longest_traj_per_cell = True  # mainly to keep the data batchable\n",
    "sumstat = SummaryStatistics(sum_stat_calculator=partial(reduce_to_coordinates,\n",
    "                                                        minimal_length=min_sequence_length,\n",
    "                                                        maximal_length=max_sequence_length,\n",
    "                                                        only_longest_traj_per_cell=only_longest_traj_per_cell))                    \n",
    "\n",
    "if on_cluster:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "    # todo: remember also change tiff path in model.xml!\n",
    "else:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "\n",
    "# parameter values used to generate the synthetic data\n",
    "obs_pars = {\n",
    "    'gradient_strength': 100.,  # strength of the gradient of chemotaxis (energy potential)\n",
    "    'move.strength': 10.,  # strength of directed motion (energy potential)\n",
    "    'move.duration.mean': 0.1,  # mean of exponential distribution (seconds)\n",
    "    'cell_nodes_real': 50.,  # volume of the cell  (\\mu m^2)\n",
    "}\n",
    "\n",
    "obs_pars_log = {key: np.log10(val) for key, val in obs_pars.items()}\n",
    "limits = {'gradient_strength': (1, 10000), #(10 ** 4, 10 ** 8),\n",
    "          'move.strength': (1, 100),\n",
    "          'move.duration.mean': (1e-4, 30), #(math.log10((10 ** -2) * 30), math.log10((10 ** 4))), # smallest time step in simulation 5\n",
    "          'cell_nodes_real': (1, 300)}\n",
    "limits_log = {key: (np.log10(val[0]), np.log10(val[1])) for key, val in limits.items()}\n",
    "\n",
    "\n",
    "prior = pyabc.Distribution(**{key: pyabc.RV(\"uniform\", loc=lb, scale=ub-lb)\n",
    "                              for key, (lb, ub) in limits_log.items()})\n",
    "param_names = list(obs_pars.keys())\n",
    "log_param_names = [f'log_{p}' for p in param_names]\n",
    "print(obs_pars)"
   ],
   "id": "74390dfb0e802a77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import bayesflow as bf\n",
    "from bayesflow.simulation import GenerativeModel, Prior, Simulator\n",
    "\n",
    "\n",
    "def prior_fun(batch_size: int) -> np.ndarray:\n",
    "    samples = []\n",
    "    for _ in range(batch_size):\n",
    "        samples.append(list(prior.rvs().values()))\n",
    "    return np.array(samples)\n",
    "\n",
    "\n",
    "def generate_population_data(param_batch: np.ndarray, cells_in_population: int, max_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate population data\n",
    "    :param param_batch:  batch of parameters\n",
    "    :param cells_in_population:  number of cells in a population (50)\n",
    "    :param max_length:  maximum length of the sequence\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_batch = []\n",
    "    for params in param_batch:\n",
    "        params_dict = {key: p for key, p in zip(obs_pars.keys(), params)}\n",
    "        sim = model.sample(params_dict)\n",
    "        data_batch.append(sim)  # generates a cell population in one experiment\n",
    "\n",
    "    data_batch_transformed = np.ones((param_batch.shape[0], cells_in_population, max_length, 2)) * np.nan\n",
    "    # each cell is of different length, each with x and y coordinates, make a tensor out of it\n",
    "    n_cells_not_visible = 0\n",
    "    for p_id, population_sim in enumerate(data_batch):\n",
    "        if len(population_sim) == 0:\n",
    "            # no cells were visible in the simulation\n",
    "            n_cells_not_visible += 1\n",
    "            continue\n",
    "        for c_id, cell_sim in enumerate(population_sim):\n",
    "            # pre-pad the data with zeros, but first write zeros as nans to compute the mean and std\n",
    "            data_batch_transformed[p_id, c_id, -len(cell_sim['x']):, 0] = cell_sim['x']\n",
    "            data_batch_transformed[p_id, c_id, -len(cell_sim['y']):, 1] = cell_sim['y']\n",
    "\n",
    "    if n_cells_not_visible > 0:\n",
    "        print(f'Simulation with no cells visible: {n_cells_not_visible}/{len(data_batch)}')\n",
    "    return data_batch_transformed"
   ],
   "id": "c606f75e3a4c0257",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "presimulate = False\n",
    "presimulation_path = 'presimulations'\n",
    "n_val_data = 100\n",
    "cells_in_population = 50\n",
    "n_params = len(obs_pars)\n",
    "batch_size = 32\n",
    "iterations_per_epoch = 100\n",
    "# 1000 batches to be generated, 10 epochs until the batch is used again\n",
    "epochs = 500\n",
    "\n",
    "# check if gpu is available\n",
    "print('gpu:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "bayesflow_prior = Prior(batch_prior_fun=prior_fun, param_names=param_names)\n",
    "bayes_simulator = Simulator(batch_simulator_fun=partial(generate_population_data,\n",
    "                                                        cells_in_population=cells_in_population,\n",
    "                                                        max_length=max_sequence_length))\n",
    "generative_model = GenerativeModel(prior=bayesflow_prior, simulator=bayes_simulator,\n",
    "                                   skip_test=True,  # once is enough, simulation takes time\n",
    "                                   name=\"Normalizing Flow Generative Model\")"
   ],
   "id": "cecd0d3e2713e759",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if presimulate:\n",
    "    print('presimulating')\n",
    "    from time import sleep\n",
    "    sleep(job_array_id)\n",
    "\n",
    "    # we create on batch per job and save it in a folder\n",
    "    epoch_id = job_array_id // iterations_per_epoch\n",
    "    generative_model.presimulate_and_save(\n",
    "        batch_size=batch_size,\n",
    "        folder_path=presimulation_path+f'/epoch_{epoch_id}',\n",
    "        iterations_per_epoch=1,\n",
    "        epochs=1,\n",
    "        extend_from=job_array_id,\n",
    "        disable_user_input=True\n",
    "    )\n",
    "    print('Done!')"
   ],
   "id": "958a416f9ac30668",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.exists(os.path.join(gp, 'validation_data.pickle')):\n",
    "    with open(os.path.join(gp, 'validation_data.pickle'), 'rb') as f:\n",
    "        valid_data = pickle.load(f)\n",
    "else:\n",
    "    print('Generating validation data')\n",
    "    valid_data = generative_model(n_val_data)\n",
    "    # save the data\n",
    "    with open(os.path.join(gp, 'validation_data.pickle'), 'wb') as f:\n",
    "        pickle.dump(valid_data, f)\n",
    "\n",
    "x_mean = np.nanmean(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "x_std = np.nanstd(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "p_mean = np.mean(valid_data['prior_draws'], axis=0)\n",
    "p_std = np.std(valid_data['prior_draws'], axis=0)\n",
    "print('Mean and std of data:', x_mean, x_std)\n",
    "print('Mean and std of parameters:', p_mean, p_std)\n",
    "\n",
    "\n",
    "# compute the mean of the summary statistics\n",
    "summary_stats_list_ = [reduced_coordinates_to_sumstat(t) for t in valid_data['sim_data']]\n",
    "(_, ad_averg, _, MSD_averg, _, TA_averg, _, VEL_averg, _, WT_averg) = compute_mean_summary_stats(summary_stats_list_, remove_nan=False)\n",
    "direct_conditions_ = np.stack([ad_averg, MSD_averg, TA_averg, VEL_averg, WT_averg]).T\n",
    "# replace inf with -1\n",
    "direct_conditions_[np.isinf(direct_conditions_)] = np.nan\n",
    "        \n",
    "summary_valid_max = np.nanmax(direct_conditions_, axis=0)\n",
    "summary_valid_min = np.nanmin(direct_conditions_, axis=0)"
   ],
   "id": "df1829ce49462436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "job_array_id = 7\n",
    "trainer, map_idx_sim = load_model(\n",
    "    model_id=job_array_id, \n",
    "    x_mean=x_mean,\n",
    "    x_std=x_std,\n",
    "    p_mean=p_mean,\n",
    "    p_std=p_std,\n",
    "    summary_valid_max=summary_valid_max,\n",
    "    summary_valid_min=summary_valid_min,\n",
    "    generative_model=generative_model\n",
    ")"
   ],
   "id": "b81539411969d014",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check if the model is already trained\n",
    "if not os.path.exists(trainer.checkpoint_path):\n",
    "    trainer._setup_optimizer(\n",
    "        optimizer=None,\n",
    "        epochs=epochs,\n",
    "        iterations_per_epoch=iterations_per_epoch\n",
    "    )\n",
    "\n",
    "    history = trainer.train_from_presimulation(\n",
    "        presimulation_path=presimulation_path,\n",
    "        optimizer=trainer.optimizer,\n",
    "        max_epochs=epochs,\n",
    "        early_stopping=True,\n",
    "        early_stopping_args={'patience': 17 - 2},\n",
    "        custom_loader=custom_loader,\n",
    "        validation_sims=valid_data\n",
    "    )\n",
    "    print('Training done!')\n",
    "else:\n",
    "    history = trainer.loss_history.get_plottable()"
   ],
   "id": "a7c0835963566e42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bf.diagnostics.plot_losses(history['train_losses'], history['val_losses'], fig_size=(10, 6));",
   "id": "aa6d79b3faa0a57a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Diagnostic plots",
   "id": "c72ff040811c2643"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "valid_data_config = trainer.configurator(valid_data)",
   "id": "e6a730d966e83d26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_samples = trainer.amortizer.sample(valid_data_config, n_samples=100)\n",
    "posterior_samples = posterior_samples * p_std + p_mean\n",
    "prior_draws = valid_data_config[\"parameters\"] * p_std + p_mean"
   ],
   "id": "f6ad80516d7191c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bf.diagnostics.plot_sbc_ecdf(posterior_samples, prior_draws, difference=True, param_names=log_param_names)\n",
    "#plt.savefig(f'{trainer.checkpoint_path}/sbc_ecdf.png')"
   ],
   "id": "bf28748d802ce799",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_samples = trainer.amortizer.sample(valid_data_config, n_samples=1000)\n",
    "posterior_samples = posterior_samples * p_std + p_mean"
   ],
   "id": "9672ea6487623555",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bf.diagnostics.plot_recovery(posterior_samples, prior_draws, param_names=log_param_names)\n",
    "#plt.savefig(f'{trainer.checkpoint_path}/recovery.png')"
   ],
   "id": "a8a443ecb123ad2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bf.diagnostics.plot_z_score_contraction(posterior_samples, prior_draws, param_names=log_param_names);",
   "id": "69522e989ab5747d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_samples_reshaped = posterior_samples.reshape(posterior_samples.shape[1],\n",
    "                                                       posterior_samples.shape[0], posterior_samples.shape[2])"
   ],
   "id": "5480764514697d11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ecp_bootstrap, alpha = get_tarp_coverage(posterior_samples_reshaped, prior_draws, references='random',\n",
    "                                         metric='euclidean', norm=True, bootstrap=True)"
   ],
   "id": "134fd5229cfe0037",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "k_sigma = [1, 2, 3]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "ax.plot([0, 1], [0, 1], ls='--', color='k', label=\"Ideal case\")\n",
    "ax.plot(alpha, ecp_bootstrap.mean(axis=0), label='TARP')\n",
    "for k in k_sigma:\n",
    "    ax.fill_between(alpha, \n",
    "                    ecp_bootstrap.mean(axis=0) - k * ecp_bootstrap.std(axis=0), \n",
    "                    ecp_bootstrap.mean(axis=0) + k * ecp_bootstrap.std(axis=0), \n",
    "                    alpha = 0.7)\n",
    "ax.legend()\n",
    "ax.set_ylabel(\"Expected Coverage\")\n",
    "ax.set_xlabel(\"Credibility Level\")\n",
    "plt.show()"
   ],
   "id": "a8c1d2d8a1cc04e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test on synthetic data",
   "id": "de8f91657ceebed0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# simulate test data\n",
    "test_params = np.array(list(obs_pars_log.values()))\n",
    "if not os.path.exists(os.path.join(gp, 'test_sim.npy')):\n",
    "    test_sim_full = bayes_simulator(test_params[np.newaxis])\n",
    "    test_sim = test_sim_full['sim_data']\n",
    "    np.save(os.path.join(gp, 'test_sim.npy'), test_sim)\n",
    "else:\n",
    "    test_sim = np.load(os.path.join(gp, 'test_sim.npy'))\n",
    "    test_sim_full = {'sim_data': test_sim}\n",
    "test_sim.shape"
   ],
   "id": "d18447b62e9c4750",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_posterior_samples = trainer.amortizer.sample(trainer.configurator(test_sim_full), n_samples=100)\n",
    "test_posterior_samples = test_posterior_samples * p_std + p_mean\n",
    "test_posterior_samples_median = np.median(test_posterior_samples, axis=0)\n",
    "# compute the log posterior of the test data\n",
    "input_dict = {\n",
    "    'sim_data': np.repeat(test_sim, repeats=100, axis=0),\n",
    "    'parameters': test_posterior_samples\n",
    "}\n",
    "log_prob = trainer.amortizer.log_posterior(trainer.configurator(input_dict))\n",
    "    \n",
    "# get the MAP\n",
    "map_idx = np.argmax(log_prob)\n",
    "\n",
    "# save posterior samples to load in abc results\n",
    "#np.save(f'{trainer.checkpoint_path}/posterior_samples_synthetic.npy', test_posterior_samples)"
   ],
   "id": "731774aebbbce43c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.plot_posterior_2d(posterior_draws=test_posterior_samples,\n",
    "                                       prior_draws=prior_draws[:test_posterior_samples.shape[0]],\n",
    "                                       param_names=log_param_names)\n",
    "# from figure get axis\n",
    "ax = fig.get_axes()\n",
    "for i, a in enumerate(ax):\n",
    "    # plot only on the diagonal\n",
    "    if i == i // len(log_param_names) * (len(log_param_names)+1):\n",
    "        a.axvline(test_params[i // len(log_param_names)], color='r', label='True parameter')\n",
    "for i, a in enumerate(ax):\n",
    "    # plot only on the diagonal\n",
    "    if i == i // len(log_param_names) * (len(log_param_names)+1):\n",
    "        a.axvline(test_posterior_samples_median[i // len(log_param_names)], color='b', label='Median parameter')\n",
    "#plt.savefig(f'{trainer.checkpoint_path}/posterior_vs_prior.png')\n",
    "plt.show()"
   ],
   "id": "dc88096666755040",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get posterior samples and simulate\n",
    "if not os.path.exists(trainer.checkpoint_path+'/posterior_sim.npy'):\n",
    "    # simulate the data\n",
    "    posterior_sim = bayes_simulator(test_posterior_samples)['sim_data']\n",
    "    np.save(trainer.checkpoint_path+'/posterior_sim.npy', posterior_sim)\n",
    "    \n",
    "    print('map_sim', map_idx, log_prob[map_idx], test_posterior_samples[map_idx])\n",
    "    map_idx_sim = map_idx\n",
    "else:\n",
    "    posterior_sim = np.load(trainer.checkpoint_path+'/posterior_sim.npy')\n",
    "\n",
    "# reorder the posterior samples\n",
    "posterior_sim = np.delete(posterior_sim, map_idx_sim, axis=0)\n",
    "posterior_sim = np.insert(posterior_sim, 0, posterior_sim[map_idx_sim][np.newaxis], axis=0)"
   ],
   "id": "ab4d1e0a7e2402",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the summary statistics\n",
    "plot_compare_summary_stats(test_sim, posterior_sim, path=f'{trainer.checkpoint_path}/Summary Stats')"
   ],
   "id": "57e10145a28be124",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the trajectories\n",
    "plot_trajectory(test_sim[0], posterior_sim[0], path=f'{trainer.checkpoint_path}/Simulations', show_umap=True)\n",
    "plot_autocorrelation(test_sim[0], posterior_sim[0], path=f'{trainer.checkpoint_path}/Autocorrelation')"
   ],
   "id": "8f78c2c056e6a37a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if trainer.amortizer.summary_loss is not None:\n",
    "    test_data_config = trainer.configurator(test_sim_full)\n",
    "    \n",
    "    MMD_sampling_distribution, MMD_observed = trainer.mmd_hypothesis_test(\n",
    "        observed_data=test_data_config, \n",
    "        reference_data=valid_data_config,  # if not provided, will use the generative model\n",
    "        num_null_samples=500,\n",
    "        bootstrap=True  # if True, use the reference data as null samples\n",
    "    )\n",
    "    fig = bf.diagnostics.plot_mmd_hypothesis_test(MMD_sampling_distribution, MMD_observed)\n",
    "    fig.savefig(f'{trainer.checkpoint_path}/Synthetic MMD.png', bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "id": "420c2b14ccdeee35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Real Data",
   "id": "748dffc6b156ef8e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from load_data import load_real_data\n",
    "\n",
    "wasserstein_distance_dict = {0: np.nan, 1: np.nan}\n",
    "samples_dict = {0: np.nan, 1: np.nan}\n",
    "prior_draws = prior_fun(1000)"
   ],
   "id": "fa8abce2ebdc9339",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "real_data_id = [0, 1][1]\n",
    "real_data, real_data_full = load_real_data(data_id=real_data_id, \n",
    "                                           max_sequence_length=max_sequence_length, \n",
    "                                           cells_in_population=cells_in_population,\n",
    "                                           plot_data=True)\n",
    "real_data = np.array([real_data[start:start+cells_in_population] for start in range(0, len(real_data), cells_in_population)])[0]\n",
    "print(real_data.shape)"
   ],
   "id": "52a039b01673630d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Zelldichte macht was aus\n",
    "\n",
    "Wo wollen Zellen hin? welche parameter beeinflussen das ganze? Zelldichte?\n",
    "\n",
    "1,5mm\n",
    "\n",
    "1 (nicht so gut, extrema)\n",
    "739.79x279.74  microns\n",
    "20231x768 pixel\n",
    "\n",
    "\n",
    "2 (wesentlich mehr der Wahrheit)\n",
    "882.94x287.03 microns\n",
    "2424x788 pixel\n"
   ],
   "id": "17248751d4533848"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# batch the real data\n",
    "batches = [real_data[i * cells_in_population:(i + 1) * cells_in_population] \n",
    "           for i in range(len(real_data) // cells_in_population)]\n",
    "real_posterior_samples_full = []\n",
    "n_samples = prior_draws.shape[0]\n",
    "\n",
    "for b in tqdm(batches):\n",
    "    real_posterior_samples = trainer.amortizer.sample(trainer.configurator({'sim_data': b[np.newaxis]}), \n",
    "                                          n_samples=n_samples)\n",
    "    real_posterior_samples = real_posterior_samples * p_std + p_mean\n",
    "    real_posterior_samples_full.append(real_posterior_samples)\n",
    "    \n",
    "    #np.save(f'{trainer.checkpoint_path}/posterior_samples_real.npy', real_posterior_samples)\n",
    "     \n",
    "real_posterior_samples = np.concatenate(real_posterior_samples_full)\n",
    "samples_dict[real_data_id] = real_posterior_samples"
   ],
   "id": "aeefa13b15ae4ce6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.plot_posterior_2d(posterior_draws=real_posterior_samples,\n",
    "                                       prior_draws=prior_draws[:real_posterior_samples.shape[0]],\n",
    "                                       param_names=log_param_names)\n",
    "plt.savefig(f'{trainer.checkpoint_path}/Real_{real_data_id} posterior_vs_prior {len(batches)}.png')\n",
    "plt.show()"
   ],
   "id": "6d0ff48279b4aebe",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "posterior_median = np.median(real_posterior_samples, axis=0)",
   "id": "d06cb264ff28b9bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get posterior samples and simulate\n",
    "if not os.path.exists(f'{trainer.checkpoint_path}/posterior_sim_real_{real_data_id}.npy'):\n",
    "    # simulate the data\n",
    "    sim_list = []\n",
    "    for i in tqdm(range(10)):\n",
    "        if i == 0:\n",
    "            posterior_sim = bayes_simulator(posterior_median[np.newaxis])['sim_data'][0]\n",
    "        else:\n",
    "            posterior_sim = bayes_simulator(real_posterior_samples[i][np.newaxis])['sim_data'][0]\n",
    "        sim_list.append(posterior_sim[np.newaxis])\n",
    "    posterior_sim_real = np.concatenate(sim_list)\n",
    "    np.save(f'{trainer.checkpoint_path}/posterior_sim_real_{real_data_id}.npy', posterior_sim_real)\n",
    "else:\n",
    "    posterior_sim_real = np.load(f'{trainer.checkpoint_path}/posterior_sim_real_{real_data_id}.npy')"
   ],
   "id": "1a119a7b623a2eb5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "wasserstein_distance_dict[real_data_id] = plot_compare_summary_stats([real_data], posterior_sim_real, \n",
    "                                                                     path=f'{trainer.checkpoint_path}/Real_{real_data_id} vs Simulations {len(batches)}')\n",
    "print(f\"Wasserstein distance both datasets: {np.sum(list(wasserstein_distance_dict.values()))}\")"
   ],
   "id": "cb5a43d6789d6653",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_trajectory(real_data, posterior_sim_real[0],\n",
    "                path=f'{trainer.checkpoint_path}/Real_{real_data_id} Simulations {len(batches)}', \n",
    "                label_true='Real Trajectories', two_plots=True, show_image=False, show_umap=True)\n",
    "plot_autocorrelation(real_data, posterior_sim_real[0], path=f'{trainer.checkpoint_path}/Real_{real_data_id} Autocorrelation {len(batches)}')"
   ],
   "id": "726750b8c6337275",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if trainer.amortizer.summary_loss is not None:\n",
    "    from matplotlib.cm import viridis\n",
    "    real_data_config = trainer.configurator({'sim_data': batches})\n",
    "    summary_statistics = trainer.amortizer.summary_net(valid_data_config['summary_conditions'])\n",
    "    summary_statistics_obs = trainer.amortizer.summary_net(real_data_config['summary_conditions'])\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    colors = viridis(np.linspace(0.1, 0.9, 2))\n",
    "    ax.scatter(\n",
    "        summary_statistics_obs[:, 0], summary_statistics_obs[:, 1], color=colors[0], label=r\"Observed: $h_{\\psi}(x_{obs})$\"\n",
    "    )\n",
    "    ax.scatter(summary_statistics[:, 0], summary_statistics[:, 1], color=colors[1], label=r\"Well-specified: $h_{\\psi}(x)$\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.2)\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "    \n",
    "    fig.savefig(f'{trainer.checkpoint_path}/Real_{real_data_id} Summary Latent Space.png',\n",
    "                bbox_inches='tight')\n",
    "    \n",
    "    MMD_sampling_distribution, MMD_observed = trainer.mmd_hypothesis_test(\n",
    "        observed_data=real_data_config, \n",
    "        reference_data=valid_data_config,  # if not provided, will use the generative model\n",
    "        num_null_samples=500,\n",
    "        bootstrap=True  # if True, use the reference data as null samples\n",
    "    )\n",
    "    fig = bf.diagnostics.plot_mmd_hypothesis_test(MMD_sampling_distribution, MMD_observed)\n",
    "    fig.savefig(f'{trainer.checkpoint_path}/Real_{real_data_id} MMD.png',\n",
    "                bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "id": "1d4036aaff03efd9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not np.isnan(samples_dict[0]).all() and not np.isnan(samples_dict[1]).all():\n",
    "    fig = bf.diagnostics.plot_posterior_2d(posterior_draws=samples_dict[0],\n",
    "                                           prior_draws=samples_dict[1],\n",
    "                                           post_alpha=0.7,\n",
    "                                           #post_color='red',\n",
    "                                           prior_alpha=0.9, # other posterior samples\n",
    "                                           prior_color='blue',\n",
    "                                           param_names=log_param_names)\n",
    "    plt.savefig(f'{trainer.checkpoint_path}/Real posterior_vs_posterior.png')\n",
    "    plt.show()"
   ],
   "id": "6fd4801fa63c53c6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "6d0b2702105f00fb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
