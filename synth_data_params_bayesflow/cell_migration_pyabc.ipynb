{
 "cells": [
  {
   "cell_type": "code",
   "id": "b3292da9556a2a68",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import pickle\n",
    "from functools import partial\n",
    "from typing import Union\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pyabc\n",
    "#from pyabc.sampler import RedisEvalParallelSampler\n",
    "import scipy.stats as stats\n",
    "from fitmulticell import model as morpheus_model\n",
    "from fitmulticell.sumstat import SummaryStatistics\n",
    "from tqdm import tqdm\n",
    "\n",
    "from summary_stats import reduced_coordinates_to_sumstat, reduce_to_coordinates, compute_mean_summary_stats\n",
    "from plotting_routines import plot_compare_summary_stats, plot_trajectory\n",
    "\n",
    "# get the job array id and number of processors\n",
    "job_array_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0))\n",
    "n_procs = 5#int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",
    "print(job_array_id)\n",
    "on_cluster = False\n",
    "population_size = 1000\n",
    "#run_old_sumstats = True\n",
    "#\n",
    "#if job_array_id == 1:\n",
    "#    run_old_sumstats = False\n",
    "\n",
    "if on_cluster:\n",
    "    parser = argparse.ArgumentParser(description='Parse necessary arguments')\n",
    "    parser.add_argument('-pt', '--port', type=str, default=\"50004\",\n",
    "                        help='Which port should be use?')\n",
    "    parser.add_argument('-ip', '--ip', type=str,\n",
    "                        help='Dynamically passed - BW: Login Node 3')\n",
    "    args = parser.parse_args()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if on_cluster:\n",
    "    gp = '/home/jarruda_hpc/CellMigration/synth_data_params_bayesflow'\n",
    "else:\n",
    "    gp = os.getcwd()\n",
    "\n",
    "par_map = {\n",
    "    'gradient_strength': './CellTypes/CellType/Constant[@symbol=\"gradient_strength\"]',\n",
    "    'move.strength': './CellTypes/CellType/Constant[@symbol=\"move.strength\"]',\n",
    "    'move.duration.mean': './CellTypes/CellType/Constant[@symbol=\"move.duration.mean\"]',\n",
    "    'cell_nodes_real': './Global/Constant[@symbol=\"cell_nodes_real\"]',\n",
    "}\n",
    "\n",
    "model_path = gp + \"/cell_movement_v24.xml\"  # time step is 30sec, move.dir completely normalized, init move.dir rand in all directions\n",
    "# defining the summary statistics function\n",
    "max_sequence_length = 120\n",
    "min_sequence_length = 0\n",
    "only_longest_traj_per_cell = True  # mainly to keep the data batchable\n",
    "cells_in_population = 50\n",
    "\n",
    "\n",
    "def make_sumstat_dict(data: Union[dict, np.ndarray]) -> dict:\n",
    "    if isinstance(data, dict):\n",
    "        # get key\n",
    "        key = list(data.keys())[0]\n",
    "        data = data[key]\n",
    "    data = data[0]  # only one sample\n",
    "    # compute the summary statistics\n",
    "    summary_stats_dict = reduced_coordinates_to_sumstat(data)\n",
    "    (ad_mean, _, msd_mean, _, ta_mean, _, vel_mean, _, wt_mean, _) = compute_mean_summary_stats([summary_stats_dict], remove_nan=False)\n",
    "    cleaned_dict = {\n",
    "        'ad': np.array(ad_mean).flatten(),\n",
    "        'msd': np.array(msd_mean).flatten(),\n",
    "        'ta': np.array(ta_mean).flatten(),\n",
    "        'vel': np.array(vel_mean).flatten(),\n",
    "        'wt': np.array(wt_mean).flatten()\n",
    "    }\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "def prepare_sumstats(output_morpheus_model) -> dict:\n",
    "    sim_coordinates = reduce_to_coordinates(output_morpheus_model, \n",
    "                          minimal_length=min_sequence_length, \n",
    "                          maximal_length=max_sequence_length,\n",
    "                          only_longest_traj_per_cell=only_longest_traj_per_cell\n",
    "                          )\n",
    "    \n",
    "    # we now do exactly the same as in the BayesFlow workflow, but here we get only one sample at a time\n",
    "    data_transformed = np.ones((1, cells_in_population, max_sequence_length, 2)) * np.nan\n",
    "    # each cell is of different length, each with x and y coordinates, make a tensor out of it\n",
    "    n_cells_not_visible = 0\n",
    "    if len(sim_coordinates) != 0:\n",
    "        # some cells were visible in the simulation\n",
    "        for c_id, cell_sim in enumerate(sim_coordinates):\n",
    "            # pre-pad the data with zeros, but first write zeros as nans to compute the mean and std\n",
    "            data_transformed[0, c_id, -len(cell_sim['x']):, 0] = cell_sim['x']\n",
    "            data_transformed[0, c_id, -len(cell_sim['y']):, 1] = cell_sim['y']\n",
    "    \n",
    "    return {'sim': data_transformed}\n",
    "\n",
    "\n",
    "sumstat = SummaryStatistics(sum_stat_calculator=prepare_sumstats)                    \n",
    "\n",
    "if on_cluster:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "    # todo: remember also change tiff path in model.xml!\n",
    "else:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "# parameter values used to generate the synthetic data\n",
    "obs_pars = {\n",
    "    'gradient_strength': 100.,  # strength of the gradient of chemotaxis\n",
    "    'move.strength': 10.,  # strength of directed motion\n",
    "    'move.duration.mean': 0.1,  # mean of exponential distribution (1/seconds)\n",
    "    'cell_nodes_real': 50.,  # volume of the cell\n",
    "}\n",
    "\n",
    "\n",
    "obs_pars_log = {key: np.log10(val) for key, val in obs_pars.items()}\n",
    "limits = {'gradient_strength': (1, 10000), #(10 ** 4, 10 ** 8),\n",
    "          'move.strength': (1, 100),\n",
    "          'move.duration.mean': (1e-4, 30), #(math.log10((10 ** -2) * 30), math.log10((10 ** 4))), # smallest time step in simulation 5\n",
    "          'cell_nodes_real': (1, 300)}\n",
    "limits_log = {key: (np.log10(val[0]), np.log10(val[1])) for key, val in limits.items()}\n",
    "\n",
    "\n",
    "prior = pyabc.Distribution(**{key: pyabc.RV(\"uniform\", loc=lb, scale=ub-lb)\n",
    "                              for key, (lb, ub) in limits_log.items()})\n",
    "param_names = list(obs_pars.keys())\n",
    "log_param_names = [f'log_{p}' for p in param_names]\n",
    "print(obs_pars)"
   ],
   "id": "74390dfb0e802a77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "sigma0 = 550\n",
    "space_x0 = 1173/2\n",
    "space_y0 = 1500/1.31/2\n",
    "x0, y0 = 1173/2, (1500+1500/2+270)/1.31\n",
    "u1 = lambda space_x, space_y: 7/(2*np.pi*(sigma0**2)) *np.exp(-1/2*(((space_x)-(x0))**2+ ((space_y)-(y0))**2)/(sigma0**2))\n",
    "\n",
    "# plot the function\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "x = np.linspace(0, 1173 , 100)\n",
    "y = np.linspace(0, 2500 , 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = u1(X, Y)\n",
    "ax.plot_surface(X, Y, Z, alpha=0.5)\n",
    "# plot start points\n",
    "ax.scatter(space_x0, space_y0, u1(space_x0, space_y0), color='r', s=100)\n",
    "\n",
    "ax.set_xlabel('space_x')\n",
    "ax.set_ylabel('space_y')\n",
    "plt.show()\n",
    "\n",
    "space_x0, space_y0, u1(space_x0, space_y0), u1(x0, y0)"
   ],
   "id": "be4eaa8d21c3bfff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# simulate test data\n",
    "test_params = np.array(list(obs_pars_log.values()))\n",
    "if not os.path.exists(os.path.join(gp, 'test_sim.npy')):\n",
    "    raise FileNotFoundError('Test data not found')\n",
    "else:\n",
    "    test_sim = np.load(os.path.join(gp, 'test_sim.npy'))"
   ],
   "id": "dc31691dc1bbede7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def obj_func_wass(sim: dict, obs: dict):\n",
    "    total = 0\n",
    "    for key in sim:\n",
    "        x, y = np.array(sim[key]), np.array(obs[key])\n",
    "        if x.size == 0:\n",
    "            return np.inf\n",
    "        total += stats.wasserstein_distance(x, y)\n",
    "    return total"
   ],
   "id": "1145158513a20ad6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#redis_sampler = RedisEvalParallelSampler(host=args.ip, port=args.port,\n",
    "#                                         adapt_look_ahead_proposal=False,\n",
    "#                                         look_ahead=False)\n",
    "\n",
    "abc = pyabc.ABCSMC(model, prior,\n",
    "                   distance_function=obj_func_wass,\n",
    "                   summary_statistics=make_sumstat_dict,\n",
    "                   population_size=population_size,\n",
    "                   sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",
    "                   #sampler=redis_sampler\n",
    "                   )\n",
    "\n",
    "db_path = os.path.join(gp, \"abc_results/synthetic_test_old_sumstats.db\")\n",
    "\n",
    "if not os.path.exists(db_path):\n",
    "    history = abc.new(\"sqlite:///\" + db_path, make_sumstat_dict(test_sim))\n",
    "\n",
    "    # start the abc fitting\n",
    "    abc.run(min_acceptance_rate=1e-2, max_nr_populations=30)\n",
    "    print('Done!')\n",
    "else:\n",
    "    history = abc.load(\"sqlite:///\" + db_path)"
   ],
   "id": "1636e7efc136f2e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, len(param_names), tight_layout=True, figsize=(12, 4))\n",
    "for i, param in enumerate(param_names):\n",
    "    for t in range(history.max_t + 1):\n",
    "        df, w = history.get_distribution(m=0, t=t)\n",
    "        pyabc.visualization.plot_kde_1d(\n",
    "            df,\n",
    "            w,\n",
    "            xmin=limits_log[param][0],\n",
    "            xmax=limits_log[param][1],\n",
    "            x=param,\n",
    "            xname=log_param_names[i],\n",
    "            ax=ax[i],\n",
    "            label=f\"PDF t={t}\",\n",
    "        )\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlim((limits_log[param][0]-0.2, limits_log[param][1]+0.2))\n",
    "plt.savefig(os.path.join(gp, 'abc_results/population_kdes.png'))\n",
    "plt.show()\n",
    "\n",
    "fig, arr_ax = plt.subplots(2, 3, figsize=(12, 6), tight_layout=True)\n",
    "arr_ax = arr_ax.flatten()\n",
    "pyabc.visualization.plot_sample_numbers(history, ax=arr_ax[0])\n",
    "pyabc.visualization.plot_walltime(history, ax=arr_ax[1], unit='h')\n",
    "pyabc.visualization.plot_epsilons(history, ax=arr_ax[2])\n",
    "pyabc.visualization.plot_effective_sample_sizes(history, ax=arr_ax[3])\n",
    "pyabc.visualization.plot_acceptance_rates_trajectory(history, ax=arr_ax[4])\n",
    "# remove last axis\n",
    "arr_ax[-1].axis('off')\n",
    "plt.savefig(os.path.join(gp, 'abc_results/diagnostics.png'))\n",
    "plt.show()\n",
    "\n",
    "pyabc.visualization.plot_credible_intervals(history, levels=[0.95]);"
   ],
   "id": "efcbb86db464a88d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def resample(points, weights, n):\n",
    "    \"\"\"\n",
    "    Resample from weighted samples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    points : np.ndarray\n",
    "        The random samples, can be of higher dimensions. Sampling is done along axis 0.\n",
    "    weights : np.ndarray\n",
    "        Weights of each sample point.\n",
    "    n : int\n",
    "        Number of samples to resample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    resampled : np.ndarray\n",
    "        A total of `n` points sampled from `points` with replacement\n",
    "        according to `weights`.\n",
    "    \"\"\"\n",
    "    weights = np.asarray(weights)\n",
    "    weights /= np.sum(weights)\n",
    "    indices = np.random.choice(points.shape[0], size=n, p=weights)\n",
    "    resampled = points[indices]\n",
    "    return resampled"
   ],
   "id": "ffb1c5cd5498088a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "abc_df, abc_w = history.get_distribution()\n",
    "abc_posterior_samples = resample(abc_df[param_names].values, abc_w, n=1000)\n",
    "abc_median = np.median(abc_posterior_samples, axis=0)"
   ],
   "id": "3367ada60af6a8fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import bayesflow as bf\n",
    "from bayesflow.amortizers import AmortizedPosterior\n",
    "from bayesflow.networks import InvertibleNetwork\n",
    "from bayesflow.helper_networks import MultiConv1D\n",
    "from bayesflow.trainers import Trainer\n",
    "from bayesflow import default_settings as defaults\n",
    "from tensorflow.keras.layers import Dense, GRU, LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Sequential"
   ],
   "id": "c606f75e3a4c0257",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior_draws = np.array([list(prior.rvs().values()) for _ in range(abc_posterior_samples.shape[0])])\n",
    "fig = bf.diagnostics.plot_posterior_2d(posterior_draws=abc_posterior_samples,\n",
    "                                       prior_draws=prior_draws,\n",
    "                                       param_names=log_param_names)\n",
    "# from figure get axis\n",
    "ax = fig.get_axes()\n",
    "for i, a in enumerate(ax):\n",
    "    # plot only on the diagonal\n",
    "    if i == i // len(log_param_names) * (len(log_param_names)+1):\n",
    "        a.axvline(test_params[i // len(log_param_names)], color='b', label='True parameter')\n",
    "plt.savefig(os.path.join(gp, 'abc_results/posterior_vs_prior.png'))\n",
    "plt.show()"
   ],
   "id": "98dbfbc06679152c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get posterior samples and simulate\n",
    "if not os.path.exists(os.path.join(gp, 'abc_results/posterior_sim.npy')):\n",
    "    # simulate the data\n",
    "    sim_list = []\n",
    "    for i in tqdm(range(10)):\n",
    "        if i == 0:\n",
    "            sim_dict = {key: p for key, p in zip(obs_pars.keys(), abc_median)}\n",
    "        else:\n",
    "            sim_dict = {key: p for key, p in zip(obs_pars.keys(), abc_posterior_samples[i])}\n",
    "        posterior_sim = model(sim_dict)\n",
    "        sim_list.append(posterior_sim['sim'])\n",
    "    posterior_sim = np.concatenate(sim_list)\n",
    "    np.save(os.path.join(gp, 'abc_results/posterior_sim.npy'), posterior_sim)\n",
    "else:\n",
    "    posterior_sim = np.load(os.path.join(gp, 'abc_results/posterior_sim.npy'))"
   ],
   "id": "e3a0b55a7408e3f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the summary statistics\n",
    "plot_compare_summary_stats(test_sim, posterior_sim, path='abc_results/Summary Stats.png');"
   ],
   "id": "dc4ea826fa0f51e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the trajectory\n",
    "plot_trajectory(test_sim[0], posterior_sim[0], path='abc_results/Simulations.png')"
   ],
   "id": "dde7247a8ea083d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.exists(os.path.join(gp, 'validation_data.pickle')):\n",
    "    with open(os.path.join(gp, 'validation_data.pickle'), 'rb') as f:\n",
    "        valid_data = pickle.load(f)\n",
    "else:\n",
    "    raise FileNotFoundError('Validation data not found')\n",
    "\n",
    "x_mean = np.nanmean(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "x_std = np.nanstd(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "p_mean = np.mean(valid_data['prior_draws'], axis=0)\n",
    "p_std = np.std(valid_data['prior_draws'], axis=0)\n",
    "print('Mean and std of data:', x_mean, x_std)\n",
    "print('Mean and std of parameters:', p_mean, p_std)\n",
    "\n",
    "\n",
    "# compute the mean of the summary statistics\n",
    "summary_stats_list_ = [reduced_coordinates_to_sumstat(t) for t in valid_data['sim_data']]\n",
    "(_, ad_averg, _, MSD_averg, _, TA_averg, _, VEL_averg, _, WT_averg) = compute_mean_summary_stats(summary_stats_list_, remove_nan=False)\n",
    "\n",
    "direct_conditions_ = np.stack([ad_averg, MSD_averg, TA_averg, VEL_averg, WT_averg]).T\n",
    "# replace inf with -1\n",
    "direct_conditions_[np.isinf(direct_conditions_)] = np.nan\n",
    "        \n",
    "summary_valid_max = np.nanmax(direct_conditions_, axis=0)\n",
    "summary_valid_min = np.nanmin(direct_conditions_, axis=0)"
   ],
   "id": "df1829ce49462436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def configurator(forward_dict: dict, manual_summary: bool = False) -> dict:\n",
    "    out_dict = {}\n",
    "\n",
    "    # Extract data\n",
    "    x = forward_dict[\"sim_data\"]\n",
    "    \n",
    "    # compute manual summary statistics\n",
    "    if manual_summary:\n",
    "        summary_stats_list = [reduced_coordinates_to_sumstat(t) for t in x]\n",
    "        # compute the mean of the summary statistics\n",
    "        (_, ad_averg, _, MSD_averg, _, \n",
    "         TA_averg, _, VEL_averg, _, WT_averg) = compute_mean_summary_stats(summary_stats_list, remove_nan=False)\n",
    "        direct_conditions = np.stack([ad_averg, MSD_averg, TA_averg, VEL_averg, WT_averg]).T   \n",
    "        # normalize statistics\n",
    "        direct_conditions = (direct_conditions - summary_valid_min) / (summary_valid_max - summary_valid_min)\n",
    "        # replace nan or inf with -1\n",
    "        direct_conditions[np.isinf(direct_conditions)] = -1\n",
    "        direct_conditions[np.isnan(direct_conditions)] = -1\n",
    "        out_dict['direct_conditions'] = direct_conditions.astype(np.float32)\n",
    "    \n",
    "    # Normalize data\n",
    "    x = (x - x_mean) / x_std\n",
    "    \n",
    "    # Check for NaN values in the first entry of the last axis\n",
    "    # If nan_mask is False (no NaNs), set to 1; otherwise, set to 0\n",
    "    nan_mask = np.isnan(x[..., 0])\n",
    "    new_dim = np.where(nan_mask, 0, 1)\n",
    "    new_dim_expanded = np.expand_dims(new_dim, axis=-1)\n",
    "    x = np.concatenate((x, new_dim_expanded), axis=-1)\n",
    "\n",
    "    # Normalize data\n",
    "    x[np.isnan(x)] = 0  # replace nan with 0, pre-padding (since we have nans in the data at the end)\n",
    "    out_dict['summary_conditions'] = x.astype(np.float32)\n",
    "\n",
    "    # Extract params\n",
    "    if 'parameters' in forward_dict.keys():\n",
    "        forward_dict[\"prior_draws\"] = forward_dict[\"parameters\"]\n",
    "    if 'prior_draws' in forward_dict.keys():\n",
    "        params = forward_dict[\"prior_draws\"]\n",
    "        params = (params - p_mean) / p_std\n",
    "        out_dict['parameters'] = params.astype(np.float32)\n",
    "    return out_dict"
   ],
   "id": "330709997cc7e561",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "import types\n",
    "import tempfile\n",
    "import keras.models\n",
    "\n",
    "def make_keras_picklable():\n",
    "    def __getstate__(self):\n",
    "        model_str = \"\"\n",
    "        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\n",
    "            keras.models.save_model(self, fd.name, overwrite=True)\n",
    "            model_str = fd.read()\n",
    "        d = { 'model_str': model_str }\n",
    "        return d\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\n",
    "            fd.write(state['model_str'])\n",
    "            fd.flush()\n",
    "            model = keras.models.load_model(fd.name)\n",
    "        self.__dict__ = model.__dict__\n",
    "\n",
    "\n",
    "    cls = keras.models.Model\n",
    "    cls.__getstate__ = __getstate__\n",
    "    cls.__setstate__ = __setstate__\n",
    "    \n",
    "make_keras_picklable()"
   ],
   "id": "dce8fdcc5d41bcbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "def load_model(model_id):\n",
    "    # Set the logger to the desired level\n",
    "    tf.get_logger().setLevel('ERROR')  # This will suppress warnings and info logs from TensorFlow\n",
    "\n",
    "    # define the network\n",
    "    # define the network\n",
    "    class GroupSummaryNetwork(tf.keras.Model):\n",
    "        \"\"\"Network to summarize the data of groups of cells.  Each group is passed through a series of convolutional layers\n",
    "        followed by an LSTM layer. The output of the LSTM layer is then pooled across the groups and dense layer applied\n",
    "        to obtain a summary of fixed dimensionality. The network is invariant to the order of the groups.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(\n",
    "                self,\n",
    "                summary_dim,\n",
    "                num_conv_layers=2,\n",
    "                rnn_units=128,\n",
    "                bidirectional=True,\n",
    "                conv_settings=None,\n",
    "                use_attention=False,\n",
    "                return_attention_weights=False,\n",
    "                use_GRU=True,\n",
    "                **kwargs\n",
    "        ):\n",
    "            super().__init__(**kwargs)\n",
    "\n",
    "            if conv_settings is None:\n",
    "                conv_settings = defaults.DEFAULT_SETTING_MULTI_CONV\n",
    "            self.conv_settings = conv_settings\n",
    "\n",
    "            conv = Sequential([MultiConv1D(conv_settings) for _ in range(num_conv_layers)])\n",
    "            self.num_conv_layers = num_conv_layers\n",
    "            self.group_conv = tf.keras.layers.TimeDistributed(conv)\n",
    "            self.use_attention = use_attention\n",
    "            self.return_attention_weights = return_attention_weights\n",
    "            self.pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "            self.rnn_units = rnn_units\n",
    "            self.use_GRU = use_GRU\n",
    "            self.bidirectional = bidirectional\n",
    "\n",
    "            if self.use_attention:\n",
    "                self.attention = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=rnn_units)\n",
    "\n",
    "            if use_GRU:\n",
    "                rnn = Bidirectional(GRU(rnn_units, return_sequences=use_attention)) if bidirectional else GRU(rnn_units,\n",
    "                                                                                                              return_sequences=use_attention)\n",
    "            else:\n",
    "                rnn = Bidirectional(LSTM(rnn_units, return_sequences=use_attention)) if bidirectional else LSTM(\n",
    "                    rnn_units, return_sequences=use_attention)\n",
    "            self.group_rnn = tf.keras.layers.TimeDistributed(rnn)\n",
    "\n",
    "            self.out_layer = Dense(summary_dim, activation=\"linear\")\n",
    "            self.summary_dim = summary_dim\n",
    "\n",
    "        def call(self, x, **kwargs):\n",
    "            \"\"\"Performs a forward pass through the network by first passing `x` through the same rnn network for\n",
    "            each household and then pooling the outputs across households.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            x : tf.Tensor\n",
    "                Input of shape (batch_size, n_groups, n_time_steps, n_features)\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            out : tf.Tensor\n",
    "                Output of shape (batch_size, summary_dim)\n",
    "            \"\"\"\n",
    "            # Apply the RNN to each group\n",
    "            out = self.group_conv(x, **kwargs)\n",
    "            out = self.group_rnn(out, **kwargs)  # (batch_size, n_groups, lstm_units)\n",
    "            # if attention is used, return full sequence (batch_size, n_groups, n_time_steps, lstm_units)\n",
    "            # bidirectional LSTM returns 2*lstm_units\n",
    "\n",
    "            if self.use_attention:\n",
    "                # learn a query vector to attend over the time points\n",
    "                query = tf.reduce_mean(out, axis=1)\n",
    "                # Reshape query to match the required shape for attention\n",
    "                query = tf.expand_dims(query, axis=1)  # (batch_size, 1, n_time_steps, lstm_units)\n",
    "                if not self.return_attention_weights:\n",
    "                    out = self.attention(query, out, **kwargs)  # (batch_size, 1, n_time_steps, lstm_units)\n",
    "                else:\n",
    "                    out, attention_weights = self.attention(query, out, return_attention_scores=True, **kwargs)\n",
    "                    attention_weights = tf.squeeze(attention_weights, axis=2)\n",
    "                out = tf.squeeze(out, axis=1)  # Remove the extra dimension (batch_size, n_time_steps, lstm_units)\n",
    "                out = self.pooling(out, **kwargs)  # (batch_size, 1, lstm_units)\n",
    "            else:\n",
    "                # pooling over groups, this totally invariants to the order of the groups\n",
    "                out = self.pooling(out, **kwargs)  # (batch_size, lstm_units)\n",
    "            # apply dense layer\n",
    "            out = self.out_layer(out, **kwargs)  # (batch_size, summary_dim)\n",
    "\n",
    "            if self.use_attention and self.return_attention_weights:\n",
    "                return out, attention_weights\n",
    "            return out\n",
    "\n",
    "        def get_config(self):\n",
    "            \"\"\"Return the config for serialization.\"\"\"\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'summary_dim': self.summary_dim,\n",
    "                'num_conv_layers': self.num_conv_layers,\n",
    "                'rnn_units': self.rnn_units,\n",
    "                'bidirectional': self.bidirectional,\n",
    "                'conv_settings': self.conv_settings,\n",
    "                'use_attention': self.use_attention,\n",
    "                'return_attention_weights': self.return_attention_weights,\n",
    "                'use_GRU': self.use_GRU,\n",
    "            })\n",
    "            return config\n",
    "\n",
    "        @classmethod\n",
    "        def from_config(cls, config):\n",
    "            \"\"\"Recreate the model from the config.\"\"\"\n",
    "            return cls(**config)\n",
    "\n",
    "    num_coupling_layers = 6\n",
    "    num_dense = 3\n",
    "    use_attention = True\n",
    "    use_bidirectional = True\n",
    "    summary_loss = 'MMD'\n",
    "    use_manual_summary = False\n",
    "    if model_id == 0:\n",
    "        checkpoint_path = 'amortizer-cell-migration-attention-6'\n",
    "    elif model_id == 1:\n",
    "        checkpoint_path = 'amortizer-cell-migration-attention-6-manual'\n",
    "        use_manual_summary = True\n",
    "    elif model_id == 2:\n",
    "        checkpoint_path = 'amortizer-cell-migration-attention-7'\n",
    "        num_coupling_layers = 7\n",
    "    elif model_id == 3:\n",
    "        checkpoint_path = 'amortizer-cell-migration-attention-7-manual'\n",
    "        num_coupling_layers = 7\n",
    "        use_manual_summary = True\n",
    "    elif model_id == 4:\n",
    "        checkpoint_path = 'amortizer-cell-migration-attention-8'\n",
    "        num_coupling_layers = 8\n",
    "    elif model_id == 5:\n",
    "        checkpoint_path = 'amortizer-cell-migration-attention-8-manual'\n",
    "        num_coupling_layers = 8\n",
    "        use_manual_summary = True\n",
    "    else:\n",
    "        raise ValueError('Checkpoint path not found')\n",
    "    os.makedirs(f\"../results/{checkpoint_path}\", exist_ok=True)\n",
    "\n",
    "    summary_net = GroupSummaryNetwork(summary_dim=len(obs_pars) * 2,\n",
    "                                      rnn_units=32,\n",
    "                                      use_attention=use_attention,\n",
    "                                      bidirectional=use_bidirectional)\n",
    "    inference_net = InvertibleNetwork(num_params=len(obs_pars),\n",
    "                                      num_coupling_layers=num_coupling_layers,\n",
    "                                      coupling_design='spline',\n",
    "                                      coupling_settings={\n",
    "                                          \"num_dense\": num_dense,\n",
    "                                          \"dense_args\": dict(\n",
    "                                              activation='relu',\n",
    "                                              kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                          ),\n",
    "                                          \"dropout_prob\": 0.2,\n",
    "                                          \"bins\": 16,\n",
    "                                      })\n",
    "\n",
    "    amortizer = AmortizedPosterior(inference_net=inference_net, summary_net=summary_net,\n",
    "                                   summary_loss_fun=summary_loss)\n",
    "\n",
    "    # Disable logging\n",
    "    logging.disable(logging.CRITICAL)\n",
    "\n",
    "    # build the trainer with networks\n",
    "    max_to_keep = 17\n",
    "    trainer = Trainer(amortizer=amortizer,\n",
    "                      configurator=partial(configurator,\n",
    "                                           manual_summary=use_manual_summary),\n",
    "                      checkpoint_path=checkpoint_path,\n",
    "                      skip_checks=True,\n",
    "                      max_to_keep=max_to_keep)\n",
    "\n",
    "    # check if file exist\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        trainer.load_pretrained_network()\n",
    "        history = trainer.loss_history.get_plottable()\n",
    "    else:\n",
    "        raise FileNotFoundError('Checkpoint path not found')\n",
    "\n",
    "    # Find the checkpoint with the lowest validation loss out of the last max_to_keep\n",
    "    recent_losses = history['val_losses'].iloc[-max_to_keep:]\n",
    "    best_valid_epoch = recent_losses['Loss'].idxmin() + 1  # checkpoints are 1-based indexed\n",
    "    new_checkpoint = trainer.manager.latest_checkpoint.rsplit('-', 1)[0] + f'-{best_valid_epoch}'\n",
    "    trainer.checkpoint.restore(new_checkpoint)\n",
    "    #print(\"Networks loaded from {}\".format(new_checkpoint))\n",
    "\n",
    "    # Re-enable logging\n",
    "    logging.disable(logging.NOTSET)\n",
    "\n",
    "    return summary_net, use_manual_summary"
   ],
   "id": "9e1bca8e9a7b3281",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "_ = load_model(5)",
   "id": "22acde4b06067b35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use trained neural net as summary statistics\n",
    "def make_sumstat_dict_nn(\n",
    "        data: Union[dict, np.ndarray],\n",
    "        config_map: callable,\n",
    ") -> dict:\n",
    "    if isinstance(data, dict):\n",
    "        # get key\n",
    "        key = list(data.keys())[0]\n",
    "        data = data[key]\n",
    "\n",
    "    summary_nn, manual_summary = load_model(5)\n",
    "\n",
    "    # configures the input for the network\n",
    "    config_input = config_map({\"sim_data\": data}, manual_summary=manual_summary)\n",
    "    # get the summary statistics\n",
    "    out_dict = {\n",
    "        'summary_net': summary_nn(config_input['summary_conditions']).numpy().flatten()\n",
    "    }\n",
    "    # if direct conditions are available, concatenate them\n",
    "    if 'direct_conditions' in config_input.keys():\n",
    "        out_dict['direct_conditions'] = config_input['direct_conditions'].flatten()\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "sumstats_nn = partial(make_sumstat_dict_nn,\n",
    "                      config_map=configurator)\n",
    "\n",
    "if on_cluster:\n",
    "    # define the model object\n",
    "    model_nn = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "    # todo: remember also change tiff path in model.xml!\n",
    "else:\n",
    "    # define the model object\n",
    "    model_nn = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)"
   ],
   "id": "a7c0835963566e42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "synthetic_data_test_nn = sumstats_nn(test_sim)\n",
    "synthetic_data_test_nn"
   ],
   "id": "e2e73120c64aa514",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#redis_sampler = RedisEvalParallelSampler(host=args.ip, port=args.port,\n",
    "#                                         adapt_look_ahead_proposal=False,\n",
    "#                                         look_ahead=False)\n",
    "\n",
    "abc_nn = pyabc.ABCSMC(model_nn, prior, # here we use now the Euclidean distance\n",
    "                      population_size=population_size,\n",
    "                      summary_statistics=sumstats_nn,\n",
    "                      sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",
    "                      #sampler=redis_sampler\n",
    "                      )\n",
    "\n",
    "db_path = os.path.join(gp, \"abc_results/synthetic_test_nn_sumstats.db\")\n",
    "\n",
    "if not os.path.exists(db_path):\n",
    "    history_nn = abc_nn.new(\"sqlite:///\" + db_path, make_sumstat_dict(test_sim))\n",
    "\n",
    "    # start the abc fitting\n",
    "    abc_nn.run(min_acceptance_rate=1e-2, max_nr_populations=30)\n",
    "    print('Done!')\n",
    "else:\n",
    "    history_nn = abc_nn.load(\"sqlite:///\" + db_path)"
   ],
   "id": "8522eceb33f5a581",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, len(param_names), tight_layout=True, figsize=(12, 4))\n",
    "for i, param in enumerate(param_names):\n",
    "    for t in range(history_nn.max_t + 1):\n",
    "        df, w = history_nn.get_distribution(m=0, t=t)\n",
    "        pyabc.visualization.plot_kde_1d(\n",
    "            df,\n",
    "            w,\n",
    "            xmin=limits_log[param][0],\n",
    "            xmax=limits_log[param][1],\n",
    "            x=param,\n",
    "            xname=log_param_names[i],\n",
    "            ax=ax[i],\n",
    "            label=f\"PDF t={t}\",\n",
    "        )\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlim((limits_log[param][0]-0.2, limits_log[param][1]+0.2))\n",
    "plt.savefig(os.path.join(gp, 'abc_results/population_kdes_nn.png'))\n",
    "plt.show()\n",
    "\n",
    "fig, arr_ax = plt.subplots(2, 3, figsize=(12, 6), tight_layout=True)\n",
    "arr_ax = arr_ax.flatten()\n",
    "pyabc.visualization.plot_sample_numbers(history_nn, ax=arr_ax[0])\n",
    "pyabc.visualization.plot_walltime(history_nn, ax=arr_ax[1], unit='h')\n",
    "pyabc.visualization.plot_epsilons(history_nn, ax=arr_ax[2])\n",
    "pyabc.visualization.plot_effective_sample_sizes(history_nn, ax=arr_ax[3])\n",
    "pyabc.visualization.plot_acceptance_rates_trajectory(history_nn, ax=arr_ax[4])\n",
    "# remove last axis\n",
    "arr_ax[-1].axis('off')\n",
    "plt.savefig(os.path.join(gp, 'abc_results/diagnostics_nn.png'))\n",
    "plt.show()\n",
    "\n",
    "pyabc.visualization.plot_credible_intervals(history_nn, levels=[0.95]);"
   ],
   "id": "82568acc7945ee86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "abc_df, abc_w = history_nn.get_distribution()\n",
    "abc_posterior_samples_nn = resample(abc_df[param_names].values, abc_w, n=1000)\n",
    "abc_nn_median = np.median(abc_posterior_samples_nn, axis=0)"
   ],
   "id": "b7d320f9bb9b7e8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.plot_posterior_2d(posterior_draws=abc_posterior_samples_nn,\n",
    "                                       prior_draws=prior_draws,\n",
    "                                       param_names=log_param_names)\n",
    "# from figure get axis\n",
    "ax = fig.get_axes()\n",
    "for i, a in enumerate(ax):\n",
    "    # plot only on the diagonal\n",
    "    if i == i // len(log_param_names) * (len(log_param_names)+1):\n",
    "        a.axvline(test_params[i // len(log_param_names)], color='b', label='True parameter')\n",
    "plt.savefig(os.path.join(gp, 'abc_results/posterior_vs_prior_nn_summary.png'))\n",
    "plt.show()"
   ],
   "id": "a2d2b093e81ddec9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get posterior samples and simulate\n",
    "if not os.path.exists(os.path.join(gp, 'abc_results/posterior_sim_nn.npy')):\n",
    "    # simulate the data\n",
    "    sim_list = []\n",
    "    for i in tqdm(range(10)):\n",
    "        if i == 0:\n",
    "            sim_dict = {key: p for key, p in zip(obs_pars.keys(), abc_nn_median)}\n",
    "        else:\n",
    "            sim_dict = {key: p for key, p in zip(obs_pars.keys(), abc_posterior_samples_nn[i])}\n",
    "        posterior_sim = model_nn(sim_dict)\n",
    "        sim_list.append(posterior_sim['sim'])\n",
    "    posterior_sim_nn = np.concatenate(sim_list)\n",
    "    np.save(os.path.join(gp, 'abc_results/posterior_sim_nn.npy'), posterior_sim_nn)\n",
    "else:\n",
    "    posterior_sim_nn = np.load(os.path.join(gp, 'abc_results/posterior_sim_nn.npy'))"
   ],
   "id": "1a52922e5befdd89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# plot the summary statistics\n",
    "plot_compare_summary_stats(test_sim, posterior_sim_nn, path='abc_results/Summary Stats NN.png');"
   ],
   "id": "2fc7d0bce16dd4f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# plot the trajectory\n",
    "plot_trajectory(test_sim[0], posterior_sim_nn[0], path='abc_results/Simulations NN.png')"
   ],
   "id": "dc00a034d0f425a0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
