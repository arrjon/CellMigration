{
 "cells": [
  {
   "cell_type": "code",
   "id": "b3292da9556a2a68",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import argparse\n",
    "import os\n",
    "import pickle\n",
    "from typing import Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pyabc\n",
    "#from pyabc.sampler import RedisEvalParallelSampler\n",
    "import scipy.stats as stats\n",
    "from fitmulticell import model as morpheus_model\n",
    "from fitmulticell.sumstat import SummaryStatistics\n",
    "from tqdm import tqdm\n",
    "\n",
    "from synth_data_params_bayesflow.load_bayesflow_model import load_model\n",
    "from synth_data_params_bayesflow.plotting_routines import plot_compare_summary_stats, plot_trajectory, \\\n",
    "    plot_autocorrelation\n",
    "from synth_data_params_bayesflow.summary_stats import reduced_coordinates_to_sumstat, reduce_to_coordinates, \\\n",
    "    compute_mean_summary_stats\n",
    "\n",
    "# get the job array id and number of processors\n",
    "job_array_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0))\n",
    "n_procs = int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",
    "print(job_array_id)\n",
    "on_cluster = False\n",
    "population_size = 1000\n",
    "load_synthetic_data = True\n",
    "\n",
    "if on_cluster:\n",
    "    parser = argparse.ArgumentParser(description='Parse necessary arguments')\n",
    "    parser.add_argument('-pt', '--port', type=str, default=\"50004\",\n",
    "                        help='Which port should be use?')\n",
    "    parser.add_argument('-ip', '--ip', type=str,\n",
    "                        help='Dynamically passed - BW: Login Node 3')\n",
    "    args = parser.parse_args()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if on_cluster:\n",
    "    gp = '/home/jarruda_hpc/CellMigration/synth_data_params_bayesflow'\n",
    "else:\n",
    "    gp = os.getcwd()\n",
    "\n",
    "par_map = {\n",
    "    'gradient_strength': './CellTypes/CellType/Constant[@symbol=\"gradient_strength\"]',\n",
    "    'move.strength': './CellTypes/CellType/Constant[@symbol=\"move.strength\"]',\n",
    "    'move.duration.mean': './CellTypes/CellType/Constant[@symbol=\"move.duration.mean\"]',\n",
    "    'cell_nodes_real': './Global/Constant[@symbol=\"cell_nodes_real\"]',\n",
    "}\n",
    "\n",
    "model_path = gp + \"/cell_movement_v24.xml\"  # time step is 30sec, move.dir completely normalized, init move.dir rand in all directions\n",
    "# defining the summary statistics function\n",
    "max_sequence_length = 120\n",
    "min_sequence_length = 0\n",
    "only_longest_traj_per_cell = True  # mainly to keep the data batchable\n",
    "cells_in_population = 50\n",
    "\n",
    "\n",
    "def make_sumstat_dict(data: Union[dict, np.ndarray]) -> dict:\n",
    "    if isinstance(data, dict):\n",
    "        # get key\n",
    "        key = list(data.keys())[0]\n",
    "        data = data[key]\n",
    "    data = data[0]  # only one sample\n",
    "    # compute the summary statistics\n",
    "    summary_stats_dict = reduced_coordinates_to_sumstat(data)\n",
    "    (ad_mean, _, msd_mean, _, ta_mean, _, vel_mean, _, wt_mean, _) = compute_mean_summary_stats([summary_stats_dict], remove_nan=False)\n",
    "    cleaned_dict = {\n",
    "        'ad': np.array(ad_mean).flatten(),\n",
    "        'msd': np.array(msd_mean).flatten(),\n",
    "        'ta': np.array(ta_mean).flatten(),\n",
    "        'vel': np.array(vel_mean).flatten(),\n",
    "        'wt': np.array(wt_mean).flatten()\n",
    "    }\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "def prepare_sumstats(output_morpheus_model) -> dict:\n",
    "    sim_coordinates = reduce_to_coordinates(output_morpheus_model, \n",
    "                          minimal_length=min_sequence_length, \n",
    "                          maximal_length=max_sequence_length,\n",
    "                          only_longest_traj_per_cell=only_longest_traj_per_cell\n",
    "                          )\n",
    "    \n",
    "    # we now do exactly the same as in the BayesFlow workflow, but here we get only one sample at a time\n",
    "    data_transformed = np.ones((1, cells_in_population, max_sequence_length, 2)) * np.nan\n",
    "    # each cell is of different length, each with x and y coordinates, make a tensor out of it\n",
    "    n_cells_not_visible = 0\n",
    "    if len(sim_coordinates) != 0:\n",
    "        # some cells were visible in the simulation\n",
    "        for c_id, cell_sim in enumerate(sim_coordinates):\n",
    "            # pre-pad the data with zeros, but first write zeros as nans to compute the mean and std\n",
    "            data_transformed[0, c_id, -len(cell_sim['x']):, 0] = cell_sim['x']\n",
    "            data_transformed[0, c_id, -len(cell_sim['y']):, 1] = cell_sim['y']\n",
    "    \n",
    "    return {'sim': data_transformed}\n",
    "\n",
    "\n",
    "sumstat = SummaryStatistics(sum_stat_calculator=prepare_sumstats)                    \n",
    "\n",
    "if on_cluster:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "    # todo: remember also change tiff path in model.xml!\n",
    "else:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "# parameter values used to generate the synthetic data\n",
    "obs_pars = {\n",
    "    'gradient_strength': 100.,  # strength of the gradient of chemotaxis\n",
    "    'move.strength': 10.,  # strength of directed motion\n",
    "    'move.duration.mean': 0.1,  # mean of exponential distribution (1/seconds)\n",
    "    'cell_nodes_real': 50.,  # volume of the cell\n",
    "}\n",
    "\n",
    "\n",
    "obs_pars_log = {key: np.log10(val) for key, val in obs_pars.items()}\n",
    "limits = {'gradient_strength': (1, 10000), #(10 ** 4, 10 ** 8),\n",
    "          'move.strength': (1, 100),\n",
    "          'move.duration.mean': (1e-4, 30), #(math.log10((10 ** -2) * 30), math.log10((10 ** 4))), # smallest time step in simulation 5\n",
    "          'cell_nodes_real': (1, 300)}\n",
    "limits_log = {key: (np.log10(val[0]), np.log10(val[1])) for key, val in limits.items()}\n",
    "\n",
    "\n",
    "prior = pyabc.Distribution(**{key: pyabc.RV(\"uniform\", loc=lb, scale=ub-lb)\n",
    "                              for key, (lb, ub) in limits_log.items()})\n",
    "param_names = list(obs_pars.keys())\n",
    "log_param_names = [f'log_{p}' for p in param_names]\n",
    "print(obs_pars)"
   ],
   "id": "74390dfb0e802a77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sigma0 = 550\n",
    "space_x0 = 1173/2\n",
    "space_y0 = 1500/1.31/2\n",
    "x0, y0 = 1173/2, (1500+1500/2+270)/1.31\n",
    "u1 = lambda space_x, space_y: 7/(2*np.pi*(sigma0**2)) *np.exp(-1/2*(((space_x)-(x0))**2+ ((space_y)-(y0))**2)/(sigma0**2))\n",
    "\n",
    "# plot the function\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "x = np.linspace(0, 1173 , 100)\n",
    "y = np.linspace(0, 2500 , 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = u1(X, Y)\n",
    "ax.plot_surface(X, Y, Z, alpha=0.5)\n",
    "# plot start points\n",
    "ax.scatter(space_x0, space_y0, u1(space_x0, space_y0), color='r', s=100)\n",
    "\n",
    "ax.set_xlabel('space_x')\n",
    "ax.set_ylabel('space_y')\n",
    "plt.show()\n",
    "\n",
    "space_x0, space_y0, u1(space_x0, space_y0), u1(x0, y0)"
   ],
   "id": "be4eaa8d21c3bfff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if load_synthetic_data:\n",
    "    # simulate test data\n",
    "    test_params = np.array(list(obs_pars_log.values()))\n",
    "    if not os.path.exists(os.path.join(gp, 'test_sim.npy')):\n",
    "        raise FileNotFoundError('Test data not found')\n",
    "    else:\n",
    "        test_sim = np.load(os.path.join(gp, 'test_sim.npy'))\n",
    "        \n",
    "    results_path = 'abc_results'\n",
    "else:\n",
    "    # load real data\n",
    "    from load_data import load_real_data\n",
    "    real_data, real_data_full = load_real_data(data_id=1, \n",
    "                                               max_sequence_length=max_sequence_length, \n",
    "                                               cells_in_population=cells_in_population)\n",
    "    test_sim = np.array([real_data[start:start+cells_in_population] for start in range(0, len(real_data), cells_in_population)])[0][np.newaxis]\n",
    "    results_path = 'abc_results_real'\n",
    "test_sim.shape"
   ],
   "id": "dc31691dc1bbede7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def obj_func_wass(sim: dict, obs: dict):\n",
    "    total = 0\n",
    "    for key in sim:\n",
    "        x, y = np.array(sim[key]), np.array(obs[key])\n",
    "        if x.size == 0:\n",
    "            return np.inf\n",
    "        total += stats.wasserstein_distance(x, y)\n",
    "    return total"
   ],
   "id": "1145158513a20ad6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#redis_sampler = RedisEvalParallelSampler(host=args.ip, port=args.port,\n",
    "#                                         adapt_look_ahead_proposal=False,\n",
    "#                                         look_ahead=False)\n",
    "\n",
    "abc = pyabc.ABCSMC(model, prior,\n",
    "                   distance_function=obj_func_wass,\n",
    "                   summary_statistics=make_sumstat_dict,\n",
    "                   population_size=population_size,\n",
    "                   sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",
    "                   #sampler=redis_sampler\n",
    "                   )\n",
    "\n",
    "db_path = os.path.join(gp, f\"{results_path}/{'synthetic' if load_synthetic_data else 'real'}_test_old_sumstats.db\")\n",
    "\n",
    "if not os.path.exists(db_path):\n",
    "    history = abc.new(\"sqlite:///\" + db_path, make_sumstat_dict(test_sim))\n",
    "\n",
    "    # start the abc fitting\n",
    "    abc.run(min_acceptance_rate=1e-2, max_nr_populations=30)\n",
    "    print('Done!')\n",
    "else:\n",
    "    history = abc.load(\"sqlite:///\" + db_path)"
   ],
   "id": "1636e7efc136f2e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, len(param_names), tight_layout=True, figsize=(12, 4))\n",
    "for i, param in enumerate(param_names):\n",
    "    for t in range(history.max_t + 1):\n",
    "        df, w = history.get_distribution(m=0, t=t)\n",
    "        pyabc.visualization.plot_kde_1d(\n",
    "            df,\n",
    "            w,\n",
    "            xmin=limits_log[param][0],\n",
    "            xmax=limits_log[param][1],\n",
    "            x=param,\n",
    "            xname=log_param_names[i],\n",
    "            ax=ax[i],\n",
    "            label=f\"PDF t={t}\",\n",
    "        )\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlim((limits_log[param][0]-0.2, limits_log[param][1]+0.2))\n",
    "plt.savefig(os.path.join(gp, f'{results_path}/population_kdes.png'))\n",
    "plt.show()\n",
    "\n",
    "fig, arr_ax = plt.subplots(2, 3, figsize=(12, 6), tight_layout=True)\n",
    "arr_ax = arr_ax.flatten()\n",
    "pyabc.visualization.plot_sample_numbers(history, ax=arr_ax[0])\n",
    "pyabc.visualization.plot_walltime(history, ax=arr_ax[1], unit='h')\n",
    "pyabc.visualization.plot_epsilons(history, ax=arr_ax[2])\n",
    "pyabc.visualization.plot_effective_sample_sizes(history, ax=arr_ax[3])\n",
    "pyabc.visualization.plot_acceptance_rates_trajectory(history, ax=arr_ax[4])\n",
    "# remove last axis\n",
    "arr_ax[-1].axis('off')\n",
    "plt.savefig(os.path.join(gp, f'{results_path}/diagnostics.png'))\n",
    "plt.show()\n",
    "\n",
    "pyabc.visualization.plot_credible_intervals(history, levels=[0.95]);"
   ],
   "id": "efcbb86db464a88d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def resample(points, weights, n):\n",
    "    \"\"\"\n",
    "    Resample from weighted samples.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    points : np.ndarray\n",
    "        The random samples, can be of higher dimensions. Sampling is done along axis 0.\n",
    "    weights : np.ndarray\n",
    "        Weights of each sample point.\n",
    "    n : int\n",
    "        Number of samples to resample.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    resampled : np.ndarray\n",
    "        A total of `n` points sampled from `points` with replacement\n",
    "        according to `weights`.\n",
    "    \"\"\"\n",
    "    weights = np.asarray(weights)\n",
    "    weights /= np.sum(weights)\n",
    "    indices = np.random.choice(points.shape[0], size=n, p=weights)\n",
    "    resampled = points[indices]\n",
    "    return resampled"
   ],
   "id": "ffb1c5cd5498088a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "abc_df, abc_w = history.get_distribution()\n",
    "abc_posterior_samples = resample(abc_df[param_names].values, abc_w, n=1000)\n",
    "abc_median = np.median(abc_posterior_samples, axis=0)"
   ],
   "id": "3367ada60af6a8fa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import bayesflow as bf",
   "id": "c606f75e3a4c0257",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "prior_draws = np.array([list(prior.rvs().values()) for _ in range(abc_posterior_samples.shape[0])])\n",
    "fig = bf.diagnostics.plot_posterior_2d(posterior_draws=abc_posterior_samples,\n",
    "                                       prior_draws=prior_draws,\n",
    "                                       param_names=log_param_names)\n",
    "# from figure get axis\n",
    "ax = fig.get_axes()\n",
    "for i, a in enumerate(ax):\n",
    "    # plot only on the diagonal\n",
    "    if i == i // len(log_param_names) * (len(log_param_names)+1):\n",
    "        a.axvline(test_params[i // len(log_param_names)], color='r', label='True parameter')\n",
    "for i, a in enumerate(ax):\n",
    "    # plot only on the diagonal\n",
    "    if i == i // len(log_param_names) * (len(log_param_names)+1):\n",
    "        a.axvline(abc_median[i // len(log_param_names)], color='b', label='Median parameter')\n",
    "plt.savefig(os.path.join(gp, f'{results_path}/posterior_vs_prior.png'))\n",
    "plt.show()"
   ],
   "id": "98dbfbc06679152c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# load bayesflow posterior samples\n",
    "if load_synthetic_data:\n",
    "    bayesflow_posterior_samples = np.load(f'amortizer-cell-migration-attention-8-manual/posterior_samples_synthetic.npy')\n",
    "else:\n",
    "    bayesflow_posterior_samples = np.load(f'amortizer-cell-migration-attention-8-manual/posterior_samples_real.npy')\n",
    "bayesflow_median = np.median(bayesflow_posterior_samples, axis=0)"
   ],
   "id": "7be518a8fcd43e45",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.plot_posterior_2d(posterior_draws=bayesflow_posterior_samples,\n",
    "                                       prior_draws=abc_posterior_samples[:bayesflow_posterior_samples.shape[0]],\n",
    "                                       param_names=log_param_names)\n",
    "# from figure get axis\n",
    "ax = fig.get_axes()\n",
    "for i, a in enumerate(ax):\n",
    "    # plot only on the diagonal\n",
    "    if i == i // len(log_param_names) * (len(log_param_names)+1):\n",
    "        a.axvline(test_params[i // len(log_param_names)], color='r', label='True parameter')\n",
    "        a.legend(loc='upper left')\n",
    "plt.savefig(os.path.join(gp, f'{results_path}/posterior_abc_vs_posterior_bayesflow.png'))\n",
    "plt.show()"
   ],
   "id": "a693bd436f12fcc1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get posterior samples and simulate\n",
    "if not os.path.exists(os.path.join(gp, f'{results_path}/posterior_sim.npy')):\n",
    "    # simulate the data\n",
    "    sim_list = []\n",
    "    for i in tqdm(range(10)):\n",
    "        if i == 0:\n",
    "            sim_dict = {key: p for key, p in zip(obs_pars.keys(), abc_median)}\n",
    "        else:\n",
    "            sim_dict = {key: p for key, p in zip(obs_pars.keys(), abc_posterior_samples[i])}\n",
    "        posterior_sim = model(sim_dict)\n",
    "        sim_list.append(posterior_sim['sim'])\n",
    "    posterior_sim = np.concatenate(sim_list)\n",
    "    np.save(os.path.join(gp, f'{results_path}/posterior_sim.npy'), posterior_sim)\n",
    "else:\n",
    "    #posterior_sim = np.load(os.path.join(gp, f'{results_path}/posterior_sim_prior.npy'))\n",
    "    posterior_sim = np.load(os.path.join(gp, f'{results_path}/posterior_sim.npy'))"
   ],
   "id": "e3a0b55a7408e3f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the summary statistics\n",
    "plot_compare_summary_stats(test_sim, posterior_sim, path=f'{results_path}/Summary Stats');"
   ],
   "id": "dc4ea826fa0f51e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the trajectory\n",
    "plot_trajectory(test_sim[0], posterior_sim[0], path=f'{results_path}/Simulations', show_umap=True)\n",
    "plot_autocorrelation(test_sim[0], posterior_sim[0], path=f'{results_path}/Autocorrelation')"
   ],
   "id": "dde7247a8ea083d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.exists(os.path.join(gp, 'validation_data.pickle')):\n",
    "    with open(os.path.join(gp, 'validation_data.pickle'), 'rb') as f:\n",
    "        valid_data = pickle.load(f)\n",
    "else:\n",
    "    raise FileNotFoundError('Validation data not found')\n",
    "\n",
    "x_mean = np.nanmean(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "x_std = np.nanstd(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "p_mean = np.mean(valid_data['prior_draws'], axis=0)\n",
    "p_std = np.std(valid_data['prior_draws'], axis=0)\n",
    "print('Mean and std of data:', x_mean, x_std)\n",
    "print('Mean and std of parameters:', p_mean, p_std)\n",
    "\n",
    "\n",
    "# compute the mean of the summary statistics\n",
    "summary_stats_list_ = [reduced_coordinates_to_sumstat(t) for t in valid_data['sim_data']]\n",
    "(_, ad_averg, _, MSD_averg, _, TA_averg, _, VEL_averg, _, WT_averg) = compute_mean_summary_stats(summary_stats_list_, remove_nan=False)\n",
    "\n",
    "direct_conditions_ = np.stack([ad_averg, MSD_averg, TA_averg, VEL_averg, WT_averg]).T\n",
    "# replace inf with -1\n",
    "direct_conditions_[np.isinf(direct_conditions_)] = np.nan\n",
    "        \n",
    "summary_valid_max = np.nanmax(direct_conditions_, axis=0)\n",
    "summary_valid_min = np.nanmin(direct_conditions_, axis=0)"
   ],
   "id": "df1829ce49462436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use trained neural net as summary statistics\n",
    "def make_sumstat_dict_nn(\n",
    "        data: Union[dict, np.ndarray],\n",
    ") -> dict:\n",
    "    if isinstance(data, dict):\n",
    "        # get key\n",
    "        key = list(data.keys())[0]\n",
    "        data = data[key]\n",
    "\n",
    "    trainer, map_idx_sim = load_model(\n",
    "        model_id=5, \n",
    "        x_mean=x_mean,\n",
    "        x_std=x_std,\n",
    "        p_mean=p_mean,\n",
    "        p_std=p_std,\n",
    "        summary_valid_max=summary_valid_max,\n",
    "        summary_valid_min=summary_valid_min,\n",
    "    )\n",
    "\n",
    "    # configures the input for the network\n",
    "    config_input = trainer.configurator({\"sim_data\": data})\n",
    "    # get the summary statistics\n",
    "    out_dict = {\n",
    "        'summary_net': trainer.amortizer.summary_net(config_input['summary_conditions']).numpy().flatten()\n",
    "    }\n",
    "    # if direct conditions are available, concatenate them\n",
    "    if 'direct_conditions' in config_input.keys():\n",
    "        out_dict['direct_conditions'] = config_input['direct_conditions'].flatten()\n",
    "        \n",
    "    del trainer\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "if on_cluster:\n",
    "    # define the model object\n",
    "    model_nn = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "    # todo: remember also change tiff path in model.xml!\n",
    "else:\n",
    "    # define the model object\n",
    "    model_nn = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)"
   ],
   "id": "a7c0835963566e42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "synthetic_data_test_nn = make_sumstat_dict_nn(test_sim)\n",
    "synthetic_data_test_nn"
   ],
   "id": "e2e73120c64aa514",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#redis_sampler = RedisEvalParallelSampler(host=args.ip, port=args.port,\n",
    "#                                         adapt_look_ahead_proposal=False,\n",
    "#                                         look_ahead=False)\n",
    "\n",
    "abc_nn = pyabc.ABCSMC(model_nn, prior, # here we use now the Euclidean distance\n",
    "                      population_size=population_size,\n",
    "                      summary_statistics=make_sumstat_dict_nn,\n",
    "                      sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",
    "                      #sampler=redis_sampler\n",
    "                      )\n",
    "\n",
    "db_path = os.path.join(gp, f\"{results_path}/{'synthetic' if load_synthetic_data else 'real'}_test_nn_sumstats.db\")\n",
    "\n",
    "if not os.path.exists(db_path):\n",
    "    history_nn = abc_nn.new(\"sqlite:///\" + db_path, make_sumstat_dict_nn(test_sim))\n",
    "\n",
    "    # start the abc fitting\n",
    "    abc_nn.run(min_acceptance_rate=1e-2, max_nr_populations=30)\n",
    "    print('Done!')\n",
    "else:\n",
    "    history_nn = abc_nn.load(\"sqlite:///\" + db_path)"
   ],
   "id": "8522eceb33f5a581",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, len(param_names), tight_layout=True, figsize=(12, 4))\n",
    "for i, param in enumerate(param_names):\n",
    "    for t in range(history_nn.max_t + 1):\n",
    "        df, w = history_nn.get_distribution(m=0, t=t)\n",
    "        pyabc.visualization.plot_kde_1d(\n",
    "            df,\n",
    "            w,\n",
    "            xmin=limits_log[param][0],\n",
    "            xmax=limits_log[param][1],\n",
    "            x=param,\n",
    "            xname=log_param_names[i],\n",
    "            ax=ax[i],\n",
    "            label=f\"PDF t={t}\",\n",
    "        )\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlim((limits_log[param][0]-0.2, limits_log[param][1]+0.2))\n",
    "plt.savefig(os.path.join(gp, f'{results_path}/population_kdes_nn.png'))\n",
    "plt.show()\n",
    "\n",
    "fig, arr_ax = plt.subplots(2, 3, figsize=(12, 6), tight_layout=True)\n",
    "arr_ax = arr_ax.flatten()\n",
    "pyabc.visualization.plot_sample_numbers(history_nn, ax=arr_ax[0])\n",
    "pyabc.visualization.plot_walltime(history_nn, ax=arr_ax[1], unit='h')\n",
    "pyabc.visualization.plot_epsilons(history_nn, ax=arr_ax[2])\n",
    "pyabc.visualization.plot_effective_sample_sizes(history_nn, ax=arr_ax[3])\n",
    "pyabc.visualization.plot_acceptance_rates_trajectory(history_nn, ax=arr_ax[4])\n",
    "# remove last axis\n",
    "arr_ax[-1].axis('off')\n",
    "plt.savefig(os.path.join(gp, f'{results_path}/diagnostics_nn.png'))\n",
    "plt.show()\n",
    "\n",
    "pyabc.visualization.plot_credible_intervals(history_nn, levels=[0.95]);"
   ],
   "id": "82568acc7945ee86",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "abc_df, abc_w = history_nn.get_distribution()\n",
    "abc_posterior_samples_nn = resample(abc_df[param_names].values, abc_w, n=1000)\n",
    "abc_nn_median = np.median(abc_posterior_samples_nn, axis=0)"
   ],
   "id": "b7d320f9bb9b7e8c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.plot_posterior_2d(posterior_draws=abc_posterior_samples_nn,\n",
    "                                       prior_draws=prior_draws,\n",
    "                                       param_names=log_param_names)\n",
    "# from figure get axis\n",
    "ax = fig.get_axes()\n",
    "for i, a in enumerate(ax):\n",
    "    # plot only on the diagonal\n",
    "    if i == i // len(log_param_names) * (len(log_param_names)+1):\n",
    "        a.axvline(test_params[i // len(log_param_names)], color='r', label='True parameter')\n",
    "plt.savefig(os.path.join(gp, f'{results_path}/posterior_vs_prior_nn_summary.png'))\n",
    "plt.show()"
   ],
   "id": "a2d2b093e81ddec9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = bf.diagnostics.plot_posterior_2d(posterior_draws=bayesflow_posterior_samples,\n",
    "                                       prior_draws=abc_posterior_samples_nn[:bayesflow_posterior_samples.shape[0]],\n",
    "                                       param_names=log_param_names)\n",
    "# from figure get axis\n",
    "ax = fig.get_axes()\n",
    "for i, a in enumerate(ax):\n",
    "    # plot only on the diagonal\n",
    "    if i == i // len(log_param_names) * (len(log_param_names)+1):\n",
    "        a.axvline(test_params[i // len(log_param_names)], color='r', label='True parameter')\n",
    "        a.legend(loc='upper left')\n",
    "plt.savefig(os.path.join(gp, f'{results_path}/posterior_abc_nn_vs_posterior_bayesflow.png'))\n",
    "plt.show()"
   ],
   "id": "aff74a7eeeaf6a79",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# get posterior samples and simulate\n",
    "if not os.path.exists(os.path.join(gp, f'{results_path}/posterior_sim_nn.npy')):\n",
    "    # simulate the data\n",
    "    sim_list = []\n",
    "    for i in tqdm(range(10)):\n",
    "        if i == 0:\n",
    "            sim_dict = {key: p for key, p in zip(obs_pars.keys(), abc_nn_median)}\n",
    "        else:\n",
    "            sim_dict = {key: p for key, p in zip(obs_pars.keys(), abc_posterior_samples_nn[i])}\n",
    "        posterior_sim = model_nn(sim_dict)\n",
    "        sim_list.append(posterior_sim['sim'])\n",
    "    posterior_sim_nn = np.concatenate(sim_list)\n",
    "    np.save(os.path.join(gp, f'{results_path}/posterior_sim_nn.npy'), posterior_sim_nn)\n",
    "else:\n",
    "    posterior_sim_nn = np.load(os.path.join(gp, f'{results_path}/posterior_sim_nn.npy'))"
   ],
   "id": "1a52922e5befdd89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the summary statistics\n",
    "plot_compare_summary_stats(test_sim, posterior_sim_nn, path=f'{results_path}/Summary Stats NN');"
   ],
   "id": "2fc7d0bce16dd4f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the trajectory\n",
    "plot_trajectory(test_sim[0], posterior_sim_nn[0], path=f'{results_path}/Simulations NN')\n",
    "plot_autocorrelation(test_sim[0], posterior_sim_nn[0], path=f'{results_path}/Autocorrelation NN')"
   ],
   "id": "dc00a034d0f425a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "bc4c420251f3f3c8",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
