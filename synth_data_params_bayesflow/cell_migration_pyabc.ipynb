{
 "cells": [
  {
   "cell_type": "code",
   "id": "b3292da9556a2a68",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import pickle\n",
    "from functools import partial\n",
    "import tempfile\n",
    "from typing import Union\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pyabc\n",
    "from pyabc.sampler import RedisEvalParallelSampler\n",
    "import scipy.stats as stats\n",
    "from fitmulticell import model as morpheus_model\n",
    "from fitmulticell.sumstat import SummaryStatistics\n",
    "\n",
    "from summary_stats import reduced_coordinates_to_sumstat, reduce_to_coordinates, compute_mean_summary_stats\n",
    "\n",
    "# get the job array id and number of processors\n",
    "job_array_id = int(os.environ.get('SLURM_ARRAY_TASK_ID', 0))\n",
    "n_procs = int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",
    "print(job_array_id)\n",
    "on_cluster = False\n",
    "population_size = 1000\n",
    "run_old_sumstats = True\n",
    "\n",
    "if job_array_id == 1:\n",
    "    run_old_sumstats = False\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Parse necessary arguments')\n",
    "parser.add_argument('-pt', '--port', type=str, default=\"50004\",\n",
    "                    help='Which port should be use?')\n",
    "parser.add_argument('-ip', '--ip', type=str,\n",
    "                    help='Dynamically passed - BW: Login Node 3')\n",
    "args = parser.parse_args()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if on_cluster:\n",
    "    gp = '/home/jarruda_hpc/CellMigration/synth_data_params_bayesflow'\n",
    "else:\n",
    "    gp = os.getcwd()\n",
    "\n",
    "par_map = {\n",
    "    'gradient_strength': './CellTypes/CellType/Constant[@symbol=\"gradient_strength\"]',\n",
    "    'move.strength': './CellTypes/CellType/Constant[@symbol=\"move.strength\"]',\n",
    "    'move.duration.mean': './CellTypes/CellType/Constant[@symbol=\"move.duration.mean\"]',\n",
    "    'cell_nodes_real': './Global/Constant[@symbol=\"cell_nodes_real\"]',\n",
    "}\n",
    "\n",
    "model_path = gp + \"/cell_movement_v24.xml\"  # time step is 30sec, move.dir completely normalized, init move.dir rand in all directions\n",
    "# defining the summary statistics function\n",
    "max_sequence_length = 120\n",
    "min_sequence_length = 0\n",
    "only_longest_traj_per_cell = True  # mainly to keep the data batchable\n",
    "cells_in_population = 50\n",
    "\n",
    "\n",
    "def make_sumstat_dict(data: Union[dict, np.ndarray]) -> dict:\n",
    "    if isinstance(data, dict):\n",
    "        # get key\n",
    "        key = list(data.keys())[0]\n",
    "        data = data[key]\n",
    "    data = data[0]  # only one sample\n",
    "    # compute the summary statistics\n",
    "    summary_stats_dict = reduced_coordinates_to_sumstat(data)\n",
    "    (ad_mean, _, msd_mean, _, ta_mean, _, vel_mean, _, wt_mean, _) = compute_mean_summary_stats([summary_stats_dict], remove_nan=False)\n",
    "    cleaned_dict = {\n",
    "        'ad': np.array(ad_mean).flatten(),\n",
    "        'msd': np.array(msd_mean).flatten(),\n",
    "        'ta': np.array(ta_mean).flatten(),\n",
    "        'vel': np.array(vel_mean).flatten(),\n",
    "        'wt': np.array(wt_mean).flatten()\n",
    "    }\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "def prepare_sumstats(output_morpheus_model) -> dict:\n",
    "    sim_coordinates = reduce_to_coordinates(output_morpheus_model, \n",
    "                          minimal_length=min_sequence_length, \n",
    "                          maximal_length=max_sequence_length,\n",
    "                          only_longest_traj_per_cell=only_longest_traj_per_cell\n",
    "                          )\n",
    "    \n",
    "    # we now do exactly the same as in the BayesFlow workflow, but here we get only one sample at a time\n",
    "    data_transformed = np.ones((1, cells_in_population, max_sequence_length, 2)) * np.nan\n",
    "    # each cell is of different length, each with x and y coordinates, make a tensor out of it\n",
    "    n_cells_not_visible = 0\n",
    "    if len(sim_coordinates) != 0:\n",
    "        # some cells were visible in the simulation\n",
    "        for c_id, cell_sim in enumerate(sim_coordinates):\n",
    "            # pre-pad the data with zeros, but first write zeros as nans to compute the mean and std\n",
    "            data_transformed[0, c_id, -len(cell_sim['x']):, 0] = cell_sim['x']\n",
    "            data_transformed[0, c_id, -len(cell_sim['y']):, 1] = cell_sim['y']\n",
    "    \n",
    "    return {'sim': data_transformed}\n",
    "\n",
    "\n",
    "sumstat = SummaryStatistics(sum_stat_calculator=prepare_sumstats)                    \n",
    "\n",
    "if on_cluster:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "    # todo: remember also change tiff path in model.xml!\n",
    "else:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "# parameter values used to generate the synthetic data\n",
    "obs_pars = {\n",
    "    'gradient_strength': 100.,  # strength of the gradient of chemotaxis\n",
    "    'move.strength': 10.,  # strength of directed motion\n",
    "    'move.duration.mean': 0.1,  # mean of exponential distribution (1/seconds)\n",
    "    'cell_nodes_real': 50.,  # volume of the cell\n",
    "}\n",
    "\n",
    "\n",
    "obs_pars_log = {key: np.log10(val) for key, val in obs_pars.items()}\n",
    "limits = {'gradient_strength': (1, 10000), #(10 ** 4, 10 ** 8),\n",
    "          'move.strength': (1, 100),\n",
    "          'move.duration.mean': (1e-4, 30), #(math.log10((10 ** -2) * 30), math.log10((10 ** 4))), # smallest time step in simulation 5\n",
    "          'cell_nodes_real': (1, 300)}\n",
    "limits_log = {key: (np.log10(val[0]), np.log10(val[1])) for key, val in limits.items()}\n",
    "\n",
    "\n",
    "prior = pyabc.Distribution(**{key: pyabc.RV(\"uniform\", lb, ub)\n",
    "                              for key, (lb, ub) in limits_log.items()})\n",
    "param_names = list(obs_pars.keys())\n",
    "print(obs_pars)"
   ],
   "id": "74390dfb0e802a77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "sigma0 = 550\n",
    "space_x0 = 1173/2\n",
    "space_y0 = 1500/1.31/2\n",
    "x0, y0 = 1173/2, (1500+1500/2+270)/1.31\n",
    "u1 = lambda space_x, space_y: 7/(2*np.pi*(sigma0**2)) *np.exp(-1/2*(((space_x)-(x0))**2+ ((space_y)-(y0))**2)/(sigma0**2))\n",
    "\n",
    "# plot the function\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "x = np.linspace(0, 1173 , 100)\n",
    "y = np.linspace(0, 2500 , 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = u1(X, Y)\n",
    "ax.plot_surface(X, Y, Z, alpha=0.5)\n",
    "# plot start points\n",
    "ax.scatter(space_x0, space_y0, u1(space_x0, space_y0), color='r', s=100)\n",
    "\n",
    "ax.set_xlabel('space_x')\n",
    "ax.set_ylabel('space_y')\n",
    "plt.show()\n",
    "\n",
    "space_x0, space_y0, u1(space_x0, space_y0), u1(x0, y0)"
   ],
   "id": "be4eaa8d21c3bfff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# simulate test data\n",
    "test_params = np.array(list(obs_pars_log.values()))\n",
    "if not os.path.exists(os.path.join(gp, 'test_sim.npy')):\n",
    "    raise FileNotFoundError('Test data not found')\n",
    "else:\n",
    "    test_sim = np.load(os.path.join(gp, 'test_sim.npy'))"
   ],
   "id": "dc31691dc1bbede7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def obj_func_wass(sim: dict, obs: dict):\n",
    "    total = 0\n",
    "    for key in sim:\n",
    "        x, y = np.array(sim[key]), np.array(obs[key])\n",
    "        if x.size == 0:\n",
    "            return np.inf\n",
    "        total += stats.wasserstein_distance(x, y)\n",
    "    return total"
   ],
   "id": "1145158513a20ad6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if run_old_sumstats:\n",
    "    redis_sampler = RedisEvalParallelSampler(host=args.ip, port=args.port,\n",
    "                                             adapt_look_ahead_proposal=False,\n",
    "                                             look_ahead=False)\n",
    "\n",
    "    abc = pyabc.ABCSMC(model, prior,\n",
    "                       distance_function=obj_func_wass,\n",
    "                       summary_statistics=make_sumstat_dict,\n",
    "                       population_size=population_size,\n",
    "                       #sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",
    "                       sampler=redis_sampler\n",
    "                       )\n",
    "\n",
    "    db_path = os.path.join(tempfile.gettempdir(), \"test.db\")\n",
    "    #db_path = os.path.join(gp, \"synthetic_test_old_sumstats.db\")\n",
    "    history = abc.new(\"sqlite:///\" + db_path, make_sumstat_dict(test_sim))\n",
    "\n",
    "    #start the abc fitting\n",
    "    abc.run(min_acceptance_rate=1e-2, max_nr_populations=30)\n",
    "    print('Done!')"
   ],
   "id": "1636e7efc136f2e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from bayesflow.amortizers import AmortizedPosterior\n",
    "from bayesflow.networks import InvertibleNetwork\n",
    "from bayesflow.helper_networks import MultiConv1D\n",
    "from bayesflow.trainers import Trainer\n",
    "from bayesflow import default_settings as defaults\n",
    "from tensorflow.keras.layers import Dense, GRU, LSTM, Bidirectional\n",
    "from tensorflow.keras.models import Sequential"
   ],
   "id": "c606f75e3a4c0257",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.exists(os.path.join(gp, 'validation_data.pickle')):\n",
    "    with open(os.path.join(gp, 'validation_data.pickle'), 'rb') as f:\n",
    "        valid_data = pickle.load(f)\n",
    "else:\n",
    "    raise FileNotFoundError('Validation data not found')\n",
    "\n",
    "x_mean = np.nanmean(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "x_std = np.nanstd(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "p_mean = np.mean(valid_data['prior_draws'], axis=0)\n",
    "p_std = np.std(valid_data['prior_draws'], axis=0)\n",
    "print('Mean and std of data:', x_mean, x_std)\n",
    "print('Mean and std of parameters:', p_mean, p_std)\n",
    "\n",
    "\n",
    "# compute the mean of the summary statistics\n",
    "summary_stats_list_ = [reduced_coordinates_to_sumstat(t) for t in valid_data['sim_data']]\n",
    "(_, ad_averg, _, MSD_averg, _, TA_averg, _, VEL_averg, _, WT_averg) = compute_mean_summary_stats(summary_stats_list_, remove_nan=False)\n",
    "\n",
    "direct_conditions_ = np.stack([ad_averg, MSD_averg, TA_averg, VEL_averg, WT_averg]).T\n",
    "# replace inf with -1\n",
    "direct_conditions_[np.isinf(direct_conditions_)] = np.nan\n",
    "        \n",
    "summary_valid_max = np.nanmax(direct_conditions_, axis=0)\n",
    "summary_valid_min = np.nanmin(direct_conditions_, axis=0)"
   ],
   "id": "df1829ce49462436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def configurator(forward_dict: dict, remove_nans: bool = False, manual_summary: bool = False) -> dict:\n",
    "    out_dict = {}\n",
    "\n",
    "    # Extract data\n",
    "    x = forward_dict[\"sim_data\"]\n",
    "\n",
    "    if remove_nans:\n",
    "        # check if simulation with only nan values in a row\n",
    "        non_nan_populations = np.isnan(x).sum(axis=(1, 2, 3)) - np.prod(x.shape[1:]) != 0\n",
    "        # print(x.shape[0]-non_nan_populations.sum(), 'samples with only nan values in a row')\n",
    "        x = x[non_nan_populations]\n",
    "\n",
    "    # compute manual summary statistics\n",
    "    if manual_summary:\n",
    "        summary_stats_list = [reduced_coordinates_to_sumstat(t) for t in x]\n",
    "        # compute the mean of the summary statistics\n",
    "        (_, ad_averg, _, MSD_averg, _,\n",
    "         TA_averg, _, VEL_averg, _, WT_averg) = compute_mean_summary_stats(summary_stats_list, remove_nan=False)\n",
    "        direct_conditions = np.stack([ad_averg, MSD_averg, TA_averg, VEL_averg, WT_averg]).T\n",
    "        # normalize statistics\n",
    "        direct_conditions = (direct_conditions - summary_valid_min) / (summary_valid_max - summary_valid_min)\n",
    "        # replace nan or inf with -1\n",
    "        direct_conditions[np.isinf(direct_conditions)] = -1\n",
    "        direct_conditions[np.isnan(direct_conditions)] = -1\n",
    "        out_dict['direct_conditions'] = direct_conditions.astype(np.float32)\n",
    "\n",
    "    # Normalize data\n",
    "    x = (x - x_mean) / x_std\n",
    "\n",
    "    # Check for NaN values in the first entry of the last axis\n",
    "    # If nan_mask is False (no NaNs), set to 1; otherwise, set to 0\n",
    "    nan_mask = np.isnan(x[..., 0])\n",
    "    new_dim = np.where(nan_mask, 0, 1)\n",
    "    new_dim_expanded = np.expand_dims(new_dim, axis=-1)\n",
    "    x = np.concatenate((x, new_dim_expanded), axis=-1)\n",
    "\n",
    "    # Normalize data\n",
    "    x[np.isnan(x)] = 0  # replace nan with 0, pre-padding (since we have nans in the data at the end)\n",
    "    out_dict['summary_conditions'] = x.astype(np.float32)\n",
    "\n",
    "    # Extract params\n",
    "    if 'parameters' in forward_dict.keys():\n",
    "        forward_dict[\"prior_draws\"] = forward_dict[\"parameters\"]\n",
    "    if 'prior_draws' in forward_dict.keys():\n",
    "        params = forward_dict[\"prior_draws\"]\n",
    "        if remove_nans:\n",
    "            params = params[non_nan_populations]\n",
    "        params = (params - p_mean) / p_std\n",
    "        out_dict['parameters'] = params.astype(np.float32)\n",
    "    return out_dict"
   ],
   "id": "330709997cc7e561",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "import types\n",
    "import tempfile\n",
    "import keras.models\n",
    "\n",
    "def make_keras_picklable():\n",
    "    def __getstate__(self):\n",
    "        model_str = \"\"\n",
    "        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\n",
    "            keras.models.save_model(self, fd.name, overwrite=True)\n",
    "            model_str = fd.read()\n",
    "        d = { 'model_str': model_str }\n",
    "        return d\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        with tempfile.NamedTemporaryFile(suffix='.hdf5', delete=True) as fd:\n",
    "            fd.write(state['model_str'])\n",
    "            fd.flush()\n",
    "            model = keras.models.load_model(fd.name)\n",
    "        self.__dict__ = model.__dict__\n",
    "\n",
    "\n",
    "    cls = keras.models.Model\n",
    "    cls.__getstate__ = __getstate__\n",
    "    cls.__setstate__ = __setstate__\n",
    "    \n",
    "make_keras_picklable()"
   ],
   "id": "dce8fdcc5d41bcbb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import logging",
   "id": "b50048d7c67622d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "def load_model(model_id):\n",
    "    # Set the logger to the desired level\n",
    "    tf.get_logger().setLevel('ERROR')  # This will suppress warnings and info logs from TensorFlow\n",
    "\n",
    "    # define the network\n",
    "    # define the network\n",
    "    class GroupSummaryNetwork(tf.keras.Model):\n",
    "        \"\"\"Network to summarize the data of groups of cells.  Each group is passed through a series of convolutional layers\n",
    "        followed by an LSTM layer. The output of the LSTM layer is then pooled across the groups and dense layer applied\n",
    "        to obtain a summary of fixed dimensionality. The network is invariant to the order of the groups.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(\n",
    "                self,\n",
    "                summary_dim,\n",
    "                num_conv_layers=2,\n",
    "                rnn_units=128,\n",
    "                bidirectional=True,\n",
    "                conv_settings=None,\n",
    "                use_attention=False,\n",
    "                return_attention_weights=False,\n",
    "                use_GRU=True,\n",
    "                **kwargs\n",
    "        ):\n",
    "            super().__init__(**kwargs)\n",
    "\n",
    "            if conv_settings is None:\n",
    "                conv_settings = defaults.DEFAULT_SETTING_MULTI_CONV\n",
    "            self.conv_settings = conv_settings\n",
    "\n",
    "            conv = Sequential([MultiConv1D(conv_settings) for _ in range(num_conv_layers)])\n",
    "            self.num_conv_layers = num_conv_layers\n",
    "            self.group_conv = tf.keras.layers.TimeDistributed(conv)\n",
    "            self.use_attention = use_attention\n",
    "            self.return_attention_weights = return_attention_weights\n",
    "            self.pooling = tf.keras.layers.GlobalAveragePooling1D()\n",
    "            self.rnn_units = rnn_units\n",
    "            self.use_GRU = use_GRU\n",
    "            self.bidirectional = bidirectional\n",
    "\n",
    "            if self.use_attention:\n",
    "                self.attention = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=rnn_units)\n",
    "\n",
    "            if use_GRU:\n",
    "                rnn = Bidirectional(GRU(rnn_units, return_sequences=use_attention)) if bidirectional else GRU(rnn_units,\n",
    "                                                                                                              return_sequences=use_attention)\n",
    "            else:\n",
    "                rnn = Bidirectional(LSTM(rnn_units, return_sequences=use_attention)) if bidirectional else LSTM(\n",
    "                    rnn_units, return_sequences=use_attention)\n",
    "            self.group_rnn = tf.keras.layers.TimeDistributed(rnn)\n",
    "\n",
    "            self.out_layer = Dense(summary_dim, activation=\"linear\")\n",
    "            self.summary_dim = summary_dim\n",
    "\n",
    "        def call(self, x, **kwargs):\n",
    "            \"\"\"Performs a forward pass through the network by first passing `x` through the same rnn network for\n",
    "            each household and then pooling the outputs across households.\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            x : tf.Tensor\n",
    "                Input of shape (batch_size, n_groups, n_time_steps, n_features)\n",
    "\n",
    "            Returns\n",
    "            -------\n",
    "            out : tf.Tensor\n",
    "                Output of shape (batch_size, summary_dim)\n",
    "            \"\"\"\n",
    "            # Apply the RNN to each group\n",
    "            out = self.group_conv(x, **kwargs)\n",
    "            out = self.group_rnn(out, **kwargs)  # (batch_size, n_groups, lstm_units)\n",
    "            # if attention is used, return full sequence (batch_size, n_groups, n_time_steps, lstm_units)\n",
    "            # bidirectional LSTM returns 2*lstm_units\n",
    "\n",
    "            if self.use_attention:\n",
    "                # learn a query vector to attend over the time points\n",
    "                query = tf.reduce_mean(out, axis=1)\n",
    "                # Reshape query to match the required shape for attention\n",
    "                query = tf.expand_dims(query, axis=1)  # (batch_size, 1, n_time_steps, lstm_units)\n",
    "                if not self.return_attention_weights:\n",
    "                    out = self.attention(query, out, **kwargs)  # (batch_size, 1, n_time_steps, lstm_units)\n",
    "                else:\n",
    "                    out, attention_weights = self.attention(query, out, return_attention_scores=True, **kwargs)\n",
    "                    attention_weights = tf.squeeze(attention_weights, axis=2)\n",
    "                out = tf.squeeze(out, axis=1)  # Remove the extra dimension (batch_size, n_time_steps, lstm_units)\n",
    "                out = self.pooling(out, **kwargs)  # (batch_size, 1, lstm_units)\n",
    "            else:\n",
    "                # pooling over groups, this totally invariants to the order of the groups\n",
    "                out = self.pooling(out, **kwargs)  # (batch_size, lstm_units)\n",
    "            # apply dense layer\n",
    "            out = self.out_layer(out, **kwargs)  # (batch_size, summary_dim)\n",
    "\n",
    "            if self.use_attention and self.return_attention_weights:\n",
    "                return out, attention_weights\n",
    "            return out\n",
    "\n",
    "        def get_config(self):\n",
    "            \"\"\"Return the config for serialization.\"\"\"\n",
    "            config = super().get_config()\n",
    "            config.update({\n",
    "                'summary_dim': self.summary_dim,\n",
    "                'num_conv_layers': self.num_conv_layers,\n",
    "                'rnn_units': self.rnn_units,\n",
    "                'bidirectional': self.bidirectional,\n",
    "                'conv_settings': self.conv_settings,\n",
    "                'use_attention': self.use_attention,\n",
    "                'return_attention_weights': self.return_attention_weights,\n",
    "                'use_GRU': self.use_GRU,\n",
    "            })\n",
    "            return config\n",
    "\n",
    "        @classmethod\n",
    "        def from_config(cls, config):\n",
    "            \"\"\"Recreate the model from the config.\"\"\"\n",
    "            return cls(**config)\n",
    "\n",
    "    num_coupling_layers = 6\n",
    "    num_dense = 3\n",
    "    use_attention = True\n",
    "    use_bidirectional = False\n",
    "    summary_loss = 'MMD'\n",
    "    use_manual_summary = False\n",
    "    if model_id == 0:\n",
    "        checkpoint_path = 'amortizer-cell-migration-attention-6-bid'\n",
    "        use_bidirectional = True\n",
    "    elif model_id == 1:\n",
    "        checkpoint_path = 'amortizer-cell-migration-conv-7'\n",
    "        num_coupling_layers = 7\n",
    "    elif model_id == 2:\n",
    "        checkpoint_path = 'amortizer-cell-migration-attention-7'\n",
    "        num_coupling_layers = 7\n",
    "    elif model_id == 3:\n",
    "        checkpoint_path = 'amortizer-cell-migration-attention-7-bid'\n",
    "        num_coupling_layers = 7\n",
    "        use_bidirectional = True\n",
    "    elif model_id == 4:\n",
    "        checkpoint_path = 'amortizer-cell-migration-attention-7-bid-manual'\n",
    "        num_coupling_layers = 7\n",
    "        use_bidirectional = True\n",
    "        map_idx_sim = np.nan\n",
    "        use_manual_summary = True\n",
    "    else:\n",
    "        raise ValueError('Checkpoint path not found')\n",
    "    os.makedirs(f\"../results/{checkpoint_path}\", exist_ok=True)\n",
    "\n",
    "    summary_net = GroupSummaryNetwork(summary_dim=len(obs_pars) * 2,\n",
    "                                      rnn_units=32,\n",
    "                                      use_attention=use_attention,\n",
    "                                      bidirectional=use_bidirectional)\n",
    "    inference_net = InvertibleNetwork(num_params=len(obs_pars),\n",
    "                                      num_coupling_layers=num_coupling_layers,\n",
    "                                      coupling_design='spline',\n",
    "                                      coupling_settings={\n",
    "                                          \"num_dense\": num_dense,\n",
    "                                          \"dense_args\": dict(\n",
    "                                              activation='relu',\n",
    "                                              kernel_regularizer=tf.keras.regularizers.l2(1e-4),\n",
    "                                          ),\n",
    "                                          \"dropout_prob\": 0.2,\n",
    "                                          \"bins\": 16,\n",
    "                                      })\n",
    "\n",
    "    amortizer = AmortizedPosterior(inference_net=inference_net, summary_net=summary_net,\n",
    "                                   summary_loss_fun=summary_loss)\n",
    "\n",
    "    # Disable logging\n",
    "    logging.disable(logging.CRITICAL)\n",
    "\n",
    "    # build the trainer with networks\n",
    "    max_to_keep = 17\n",
    "    trainer = Trainer(amortizer=amortizer,\n",
    "                      configurator=partial(configurator,\n",
    "                                           manual_summary=use_manual_summary),\n",
    "                      checkpoint_path=checkpoint_path,\n",
    "                      skip_checks=True,\n",
    "                      max_to_keep=max_to_keep)\n",
    "\n",
    "    # check if file exist\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        trainer.load_pretrained_network()\n",
    "        history = trainer.loss_history.get_plottable()\n",
    "    else:\n",
    "        raise FileNotFoundError('Checkpoint path not found')\n",
    "\n",
    "    # Find the checkpoint with the lowest validation loss out of the last max_to_keep\n",
    "    recent_losses = history['val_losses'].iloc[-max_to_keep:]\n",
    "    best_valid_epoch = recent_losses['Loss'].idxmin() + 1  # checkpoints are 1-based indexed\n",
    "    new_checkpoint = trainer.manager.latest_checkpoint.rsplit('-', 1)[0] + f'-{best_valid_epoch}'\n",
    "    trainer.checkpoint.restore(new_checkpoint)\n",
    "    #print(\"Networks loaded from {}\".format(new_checkpoint))\n",
    "\n",
    "    # Re-enable logging\n",
    "    logging.disable(logging.NOTSET)\n",
    "\n",
    "    return summary_net, use_manual_summary"
   ],
   "id": "9e1bca8e9a7b3281",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "_ = load_model(0)",
   "id": "22acde4b06067b35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# use trained neural net as summary statistics\n",
    "def make_sumstat_dict_nn(\n",
    "        data: Union[dict, np.ndarray],\n",
    "        config_map: callable,\n",
    ") -> dict:\n",
    "    if isinstance(data, dict):\n",
    "        # get key\n",
    "        key = list(data.keys())[0]\n",
    "        data = data[key]\n",
    "\n",
    "    summary_nn, manual_summary = load_model(0)\n",
    "\n",
    "    # configures the input for the network\n",
    "    config_input = config_map({\"sim_data\": data}, manual_summary=manual_summary)\n",
    "    # get the summary statistics\n",
    "    out_dict = {\n",
    "        'summary_net': summary_nn(config_input['summary_conditions']).numpy().flatten()\n",
    "    }\n",
    "    # if direct conditions are available, concatenate them\n",
    "    if 'direct_conditions' in config_input.keys():\n",
    "        out_dict['direct_conditions'] = config_input['direct_conditions'].flatten()\n",
    "    return out_dict\n",
    "\n",
    "\n",
    "sumstats_nn = partial(make_sumstat_dict_nn,\n",
    "                      config_map=configurator)\n",
    "\n",
    "if on_cluster:\n",
    "    # define the model object\n",
    "    model_nn = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "    # todo: remember also change tiff path in model.xml!\n",
    "else:\n",
    "    # define the model object\n",
    "    model_nn = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)"
   ],
   "id": "a7c0835963566e42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%time\n",
    "synthetic_data_test_nn = sumstats_nn(test_sim)\n",
    "synthetic_data_test_nn"
   ],
   "id": "e2e73120c64aa514",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "redis_sampler = RedisEvalParallelSampler(host=args.ip, port=args.port,\n",
    "                                         adapt_look_ahead_proposal=False,\n",
    "                                         look_ahead=False)\n",
    "\n",
    "abc = pyabc.ABCSMC(model_nn, prior, # here we use now the Euclidean distance\n",
    "                   population_size=population_size,\n",
    "                   summary_statistics=sumstats_nn,\n",
    "                   #sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",
    "                   sampler=redis_sampler)\n",
    "#db_path = os.path.join(tempfile.gettempdir(), \"test.db\")\n",
    "db_path = os.path.join(gp, \"synthetic_test_new_sumstats.db\")\n",
    "history = abc.new(\"sqlite:///\" + db_path, sumstats_nn(test_sim))\n",
    "\n",
    "#start the abc fitting\n",
    "abc.run(min_acceptance_rate=1e-2, max_nr_populations=30)"
   ],
   "id": "8522eceb33f5a581",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "a2d2b093e81ddec9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
