{ "cells": [  {   "metadata": {},   "cell_type": "markdown",   "source": "# Inference with Approximate Bayesian Computation (ABC)",   "id": "2885213ace1b9d2c"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "import argparse\n",    "import os\n",    "import pickle\n",    "from functools import partial\n",    "from typing import Union\n",    "\n",    "import matplotlib.pyplot as plt\n",    "import multiprocess\n",    "import numpy as np\n",    "import pyabc\n",    "#from pyabc.sampler import RedisEvalParallelSampler\n",    "import scipy.stats as stats\n",    "import umap\n",    "from fitmulticell import model as morpheus_model\n",    "from fitmulticell.sumstat import SummaryStatistics\n",    "from matplotlib.patches import Patch\n",    "\n",    "from load_bayesflow_model import load_model\n",    "from plotting_routines import sampling_parameter_cis, plot_posterior_1d, plot_posterior_2d, plot_sumstats_distance_hist, plot_sumstats_distance_stats\n",    "from summary_stats import compute_summary_stats, reduce_to_coordinates"   ],   "id": "3ce04deed61e1ae4",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# get the job array id and number of processors\n",    "test_id = 0 #int(os.environ.get('SLURM_ARRAY_TASK_ID', 0))\n",    "n_procs = 10 # int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",    "print('test_id', test_id)\n",    "on_cluster = False\n",    "population_size = 1000\n",    "\n",    "if on_cluster:\n",    "    parser = argparse.ArgumentParser(description='Parse necessary arguments')\n",    "    parser.add_argument('-pt', '--port', type=str, default=\"50004\",\n",    "                        help='Which port should be use?')\n",    "    parser.add_argument('-ip', '--ip', type=str,\n",    "                        help='Dynamically passed - BW: Login Node 3')\n",    "    args = parser.parse_args()"   ],   "id": "abbff289ceca71a5",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "if on_cluster:\n",    "    gp = '/home/jarruda_hpc/CellMigration/synth_data_params_bayesflow'\n",    "else:\n",    "    gp = os.getcwd()\n",    "\n",    "par_map = {\n",    "    'gradient_strength': './CellTypes/CellType/Constant[@symbol=\"gradient_strength\"]',\n",    "    'move.strength': './CellTypes/CellType/Constant[@symbol=\"move.strength\"]',\n",    "    'move.duration.mean': './CellTypes/CellType/Constant[@symbol=\"move.duration.mean\"]',\n",    "    'cell_nodes_real': './Global/Constant[@symbol=\"cell_nodes_real\"]',\n",    "}\n",    "\n",    "model_path = gp + \"/cell_movement_v24.xml\"  # time step is 30sec\n",    "# defining the summary statistics function\n",    "max_sequence_length = 120\n",    "min_sequence_length = 0\n",    "only_longest_traj_per_cell = True  # mainly to keep the data batchable\n",    "cells_in_population = 50\n",    "\n",    "\n",    "def make_sumstat_dict(data: Union[dict, np.ndarray]) -> dict:\n",    "    if isinstance(data, dict):\n",    "        # get key\n",    "        key = list(data.keys())[0]\n",    "        data = data[key]\n",    "    data = data[0]  # only one sample\n",    "    # compute the summary statistics\n",    "    msd_list, ta_list, v_list, ad_list = compute_summary_stats(data)\n",    "    cleaned_dict = {\n",    "        'msd': np.array(msd_list).flatten(),\n",    "        'ta': np.array(ta_list).flatten(),\n",    "        'vel': np.array(v_list).flatten(),\n",    "        'ad': np.array(ad_list).flatten(),\n",    "    }\n",    "    return cleaned_dict\n",    "\n",    "\n",    "def prepare_sumstats(output_morpheus_model) -> dict:\n",    "    sim_coordinates = reduce_to_coordinates(output_morpheus_model, \n",    "                          minimal_length=min_sequence_length, \n",    "                          maximal_length=max_sequence_length,\n",    "                          only_longest_traj_per_cell=only_longest_traj_per_cell\n",    "                          )\n",    "    \n",    "    # we now do exactly the same as in the BayesFlow workflow, but here we get only one sample at a time\n",    "    data_transformed = np.ones((1, cells_in_population, max_sequence_length, 2)) * np.nan\n",    "    # each cell is of different length, each with x and y coordinates, make a tensor out of it\n",    "    n_cells_not_visible = 0\n",    "    if len(sim_coordinates) != 0:\n",    "        # some cells were visible in the simulation\n",    "        for c_id, cell_sim in enumerate(sim_coordinates):\n",    "            # pre-pad the data with zeros, but first write zeros as nans to compute the mean and std\n",    "            data_transformed[0, c_id, -len(cell_sim['x']):, 0] = cell_sim['x']\n",    "            data_transformed[0, c_id, -len(cell_sim['y']):, 1] = cell_sim['y']\n",    "    \n",    "    return {'sim': data_transformed}\n",    "\n",    "\n",    "sumstat = SummaryStatistics(sum_stat_calculator=prepare_sumstats)                    \n",    "\n",    "if on_cluster:\n",    "    # define the model object\n",    "    model = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)\n",    "\n",    "    # note: remember also change tiff path in model.xml!\n",    "else:\n",    "    # define the model object\n",    "    model = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)\n",    "\n",    "# parameter values used to generate the synthetic data\n",    "obs_pars = {\n",    "    'gradient_strength': 100.,  # strength of the gradient of chemotaxis\n",    "    'move.strength': 10.,  # strength of directed motion\n",    "    'move.duration.mean': 0.1,  # mean of exponential distribution (seconds)\n",    "    'cell_nodes_real': 50.,  # area of the cell (\\mu m^2), macrophages have a volume of 4990\\mu m^3 -> radius of 17 if they would are sphere\n",    "}\n",    "\n",    "\n",    "obs_pars_log = {key: np.log10(val) for key, val in obs_pars.items()}\n",    "limits = {'gradient_strength': (1, 10000), #(10 ** 4, 10 ** 8),\n",    "          'move.strength': (1, 100),\n",    "          'move.duration.mean': (1e-4, 30), #(math.log10((10 ** -2) * 30), math.log10((10 ** 4))), # smallest time step in simulation 5\n",    "          'cell_nodes_real': (1, 300)}\n",    "limits_log = {key: (np.log10(val[0]), np.log10(val[1])) for key, val in limits.items()}\n",    "\n",    "\n",    "prior = pyabc.Distribution(**{key: pyabc.RV(\"uniform\", loc=lb, scale=ub-lb)\n",    "                              for key, (lb, ub) in limits_log.items()})\n",    "#param_names = list(obs_pars.keys())\n",    "#log_param_names = [f'log_{p}' for p in param_names]\n",    "param_names = ['$m_{\\\\text{dir}}$', '$m_{\\\\text{rand}}$', '$w$', '$a$']\n",    "log_param_names = ['$\\log_{10}(m_{\\\\text{dir}})$', '$\\log_{10}(m_{\\\\text{rand}})$', '$\\log_{10}(w)$', '$\\log_{10}(a)$']\n",    "print(obs_pars)\n",    "print(limits_log)"   ],   "id": "74390dfb0e802a77",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "sigma0 = 550\n",    "space_x0 = 1173/2\n",    "space_y0 = 1500/1.31/2\n",    "x0, y0 = 1173/2, (1500+1500/2+270)/1.31\n",    "u1 = lambda space_x, space_y: 7/(2*np.pi*(sigma0**2)) *np.exp(-1/2*(((space_x)-(x0))**2+ ((space_y)-(y0))**2)/(sigma0**2))\n",    "\n",    "# plot the function\n",    "fig = plt.figure()\n",    "ax = fig.add_subplot(111, projection='3d')\n",    "x = np.linspace(0, 1173 , 100)\n",    "y = np.linspace(0, 2500 , 100)\n",    "X, Y = np.meshgrid(x, y)\n",    "Z = u1(X, Y)\n",    "ax.plot_surface(X, Y, Z, alpha=0.5)\n",    "# plot start points\n",    "ax.scatter(space_x0, space_y0, u1(space_x0, space_y0), color='r', s=100)\n",    "\n",    "ax.set_xlabel('space_x')\n",    "ax.set_ylabel('space_y')\n",    "plt.show()\n",    "\n",    "space_x0, space_y0, u1(space_x0, space_y0), u1(x0, y0)"   ],   "id": "be4eaa8d21c3bfff",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# load test data\n",    "np.random.seed(test_id)\n",    "test_params = np.array(list(prior.rvs().values()))\n",    "if not os.path.exists(os.path.join(gp, f'test_sim_{test_id}.npy')):\n",    "    raise FileNotFoundError('Test data not found')\n",    "else:\n",    "    test_sim = np.load(os.path.join(gp, f'test_sim_{test_id}.npy'))\n",    "    test_sim_full = {'sim_data': test_sim}\n",    "results_path = f'abc_results_{test_id}'\n",    "test_sim.shape"   ],   "id": "b90db5d16d7fa2b",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": "prior_draws = np.array([list(prior.rvs().values()) for _ in range(1000)])",   "id": "ab3a9bf0fcadc7d2",   "outputs": [],   "execution_count": null  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-01-07T10:49:32.834405Z",     "start_time": "2025-01-07T10:49:32.246623Z"    }   },   "cell_type": "markdown",   "source": "## ABC with Wasserstein distance",   "id": "1145158513a20ad6"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "def obj_func_wass_helper(sim: dict, obs: dict, key: str) -> float:\n",    "    x, y = np.array(sim[key]), np.array(obs[key])\n",    "    if x.size == 0 or y.size == 0:\n",    "        return np.inf\n",    "    return stats.wasserstein_distance(x, y)\n",    "\n",    "distances = {\n",    "    'ad': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='ad')),\n",    "    'msd': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='msd')),\n",    "    'ta': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='ta')),\n",    "    'vel': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='vel')),\n",    "}\n",    "adaptive_weights = {\n",    "    d: 1. / max(np.max(make_sumstat_dict(test_sim)[d]), 1e-4) for d in distances.keys()\n",    "}\n",    "\n",    "def obj_func_wass(sim: dict, obs: dict, return_marginal: bool = False, weights: Union[dict, list] = None) -> Union[float, np.ndarray]:\n",    "    total = np.zeros(len(sim.keys()))\n",    "    for k_i, key in enumerate(sim):\n",    "        total[k_i] = distances[key](sim, obs)\n",    "        if weights is not None:\n",    "            if isinstance(weights, dict):\n",    "                total[k_i] = total[k_i] * weights[key]\n",    "            elif isinstance(weights, list):\n",    "                total[k_i] = total[k_i] * weights[k_i]\n",    "            else:\n",    "                raise ValueError('Weights must be a list or a dictionary')\n",    "    if return_marginal:\n",    "        return total\n",    "    return total.sum()\n",    "\n",    "# adaptive distance\n",    "adaptive_wasserstein_distance = pyabc.distance.AdaptiveAggregatedDistance(\n",    "    distances=list(distances.values()),\n",    "    initial_weights=list(adaptive_weights.values()),\n",    "    log_file=os.path.join(gp, f\"{results_path}/adaptive_distance_log.txt\")\n",    ")"   ],   "id": "1636e7efc136f2e0",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "abc_posterior_samples = None\n",    "#redis_sampler = RedisEvalParallelSampler(host=args.ip, port=args.port,\n",    "#                                         adapt_look_ahead_proposal=False,\n",    "#                                         look_ahead=False)\n",    "\n",    "abc = pyabc.ABCSMC(model, prior,\n",    "                   distance_function=adaptive_wasserstein_distance,\n",    "                   summary_statistics=make_sumstat_dict,\n",    "                   population_size=population_size,\n",    "                   sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",    "                   #sampler=redis_sampler\n",    "                   )\n",    "\n",    "db_path = os.path.join(gp, f\"{results_path}/synthetic_test_wasserstein_sumstats_adaptive.db\")\n",    "if not os.path.exists(db_path):\n",    "    history_abc = abc.new(\"sqlite:///\" + db_path, make_sumstat_dict(test_sim))\n",    "\n",    "    # start the abc fitting\n",    "    abc.run(min_acceptance_rate=1e-2, max_nr_populations=15)\n",    "    print('Done!')\n",    "else:\n",    "    history_abc = abc.load(\"sqlite:///\" + db_path)\n",    "    adaptive_weights = pyabc.storage.load_dict_from_json(os.path.join(gp, f'{results_path}/adaptive_distance_log.txt'))[history_abc.max_t+1]"   ],   "id": "efcbb86db464a88d",   "outputs": [],   "execution_count": null  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-01-07T10:49:48.876700Z",     "start_time": "2025-01-07T10:49:35.742219Z"    }   },   "cell_type": "markdown",   "source": "## ABC with neural network summary statistics",   "id": "df1829ce49462436"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "if os.path.exists(os.path.join(gp, 'validation_data.pickle')):\n",    "    with open(os.path.join(gp, 'validation_data.pickle'), 'rb') as f:\n",    "        valid_data = pickle.load(f)\n",    "else:\n",    "    raise FileNotFoundError('Validation data not found')\n",    "\n",    "x_mean = np.nanmean(valid_data['sim_data'], axis=(0, 1, 2))\n",    "x_std = np.nanstd(valid_data['sim_data'], axis=(0, 1, 2))\n",    "p_mean = np.mean(valid_data['prior_draws'], axis=0)\n",    "p_std = np.std(valid_data['prior_draws'], axis=0)\n",    "print('Mean and std of data:', x_mean, x_std)\n",    "print('Mean and std of parameters:', p_mean, p_std)"   ],   "id": "a7c0835963566e42",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# use trained neural net as summary statistics\n",    "def make_sumstat_dict_nn(data: Union[dict, np.ndarray], use_npe_summaries: bool = True) -> dict:\n",    "    if use_npe_summaries:\n",    "        model_id = 2\n",    "    else:\n",    "        model_id = 10\n",    "    if isinstance(data, dict):\n",    "        # get key\n",    "        key = list(data.keys())[0]\n",    "        data = data[key]\n",    "\n",    "    trainer, map_idx_sim = load_model(\n",    "        model_id=model_id,\n",    "        x_mean=x_mean,\n",    "        x_std=x_std,\n",    "        p_mean=p_mean,\n",    "        p_std=p_std,\n",    "    )\n",    "\n",    "    # configures the input for the network\n",    "    config_input = trainer.configurator({\"sim_data\": data})\n",    "    # get the summary statistics\n",    "    out_dict = {\n",    "        'summary_net': trainer.amortizer.summary_net(config_input['summary_conditions']).numpy().flatten()\n",    "    }\n",    "    if model_id == 10:\n",    "        # renormalize the parameters\n",    "        out_dict['summary_net'] = out_dict['summary_net'] * p_std + p_mean\n",    "\n",    "    # if direct conditions are available, concatenate them\n",    "    if 'direct_conditions' in config_input.keys():\n",    "        out_dict['direct_conditions'] = config_input['direct_conditions'].flatten()\n",    "\n",    "    del trainer\n",    "    return out_dict\n",    "\n",    "\n",    "if on_cluster:\n",    "    # define the model object\n",    "    model_nn = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)\n",    "\n",    "    # note: remember also change tiff path in model.xml!\n",    "else:\n",    "    # define the model object\n",    "    model_nn = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)"   ],   "id": "bf7b71404095f91e",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "%%time\n",    "print(make_sumstat_dict_nn(test_sim), test_params)\n",    "\n",    "if test_params is not None:\n",    "    p = 1\n",    "    summary_error = (np.abs(make_sumstat_dict_nn(test_sim, use_npe_summaries=False)['summary_net']-test_params)**p).sum() ** (1 / p)\n",    "    print(make_sumstat_dict_nn(test_sim, use_npe_summaries=False), test_params)\n",    "    print('error:', summary_error)"   ],   "id": "8522eceb33f5a581",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# abc with summary net trained on posterior mean\n",    "abc_nn = pyabc.ABCSMC(model_nn, prior, # here we use now the Euclidean distance, Wasserstein distance is not possible\n",    "                      population_size=population_size,\n",    "                      summary_statistics=partial(make_sumstat_dict_nn, use_npe_summaries=False),\n",    "                      sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",    "                      #sampler=redis_sampler\n",    "                      )\n",    "\n",    "db_path = os.path.join(gp, f\"{results_path}/synthetic_test_nn_sumstats_posterior_mean.db\")\n",    "\n",    "if not os.path.exists(db_path):\n",    "    history_nn = abc_nn.new(\"sqlite:///\" + db_path, make_sumstat_dict_nn(test_sim))\n",    "\n",    "    # start the abc fitting\n",    "    abc_nn.run(min_acceptance_rate=1e-2, max_nr_populations=15)\n",    "    print('Done!')\n",    "else:\n",    "    history_nn = abc_nn.load(\"sqlite:///\" + db_path)"   ],   "id": "82568acc7945ee86",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "#redis_sampler = RedisEvalParallelSampler(host=args.ip, port=args.port,\n",    "#                                         adapt_look_ahead_proposal=False,\n",    "#                                         look_ahead=False)\n",    "\n",    "# abc with summary net trained with NPE\n",    "abc_npe = pyabc.ABCSMC(model_nn, prior, # here we use now the Euclidean distance, Wasserstein distance is not possible\n",    "                      population_size=population_size,\n",    "                      summary_statistics=partial(make_sumstat_dict_nn, use_npe_summaries=True),\n",    "                      sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",    "                      #sampler=redis_sampler\n",    "                      )\n",    "\n",    "db_path = os.path.join(gp, f\"{results_path}/synthetic_test_nn_sumstats.db\")\n",    "if not os.path.exists(db_path):\n",    "    history_npe = abc_npe.new(\"sqlite:///\" + db_path,\n",    "                              make_sumstat_dict_nn(test_sim, use_npe_summaries=True))\n",    "\n",    "    # start the abc fitting\n",    "    abc_npe.run(min_acceptance_rate=1e-2, max_nr_populations=15)\n",    "    print('Done!')\n",    "else:\n",    "    history_npe = abc_npe.load(\"sqlite:///\" + db_path)"   ],   "id": "29a244aeacb895ad",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": "history_nn, history_npe = None, None  # todo: remove",   "id": "20b5e3d038cb8b94",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "for hist, name in zip([history_abc, history_nn, history_npe], ['abc', 'abc_mean', 'abc_npe']):\n",    "    if hist is None:\n",    "        continue\n",    "    print(name)\n",    "    fig, ax = plt.subplots(1, len(param_names), tight_layout=True, figsize=(12, 4))\n",    "    for i, param in enumerate(limits.keys()):\n",    "        for t in range(hist.max_t + 1):\n",    "            df, w = hist.get_distribution(m=0, t=t)\n",    "            pyabc.visualization.plot_kde_1d(\n",    "                df,\n",    "                w,\n",    "                xmin=limits_log[param][0],\n",    "                xmax=limits_log[param][1],\n",    "                x=param,\n",    "                xname=log_param_names[i],\n",    "                ax=ax[i],\n",    "                label=f\"PDF t={t}\",\n",    "            )\n",    "        ax[i].set_xlim((limits_log[param][0]-0.2, limits_log[param][1]+0.2))\n",    "    plt.savefig(os.path.join(gp, f'{results_path}/synthetic_population_kdes_{name}.pdf'), bbox_inches='tight')\n",    "    plt.show()\n",    "\n",    "    fig, arr_ax = plt.subplots(1, 5, figsize=(12, 3), tight_layout=True)\n",    "    arr_ax = arr_ax.flatten()\n",    "    pyabc.visualization.plot_sample_numbers(hist, ax=arr_ax[0])\n",    "    arr_ax[0].get_legend().remove()\n",    "    pyabc.visualization.plot_walltime(hist, ax=arr_ax[1], unit='h')\n",    "    arr_ax[1].get_legend().remove()\n",    "    pyabc.visualization.plot_epsilons(hist, ax=arr_ax[2])\n",    "    pyabc.visualization.plot_effective_sample_sizes(hist, ax=arr_ax[3])\n",    "    pyabc.visualization.plot_acceptance_rates_trajectory(hist, ax=arr_ax[4])\n",    "    plt.savefig(os.path.join(gp, f'{results_path}/synthetic_diagnostics_{name}.pdf'), bbox_inches='tight')\n",    "    plt.show()"   ],   "id": "b7d320f9bb9b7e8c",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "markdown",   "source": "# Synthetic Tests",   "id": "287e3fde6457f51d"  },  {   "metadata": {},   "cell_type": "markdown",   "source": "## Compare Posterior Samples",   "id": "e3b932da8cac0311"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# get posterior samples\n",    "posterior_samples = {}\n",    "for hist, name in zip([history_abc, history_nn, history_npe], ['abc', 'abc_mean', 'abc_npe']):\n",    "    if hist is None:\n",    "        continue\n",    "    abc_df, abc_w = hist.get_distribution()\n",    "    posterior_samples[name] = pyabc.resample(abc_df[limits.keys()].values, abc_w, n=1000)\n",    "\n",    "# add bayesflow posterior samples\n",    "posterior_samples['npe'] = bayesflow_posterior_samples = np.load(\n",    "        f'abc_results_{test_id}/posterior_samples_npe.npy'\n",    ")"   ],   "id": "488d22682851a587",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "labels_colors = {\n",    "    'abc': ('ABC with hand-crafted summaries', '#9AB8D7'),\n",    "    'abc_mean': ('ABC with posterior mean summaries', '#C4B7D4'),\n",    "    'abc_npe': ('ABC with inference-tailored summaries', '#EEBC88'),\n",    "    'npe': ('NPE with jointly learned summaries', '#A7CE97')\n",    "}\n",    "\n",    "colors = [labels_colors[name][1] for name in posterior_samples.keys()]\n",    "labels = [labels_colors[name][0] for name in posterior_samples.keys()]"   ],   "id": "4f919a570c48e60d",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "for i, (name, ps) in enumerate(posterior_samples.items()):\n",    "    if ps is None:\n",    "        continue\n",    "    print(name)\n",    "    if name == 'abc_mean':\n",    "         reference_params=make_sumstat_dict_nn(test_sim, use_npe_summaries=False)['summary_net']\n",    "    else:\n",    "        reference_params = None\n",    "    # fig = plot_posterior_2d(\n",    "    #     posterior_draws=ps,\n",    "    #     prior_draws=prior_draws,\n",    "    #     param_names=log_param_names,\n",    "    #     true_params=test_params,\n",    "    #     reference_params=reference_params,  # only for nn-mean\n",    "    #     post_color=colors[i],\n",    "    #     post_alpha=1,\n",    "    #     bins=20, #'auto',\n",    "    # )\n",    "    # plt.savefig(os.path.join(gp, f'{results_path}/synthetic_posterior_2d_{name}.png'))\n",    "    # plt.show()\n",    "\n",    "    fig = plot_posterior_1d(\n",    "        posterior_draws=ps,\n",    "        prior_draws=prior_draws,\n",    "        param_names=log_param_names,\n",    "        true_params=test_params,\n",    "        reference_params=reference_params,  # only for nn-mean\n",    "        post_color=labels_colors[name][1],\n",    "        post_alpha=1,\n",    "        bins=20, #'auto',\n",    "    )\n",    "    plt.savefig(os.path.join(gp, f'{results_path}/synthetic_posterior_1d_{name}.pdf'), bbox_inches='tight')\n",    "    plt.show()"   ],   "id": "fb7c8bdd7244f0a7",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "prior_mean = np.mean(prior_draws, axis=0)\n",    "prior_std = np.std(prior_draws, axis=0)\n",    "\n",    "def compute_z_score(posterior_mean):\n",    "    return (posterior_mean - prior_mean) / prior_std\n",    "\n",    "def compute_contraction(posterior_std):\n",    "    return 1. - (posterior_std / prior_std)\n",    "\n",    "posterior_stats = {}\n",    "for name, ps in posterior_samples.items():\n",    "    posterior_stats[name] = {\n",    "        'mean': np.mean(ps, axis=0),\n",    "        'std': np.std(ps, axis=0),\n",    "        'median': np.median(ps, axis=0),\n",    "        'z_score': compute_z_score(np.mean(ps, axis=0)),\n",    "        'contraction': compute_contraction(np.std(ps, axis=0))\n",    "    }"   ],   "id": "87b27ca1a2a0f549",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "z_scores = [posterior_stats[name]['z_score'] for name in posterior_samples.keys()]\n",    "contractions = [posterior_stats[name]['contraction'] for name in posterior_samples.keys()]\n",    "\n",    "# Plotting Z-scores and contractions for both methods\n",    "fig, ax1 = plt.subplots(figsize=(8, 4), tight_layout=True)\n",    "\n",    "# Z-Scores for both methods\n",    "for i, z in enumerate(z_scores):\n",    "    ax1.bar(np.arange(len(param_names)) + 0.2 * i, z, width=0.15, align='center', color=colors[i],\n",    "            label=labels[i])\n",    "ax1.set_ylabel('Z-Score')\n",    "ax1.tick_params(axis='y')\n",    "ax1.set_xticks(np.arange(len(param_names)) + 0.3)\n",    "ax1.set_xticklabels(log_param_names)\n",    "ax1.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",    "ax1.grid()\n",    "\n",    "# Plot Contractions for both methods on secondary axis\n",    "ax2 = ax1.twinx()\n",    "for i, c in enumerate(contractions):\n",    "    ax2.plot(np.arange(0.3, len(param_names)), c, color=colors[i], marker='o', linestyle='--')\n",    "ax2.set_ylabel('--‚óè--  Contraction')\n",    "ax2.tick_params(axis='y')\n",    "ax2.set_ylim(ax1.get_ylim())  # Align y-limits of both axes\n",    "\n",    "# Combine legends\n",    "handles1, labels1 = ax1.get_legend_handles_labels()\n",    "fig.legend(handles1, labels1,\n",    "           loc='lower center', bbox_to_anchor=(0.5, -0.12), ncol=2)\n",    "fig.savefig(os.path.join(gp, f'{results_path}/synthetic_z_scores_contraction.png'), bbox_inches='tight')\n",    "plt.show()"   ],   "id": "c851aaf76d152b11",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "#ordering = [0,4,1,5,2,6,3,7]\n",    "ordering = np.concatenate([[i,i+4, i+8, i+12] for i in range(4)])\n",    "all_params = np.concatenate((posterior_samples['abc'],\n",    "                             posterior_samples['abc_mean'],\n",    "                             posterior_samples['abc_npe'],\n",    "                             posterior_samples['npe']), axis=-1)\n",    "log_param_names_plot = np.array(\n",    "    [f'{n} $\\qquad$ ABC hand-crafted' for n in log_param_names] +\n",    "    [f'ABC posterior mean' for n in log_param_names] +\n",    "    [f'ABC inference tailored' for n in log_param_names] +\n",    "    [f'NPE' for n in log_param_names]\n",    ")[ordering]\n",    "param_names_plot = np.array(\n",    "    [f'{n} $\\qquad$ ABC hand-crafted' for n in param_names] +\n",    "    [f'ABC posterior mean' for n in param_names] +\n",    "    [f'ABC inference tailored' for n in param_names] +\n",    "    [f'NPE' for n in param_names]\n",    ")[ordering]\n",    "color_list = colors*len(param_names)\n",    "\n",    "ax = sampling_parameter_cis(\n",    "    all_params[:, ordering],\n",    "    true_param=np.concatenate((test_params, test_params, test_params, test_params))[ordering] if test_params is not None else None,\n",    "    prior_bounds=limits_log.values() if test_params is not None else None,\n",    "    param_names=log_param_names_plot,\n",    "    alpha=[95, 90 , 80],\n",    "    color_list=color_list,\n",    "    show_median=False if test_params is not None else True,\n",    "    size=(7, 4),\n",    "    legend_bbox_to_anchor=(0.45,1) if test_params is not None else (0.31,1)\n",    ")\n",    "plt.savefig(os.path.join(gp, f'{results_path}/synthetic_posterior_credible_intervals_log.png'))\n",    "plt.show()\n",    "\n",    "all_params = np.power(10, all_params)\n",    "ax = sampling_parameter_cis(\n",    "    all_params[:, ordering],\n",    "    true_param=np.power(10, np.concatenate((test_params, test_params, test_params, test_params))[ordering]) if test_params is not None else None,\n",    "    param_names=param_names_plot,\n",    "    alpha=[95, 90 , 80],\n",    "    color_list=color_list,\n",    "    show_median=False if test_params is not None else True,\n",    "    size=(7, 4),\n",    "    legend_bbox_to_anchor=(0.99,0.35)\n",    ")\n",    "plt.savefig(os.path.join(gp, f'{results_path}/synthetic_posterior_credible_intervals.png'))\n",    "plt.show()"   ],   "id": "4033f43895a45b0f",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "markdown",   "source": "## Compare Simulations from Posterior Samples",   "id": "6ed9a874bf49cd3b"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "def check_if_done(result):\n",    "    print(\"Task finished\")\n",    "\n",    "file_name = os.path.join(gp, f'{results_path}/synthetic_posterior_simulations.pickle')\n",    "if os.path.exists(file_name):\n",    "    with open(file_name, 'rb') as f:\n",    "        posterior_simulations = pickle.load(f)\n",    "else:\n",    "    posterior_simulations = {}\n",    "    for name, ps in posterior_samples.items():\n",    "        def wrapper_fun(sample_i):\n",    "            sim_dict = {key: p for key, p in zip(obs_pars.keys(), ps[sample_i])}\n",    "            posterior_sim = model(sim_dict)\n",    "            return posterior_sim['sim']\n",    "\n",    "        with multiprocess.Pool() as pool:\n",    "            async_results = [pool.apply_async(wrapper_fun, (i,), callback=check_if_done) for i in range(100)]\n",    "            # Wait for all results to complete\n",    "            sim_list = [result.get() for result in async_results]\n",    "        posterior_simulations[name] = np.concatenate(sim_list)\n",    "\n",    "    with open(file_name, 'wb') as f:\n",    "        pickle.dump(posterior_simulations, f)\n",    "\n",    "simulation_sumstats = {}\n",    "for name, ps in posterior_simulations.items():\n",    "    simulation_sumstats[name] = [make_sumstat_dict(p_sim[np.newaxis]) for p_sim in ps]"   ],   "id": "903ac7db2e60002a",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "plot_sumstats_distance_hist(partial(obj_func_wass, weights=adaptive_weights),\n",    "                            make_sumstat_dict(test_sim),\n",    "                            [ps for ps in simulation_sumstats.values()],\n",    "                            labels=labels,\n",    "                            sharey=True, sharex=False,\n",    "                            colors=colors,\n",    "                            path=os.path.join(gp, f'{results_path}/synthetic_sumstats_wasserstein_hist.png'))\n",    "\n",    "plot_sumstats_distance_stats(partial(obj_func_wass, weights=adaptive_weights),\n",    "                             make_sumstat_dict(test_sim),\n",    "                             [ps for ps in simulation_sumstats.values()],\n",    "                             labels=labels,\n",    "                             sharey=True, sharex=False,\n",    "                             colors=colors,\n",    "                             path=os.path.join(gp, f'{results_path}/synthetic_sumstats_wasserstein.png'))"   ],   "id": "c2e7c4e0aacd9d18",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "markdown",   "source": "# Real Data Analysis",   "id": "9eae72208b78f06"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# load real data in morpheus format\n",    "from load_data import load_real_data\n",    "\n",    "test_params = None\n",    "results_path = 'abc_results_real'"   ],   "id": "a71696cfa89b749d",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "real_data, real_data_full = load_real_data(data_id=1,\n",    "                                           max_sequence_length=max_sequence_length,\n",    "                                           cells_in_population=cells_in_population)\n",    "factor = 1#.31  # convert from Morpheus to real coordinates in micrometers for plotting\n",    "real_sim = real_data_full[np.newaxis] #np.array([real_data[start:start+cells_in_population] for start in range(0, len(real_data), cells_in_population)])[0][np.newaxis]\n",    "\n",    "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True, sharey=True,\n",    "                       tight_layout=True, figsize=(4, 3))\n",    "\n",    "plt.plot(real_data_full[0, :, 0]*factor, real_data_full[0, :, 1]*factor, color='blue', alpha=0.7)\n",    "for cell_id in range(1, real_data_full.shape[0]):\n",    "    plt.plot(real_data_full[cell_id, :, 0]*factor, real_data_full[cell_id, :, 1]*factor,\n",    "             linewidth=0.5, color='blue', alpha=0.7)\n",    "    plt.scatter(real_data_full[cell_id, :, 0]*factor, real_data_full[cell_id, :, 1]*factor,\n",    "                s=1, color='blue', alpha=0.7)\n",    "\n",    "lims = ax.get_xlim(), ax.get_ylim()\n",    "img_path = 'Cell_migration_grid_v3_final2_invers.tiff'\n",    "img = plt.imread(img_path)\n",    "ax.imshow(img,\n",    "          origin='lower',\n",    "          #extent=[x_min, x_max, y_min, y_max],\n",    "          aspect='auto',          # preserve pixel squares\n",    "          zorder=0,\n",    "          interpolation=['bicubic', 'bessel', 'hermite', 'nearest', 'spline16', 'mitchell', 'bilinear', 'sinc', 'none', 'gaussian', 'blackman', 'spline36', 'hanning', 'kaiser', 'lanczos', 'antialiased', 'hamming', 'catrom', 'quadric'][14]) # keep grid sharp\n",    "ax.set_xlim(lims[0])\n",    "ax.set_ylim(lims[1])\n",    "\n",    "plt.ylabel('$y$ Position (in $\\mu m$)')\n",    "plt.xlabel('$x$ Position (in $\\mu m$)')\n",    "#plt.savefig(os.path.join(gp, f'{results_path}/real_data.pdf'), bbox_inches='tight')\n",    "plt.show()"   ],   "id": "e2a91e786c383b9",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "#redis_sampler = RedisEvalParallelSampler(host=args.ip, port=args.port,\n",    "#                                         adapt_look_ahead_proposal=False,\n",    "#                                         look_ahead=False)\n",    "\n",    "# abc with summary net trained with NPE\n",    "abc_npe = pyabc.ABCSMC(model_nn, prior, # here we use now the Euclidean distance, Wasserstein distance is not possible\n",    "                      population_size=population_size,\n",    "                      summary_statistics=partial(make_sumstat_dict_nn, use_npe_summaries=True),\n",    "                      sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",    "                      #sampler=redis_sampler\n",    "                      )\n",    "\n",    "db_path = os.path.join(gp, f\"{results_path}/real_test_nn_sumstats.db\")\n",    "if not os.path.exists(db_path):\n",    "    history_npe_real = abc_npe.new(\"sqlite:///\" + db_path,\n",    "                              make_sumstat_dict_nn(real_sim, use_npe_summaries=True))\n",    "\n",    "    # start the abc fitting\n",    "    abc_npe.run(min_acceptance_rate=1e-2, max_nr_populations=15)\n",    "    print('Done!')\n",    "else:\n",    "    history_npe_real = abc_npe.load(\"sqlite:///\" + db_path)"   ],   "id": "4e98e2bd7b87b144",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "for hist, name in zip([history_npe_real], ['abc_npe']):\n",    "    if hist is None:\n",    "        continue\n",    "    print(name)\n",    "    fig, ax = plt.subplots(1, len(param_names), tight_layout=True, figsize=(12, 4))\n",    "    for i, param in enumerate(limits.keys()):\n",    "        for t in range(hist.max_t + 1):\n",    "            df, w = hist.get_distribution(m=0, t=t)\n",    "            pyabc.visualization.plot_kde_1d(\n",    "                df,\n",    "                w,\n",    "                xmin=limits_log[param][0],\n",    "                xmax=limits_log[param][1],\n",    "                x=param,\n",    "                xname=log_param_names[i],\n",    "                ax=ax[i],\n",    "                label=f\"PDF t={t}\",\n",    "            )\n",    "        ax[i].set_xlim((limits_log[param][0]-0.2, limits_log[param][1]+0.2))\n",    "    plt.savefig(os.path.join(gp, f'{results_path}/real_population_kdes_{name}.png'))\n",    "    plt.show()\n",    "\n",    "    fig, arr_ax = plt.subplots(1, 5, figsize=(12, 3), tight_layout=True)\n",    "    arr_ax = arr_ax.flatten()\n",    "    pyabc.visualization.plot_sample_numbers(hist, ax=arr_ax[0])\n",    "    arr_ax[0].get_legend().remove()\n",    "    pyabc.visualization.plot_walltime(hist, ax=arr_ax[1], unit='h')\n",    "    arr_ax[1].get_legend().remove()\n",    "    pyabc.visualization.plot_epsilons(hist, ax=arr_ax[2])\n",    "    pyabc.visualization.plot_effective_sample_sizes(hist, ax=arr_ax[3])\n",    "    pyabc.visualization.plot_acceptance_rates_trajectory(hist, ax=arr_ax[4])\n",    "    plt.savefig(os.path.join(gp, f'{results_path}/real_diagnostics_{name}.png'))\n",    "    plt.show()"   ],   "id": "78393a7229bcda78",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# get posterior samples\n",    "posterior_samples = {}\n",    "for hist, name in zip([history_npe_real], ['abc_npe']):\n",    "    if hist is None:\n",    "        posterior_samples[name] = None\n",    "        continue\n",    "    abc_df, abc_w = hist.get_distribution()\n",    "    posterior_samples[name] = pyabc.resample(abc_df[limits.keys()].values, abc_w, n=1000)\n",    "\n",    "# add bayesflow posterior samples\n",    "posterior_samples['npe'] = bayesflow_posterior_samples = np.load(\n",    "        f'abc_results_real/posterior_samples_npe.npy'\n",    ")"   ],   "id": "30b4ffca42deedfd",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "for i, (name, ps) in enumerate(posterior_samples.items()):\n",    "    if ps is None:\n",    "        continue\n",    "    print(name)\n",    "    if name == 'abc_mean':\n",    "         reference_params=make_sumstat_dict_nn(test_sim, use_npe_summaries=False)['summary_net']\n",    "    else:\n",    "        reference_params = None\n",    "    # fig = plot_posterior_2d(\n",    "    #     posterior_draws=ps,\n",    "    #     prior_draws=prior_draws,\n",    "    #     param_names=log_param_names,\n",    "    #     true_params=test_params,\n",    "    #     reference_params=reference_params,  # only for nn-mean\n",    "    #     post_color=colors[i],\n",    "    #     post_alpha=1,\n",    "    #     bins=20, #'auto',\n",    "    # )\n",    "    # plt.savefig(os.path.join(gp, f'{results_path}/real_posterior_2d_{name}.png'))\n",    "    # plt.show()\n",    "\n",    "    fig = plot_posterior_1d(\n",    "        posterior_draws=ps,\n",    "        prior_draws=prior_draws,\n",    "        param_names=log_param_names,\n",    "        true_params=test_params,\n",    "        reference_params=reference_params,  # only for nn-mean\n",    "        post_color=labels_colors[name][1],\n",    "        post_alpha=1,\n",    "        bins=20, #'auto',\n",    "    )\n",    "    plt.savefig(os.path.join(gp, f'{results_path}/real_posterior_1d_{name}.png'))\n",    "    plt.show()"   ],   "id": "d4caf077c66ab35f",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "ordering = np.concatenate([[i,i+4] for i in range(4)])\n",    "all_params = np.concatenate((posterior_samples['abc_npe'],\n",    "                             posterior_samples['npe']), axis=-1)\n",    "log_param_names_plot = np.array(\n",    "    [f'{n} $\\qquad$ ABC inference tailored' for n in log_param_names] + [f'NPE' for n in log_param_names]\n",    ")[ordering]\n",    "param_names_plot = np.array(\n",    "    [f'{n} $\\qquad$ ABC inference tailored' for n in param_names] + [f'NPE' for n in param_names]\n",    ")[ordering]\n",    "color_list = [labels_colors['abc_npe'][1], labels_colors['npe'][1]]*len(param_names)\n",    "\n",    "ax = sampling_parameter_cis(\n",    "    all_params[:, ordering],\n",    "    true_param=np.concatenate((test_params, test_params, test_params, test_params))[ordering] if test_params is not None else None,\n",    "    prior_bounds=limits_log.values() if test_params is not None else None,\n",    "    param_names=log_param_names_plot,\n",    "    alpha=[95, 90 , 80],\n",    "    color_list=color_list,\n",    "    show_median=False if test_params is not None else True,\n",    "    size=(7, 4),\n",    "    legend_bbox_to_anchor=(0.45,1) if test_params is not None else (0.31,1)\n",    ")\n",    "plt.savefig(os.path.join(gp, f'{results_path}/real_posterior_credible_intervals_log.png'))\n",    "plt.show()\n",    "\n",    "all_params = np.power(10, all_params)\n",    "ax = sampling_parameter_cis(\n",    "    all_params[:, ordering],\n",    "    true_param=np.power(10, np.concatenate((test_params, test_params, test_params, test_params))[ordering]) if test_params is not None else None,\n",    "    param_names=param_names_plot,\n",    "    alpha=[95, 90 , 80],\n",    "    color_list=color_list,\n",    "    show_median=False if test_params is not None else True,\n",    "    size=(7, 4),\n",    "    legend_bbox_to_anchor=(0.99,0.35)\n",    ")\n",    "plt.savefig(os.path.join(gp, f'{results_path}/real_posterior_credible_intervals.png'))\n",    "plt.show()"   ],   "id": "e441cbe9264bdc03",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "def check_if_done(result):\n",    "    print(\"Task finished\")\n",    "\n",    "file_name = os.path.join(gp, f'{results_path}/real_posterior_simulations.pickle')\n",    "if os.path.exists(file_name):\n",    "    with open(file_name, 'rb') as f:\n",    "        posterior_simulations = pickle.load(f)\n",    "else:\n",    "    posterior_simulations = {}\n",    "    for name, ps in posterior_samples.items():\n",    "        def wrapper_fun(sample_i):\n",    "            sim_dict = {key: p for key, p in zip(obs_pars.keys(), ps[sample_i])}\n",    "            posterior_sim = model(sim_dict)\n",    "            return posterior_sim['sim']\n",    "\n",    "        with multiprocess.Pool() as pool:\n",    "            async_results = [pool.apply_async(wrapper_fun, (i,), callback=check_if_done) for i in range(1)]\n",    "            # Wait for all results to complete\n",    "            sim_list = [result.get() for result in async_results]\n",    "        posterior_simulations[name] = np.concatenate(sim_list)\n",    "\n",    "    with open(file_name, 'wb') as f:\n",    "        pickle.dump(posterior_simulations, f)\n",    "\n",    "simulation_sumstats = {}\n",    "for name, ps in posterior_simulations.items():\n",    "    simulation_sumstats[name] = [make_sumstat_dict(p_sim[np.newaxis]) for p_sim in ps]"   ],   "id": "593c018cec588431",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "all_trajec = np.concatenate((\n",    "    real_sim[0],\n",    "    posterior_simulations['abc_npe'][0],\n",    "    posterior_simulations['npe'][0]),\n",    "    axis=0)\n",    "all_trajec = np.concatenate([all_trajec[..., 0], all_trajec[..., 1]], axis=1)\n",    "all_trajec[np.isnan(all_trajec)] = -1\n",    "\n",    "color_code = np.concatenate([\n",    "    np.zeros(real_sim[0].shape[0]),\n",    "    np.ones(posterior_simulations['abc_npe'][0].shape[0]),\n",    "    np.ones(posterior_simulations['npe'][0].shape[0])*2\n",    "])\n",    "\n",    "# make a umap plot\n",    "colors_umap = ['#1f77b4', labels_colors['abc_npe'][1], labels_colors['npe'][1]]\n",    "reducer = umap.UMAP(random_state=42, n_jobs=1)\n",    "embedding = reducer.fit_transform(all_trajec)\n",    "\n",    "plt.figure(tight_layout=True, figsize=(5, 5))\n",    "plt.scatter(\n",    "    embedding[:, 0],\n",    "    embedding[:, 1],\n",    "    c=[colors_umap[int(i)] for i in color_code],\n",    "    alpha=0.7,\n",    ")\n",    "plt.gca().set_aspect('equal', 'datalim')\n",    "patches = [Patch(color=colors_umap[i], label=f'{[\"Real\", \"ABC\", \"NPE\"][i]}')\n",    "           for i in range(len(colors_umap))]\n",    "plt.legend(handles=patches)\n",    "#plt.title('UMAP Trajectory Projection')\n",    "plt.savefig(os.path.join(gp, f'{results_path}/umap_trajectory.png'))\n",    "plt.show()"   ],   "id": "c3b561d39cd3e075",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": "",   "id": "a952b03893ec486e",   "outputs": [],   "execution_count": null  } ], "metadata": {  "kernelspec": {   "display_name": "Python 3",   "language": "python",   "name": "python3"  },  "language_info": {   "codemirror_mode": {    "name": "ipython",    "version": 2   },   "file_extension": ".py",   "mimetype": "text/x-python",   "name": "python",   "nbconvert_exporter": "python",   "pygments_lexer": "ipython2",   "version": "2.7.6"  } }, "nbformat": 4, "nbformat_minor": 5}