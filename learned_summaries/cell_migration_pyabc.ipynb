{ "cells": [  {   "metadata": {},   "cell_type": "markdown",   "source": "# Inference with Approximate Bayesian Computation (ABC)",   "id": "2885213ace1b9d2c"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "import argparse\n",    "import os\n",    "import pickle\n",    "from functools import partial\n",    "from typing import Union\n",    "\n",    "import matplotlib.pyplot as plt\n",    "from joblib import Parallel, delayed\n",    "import numpy as np\n",    "import pyabc\n",    "#from pyabc.sampler import RedisEvalParallelSampler\n",    "import scipy.stats as stats\n",    "from fitmulticell import model as morpheus_model\n",    "from fitmulticell.sumstat import SummaryStatistics\n",    "\n",    "from load_bayesflow_model import load_model, EnsembleTrainer\n",    "from plotting_routines import sampling_parameter_cis, plot_posterior_1d, plot_sumstats_distance_stats\n",    "from summary_stats import compute_summary_stats, reduce_to_coordinates, span, euclidean_distance"   ],   "id": "3ce04deed61e1ae4",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# get the job array id and number of processors\n",    "test_id = 2 #int(os.environ.get('SLURM_ARRAY_TASK_ID', 0))\n",    "n_procs = 10 # int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",    "print('test_id', test_id)\n",    "on_cluster = False"   ],   "id": "81c3fdf26f2b28f1",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "population_size = 1000\n",    "\n",    "if on_cluster:\n",    "    parser = argparse.ArgumentParser(description='Parse necessary arguments')\n",    "    parser.add_argument('-pt', '--port', type=str, default=\"50004\",\n",    "                        help='Which port should be use?')\n",    "    parser.add_argument('-ip', '--ip', type=str,\n",    "                        help='Dynamically passed - BW: Login Node 3')\n",    "    args = parser.parse_args()"   ],   "id": "abbff289ceca71a5",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "if on_cluster:\n",    "    gp = '/home/jarruda_hpc/CellMigration/synth_data_params_bayesflow'\n",    "else:\n",    "    gp = os.getcwd()\n",    "\n",    "par_map = {\n",    "    'gradient_strength': './CellTypes/CellType/Constant[@symbol=\"gradient_strength\"]',\n",    "    'move.strength': './CellTypes/CellType/Constant[@symbol=\"move.strength\"]',\n",    "    'move.duration.mean': './CellTypes/CellType/Constant[@symbol=\"move.duration.mean\"]',\n",    "    'cell_nodes_real': './Global/Constant[@symbol=\"cell_nodes_real\"]',\n",    "}\n",    "\n",    "dt = 30\n",    "model_path = gp + \"/cell_movement_v24.xml\"  # time step is 30sec, for inference\n",    "\n",    "# defining the summary statistics function\n",    "min_sequence_length = 0\n",    "max_sequence_length = 3600 // dt\n",    "only_longest_traj_per_cell = True  # mainly to keep the data batchable\n",    "cells_in_population = 143\n",    "\n",    "def make_sumstat_dict(data: Union[dict, np.ndarray]) -> dict:\n",    "    if isinstance(data, dict):\n",    "        # get key\n",    "        key = list(data.keys())[0]\n",    "        data = data[key]\n",    "    data = data[0]  # only one full simulation\n",    "    assert data.ndim == 3\n",    "    # compute the summary statistics\n",    "    msd_list, ta_list, v_list, ad_list = compute_summary_stats(data, dt=dt)\n",    "    cleaned_dict = {\n",    "        'msd': np.array(msd_list).flatten(),\n",    "        'ta': np.array(ta_list).flatten(),\n",    "        'vel': np.array(v_list).flatten(),\n",    "        'ad': np.array(ad_list).flatten(),\n",    "    }\n",    "    return cleaned_dict\n",    "\n",    "\n",    "def prepare_sumstats(output_morpheus_model) -> dict:\n",    "    sim_coordinates = reduce_to_coordinates(output_morpheus_model, \n",    "                          minimal_length=min_sequence_length, \n",    "                          maximal_length=max_sequence_length,\n",    "                          only_longest_traj_per_cell=only_longest_traj_per_cell,\n",    "                          )\n",    "    \n",    "    # we now do exactly the same as in the BayesFlow workflow, but here we get only one sample at a time\n",    "    data_transformed = np.ones((1, cells_in_population, max_sequence_length, 3)) * np.nan\n",    "    # each cell is of different length, each with x and y coordinates, make a tensor out of it\n",    "    if len(sim_coordinates) != 0:\n",    "        # some cells were visible in the simulation\n",    "        for c_id, cell_sim in enumerate(sim_coordinates):\n",    "            # pre-pad the data with zeros, but first write zeros as nans to compute the mean and std\n",    "            data_transformed[0, c_id, -len(cell_sim['x']):, 0] = cell_sim['x']\n",    "            data_transformed[0, c_id, -len(cell_sim['y']):, 1] = cell_sim['y']\n",    "            data_transformed[0, c_id, -len(cell_sim['t']):, 2] = cell_sim['t']\n",    "    \n",    "    return {'sim': data_transformed}\n",    "\n",    "\n",    "sumstat = SummaryStatistics(sum_stat_calculator=prepare_sumstats)                    \n",    "\n",    "if on_cluster:\n",    "    # define the model object\n",    "    model = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)\n",    "\n",    "    # note: remember also change tiff path in model.xml!\n",    "else:\n",    "    # define the model object\n",    "    model = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)\n",    "\n",    "# parameter values used to generate the synthetic data\n",    "obs_pars = {\n",    "    'gradient_strength': 100.,  # strength of the gradient of chemotaxis\n",    "    'move.strength': 10.,  # strength of directed motion\n",    "    'move.duration.mean': 0.1,  # mean of exponential distribution (seconds)\n",    "    'cell_nodes_real': 50.,  # area of the cell (\\mu m^2), macrophages have a volume of 4990\\mu m^3 -> radius of 17 if they would are sphere\n",    "}\n",    "\n",    "\n",    "obs_pars_log = {key: np.log10(val) for key, val in obs_pars.items()}\n",    "limits = {'gradient_strength': (1, 10000), #(10 ** 4, 10 ** 8),\n",    "          'move.strength': (1, 100),\n",    "          'move.duration.mean': (1e-4, 30), #(math.log10((10 ** -2) * 30), math.log10((10 ** 4))), # smallest time step in simulation 5\n",    "          'cell_nodes_real': (1, 300)}\n",    "limits_log = {key: (np.log10(val[0]), np.log10(val[1])) for key, val in limits.items()}\n",    "\n",    "\n",    "prior = pyabc.Distribution(**{key: pyabc.RV(\"uniform\", loc=lb, scale=ub-lb)\n",    "                              for key, (lb, ub) in limits_log.items()})\n",    "\n",    "param_names = ['$m_{\\\\text{dir}}$', '$m_{\\\\text{rand}}$', '$w$', '$a$']\n",    "log_param_names = ['$\\log_{10}(m_{\\\\text{dir}})$', '$\\log_{10}(m_{\\\\text{rand}})$',\n",    "                   '$\\log_{10}(w)$', '$\\log_{10}(a)$']\n",    "print(obs_pars)\n",    "print(limits_log)"   ],   "id": "74390dfb0e802a77",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "sigma0 = 550\n",    "space_x0 = 1173/2\n",    "space_y0 = 1500/1.31/2\n",    "x0, y0 = 1173/2, (1500+1500/2+270)/1.31\n",    "u1 = lambda space_x, space_y: 7/(2*np.pi*(sigma0**2)) *np.exp(-1/2*(((space_x)-(x0))**2+ ((space_y)-(y0))**2)/(sigma0**2))\n",    "\n",    "# plot the function\n",    "fig = plt.figure()\n",    "ax = fig.add_subplot(111, projection='3d')\n",    "x = np.linspace(0, 1173 , 100)\n",    "y = np.linspace(0, 2500 , 100)\n",    "X, Y = np.meshgrid(x, y)\n",    "Z = u1(X, Y)\n",    "ax.plot_surface(X, Y, Z, alpha=0.5)\n",    "# plot start points\n",    "ax.scatter(space_x0, space_y0, u1(space_x0, space_y0), color='r', s=100)\n",    "\n",    "ax.set_xlabel('space_x')\n",    "ax.set_ylabel('space_y')\n",    "plt.show()\n",    "\n",    "space_x0, space_y0, u1(space_x0, space_y0), u1(x0, y0)"   ],   "id": "be4eaa8d21c3bfff",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# load test data\n",    "np.random.seed(42+test_id)\n",    "test_params = np.array(list(prior.rvs().values()))\n",    "if not os.path.exists(os.path.join(gp, f'test_sim_{test_id}.npy')):\n",    "    raise FileNotFoundError('Test data not found')\n",    "else:\n",    "    test_sim = np.load(os.path.join(gp, f'test_sim_{test_id}.npy'))\n",    "    test_sim_full = {'sim_data': test_sim}\n",    "results_path = f'abc_results_{test_id}'\n",    "test_sim.shape"   ],   "id": "b90db5d16d7fa2b",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": "prior_draws = np.array([list(prior.rvs().values()) for _ in range(1000)])",   "id": "ab3a9bf0fcadc7d2",   "outputs": [],   "execution_count": null  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-01-07T10:49:32.834405Z",     "start_time": "2025-01-07T10:49:32.246623Z"    }   },   "cell_type": "markdown",   "source": "## ABC with Wasserstein distance",   "id": "1145158513a20ad6"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "def obj_func_wass_helper(sim: dict, obs: dict, key: str) -> float:\n",    "    x, y = np.array(sim[key]), np.array(obs[key])\n",    "    if x.size == 0 or y.size == 0:\n",    "        return np.inf\n",    "    return stats.wasserstein_distance(x, y)\n",    "\n",    "distances = {\n",    "    'msd': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='msd')),\n",    "    'ta': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='ta')),\n",    "    'vel': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='vel')),\n",    "    'ad': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='ad')),\n",    "}\n",    "\n",    "# adaptive distance\n",    "log_file_weights = f\"{results_path}/adaptive_distance_log_{test_id}.txt\"\n",    "\n",    "\n",    "adaptive_wasserstein_distance = pyabc.distance.AdaptiveAggregatedDistance(\n",    "    distances=list(distances.values()),\n",    "    scale_function=span,\n",    "    log_file=log_file_weights\n",    ")"   ],   "id": "1636e7efc136f2e0",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "#redis_sampler = RedisEvalParallelSampler(host=args.ip, port=args.port,\n",    "#                                         adapt_look_ahead_proposal=False,\n",    "#                                         look_ahead=False)\n",    "\n",    "abc = pyabc.ABCSMC(model, prior,\n",    "                   distance_function=adaptive_wasserstein_distance,\n",    "                   summary_statistics=make_sumstat_dict,\n",    "                   population_size=population_size,\n",    "                   sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",    "                   #sampler=redis_sampler\n",    "                   )\n",    "\n",    "db_path = os.path.join(gp, f\"{results_path}/synthetic_{test_id}_test_wasserstein_sumstats_adaptive.db\")\n",    "if not os.path.exists(db_path):\n",    "    history_abc = abc.new(\"sqlite:///\" + db_path, make_sumstat_dict(test_sim))\n",    "\n",    "    # start the abc fitting\n",    "    abc.run(min_acceptance_rate=1e-2, max_nr_populations=15)\n",    "    print('Done!')\n",    "    adaptive_weights = pyabc.storage.load_dict_from_json(log_file_weights)[history_abc.max_t]\n",    "else:\n",    "    history_abc = abc.load(\"sqlite:///\" + db_path)\n",    "    if len(history_abc.all_runs()) > 1:\n",    "        history_abc = abc.load(\"sqlite:///\" + db_path, abc_id=len(history_abc.all_runs()))\n",    "    #adaptive_weights = list(adaptive_weights.values())\n",    "    adaptive_weights = pyabc.storage.load_dict_from_json(log_file_weights)[history_abc.max_t]"   ],   "id": "efcbb86db464a88d",   "outputs": [],   "execution_count": null  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-01-07T10:49:48.876700Z",     "start_time": "2025-01-07T10:49:35.742219Z"    }   },   "cell_type": "markdown",   "source": "## ABC with neural network summary statistics",   "id": "df1829ce49462436"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "if os.path.exists(os.path.join(gp, 'validation_data.pickle')):\n",    "    with open(os.path.join(gp, 'validation_data.pickle'), 'rb') as f:\n",    "        valid_data = pickle.load(f)\n",    "else:\n",    "    raise FileNotFoundError('Validation data not found')\n",    "\n",    "x_mean = np.nanmean(valid_data['sim_data'], axis=(0, 1, 2))\n",    "x_std = np.nanstd(valid_data['sim_data'], axis=(0, 1, 2))\n",    "p_mean = np.mean(valid_data['prior_draws'], axis=0)\n",    "p_std = np.std(valid_data['prior_draws'], axis=0)\n",    "print('Mean and std of data:', x_mean, x_std)\n",    "print('Mean and std of parameters:', p_mean, p_std)"   ],   "id": "a7c0835963566e42",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# use trained neural net as summary statistics\n",    "def make_sumstat_dict_nn(data: Union[dict, np.ndarray], use_npe_summaries: bool = True) -> dict:\n",    "    if use_npe_summaries:\n",    "        model_id = 0\n",    "    else:\n",    "        model_id = 10\n",    "    if isinstance(data, dict):\n",    "        # get key\n",    "        key = list(data.keys())[0]\n",    "        data = data[key]\n",    "\n",    "    trainer = load_model(\n",    "        model_id=model_id,\n",    "        x_mean=x_mean,\n",    "        x_std=x_std,\n",    "        p_mean=p_mean,\n",    "        p_std=p_std,\n",    "    )\n",    "\n",    "    # configures the input for the network\n",    "    config_input = trainer.configurator({\"sim_data\": data})\n",    "    # get the summary statistics\n",    "    if isinstance(trainer, EnsembleTrainer):\n",    "        out_dict = {\n",    "            'summary_net': trainer.amortizer.summary_net(config_input).flatten()\n",    "        }\n",    "    else:\n",    "        out_dict = {\n",    "            'summary_net': trainer.amortizer.summary_net(config_input['summary_conditions']).numpy().flatten()\n",    "        }\n",    "    if model_id == 10:\n",    "        # renormalize the parameters\n",    "        out_dict['summary_net'] = out_dict['summary_net'] * p_std + p_mean\n",    "\n",    "    del trainer\n",    "    return out_dict\n",    "\n",    "\n",    "if on_cluster:\n",    "    # define the model object\n",    "    model_nn = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)\n",    "\n",    "    # note: remember also change tiff path in model.xml!\n",    "else:\n",    "    # define the model object\n",    "    model_nn = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)"   ],   "id": "bf7b71404095f91e",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# %%time\n",    "# print(make_sumstat_dict_nn(test_sim), test_params)\n",    "#\n",    "# p = 1\n",    "# summary_error = (np.abs(make_sumstat_dict_nn(test_sim, use_npe_summaries=False)['summary_net']-test_params)**p).sum() ** (1 / p)\n",    "# print(make_sumstat_dict_nn(test_sim, use_npe_summaries=False), test_params)\n",    "# print('error:', summary_error)"   ],   "id": "8522eceb33f5a581",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# abc with summary net trained on posterior mean\n",    "abc_nn = pyabc.ABCSMC(model_nn, prior, # here we use now the Euclidean distance, Wasserstein distance is not possible\n",    "                      population_size=population_size,\n",    "                      summary_statistics=partial(make_sumstat_dict_nn, use_npe_summaries=False),\n",    "                      sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",    "                      #sampler=redis_sampler\n",    "                      )\n",    "\n",    "db_path = os.path.join(gp, f\"{results_path}/synthetic_{test_id}_test_nn_sumstats_posterior_mean.db\")\n",    "\n",    "if not os.path.exists(db_path):\n",    "    history_nn = abc_nn.new(\"sqlite:///\" + db_path, make_sumstat_dict_nn(test_sim))\n",    "\n",    "    # start the abc fitting\n",    "    abc_nn.run(min_acceptance_rate=1e-2, max_nr_populations=15)\n",    "    print('Done!')\n",    "else:\n",    "    history_nn = abc_nn.load(\"sqlite:///\" + db_path)\n",    "    if len(history_nn.all_runs()) > 1:\n",    "        history_nn = abc_nn.load(\"sqlite:///\" + db_path, abc_id=len(history_nn.all_runs()))"   ],   "id": "82568acc7945ee86",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "#redis_sampler = RedisEvalParallelSampler(host=args.ip, port=args.port,\n",    "#                                         adapt_look_ahead_proposal=False,\n",    "#                                         look_ahead=False)\n",    "\n",    "# abc with summary net trained with NPE\n",    "abc_npe = pyabc.ABCSMC(model_nn, prior, # here we use now the Euclidean distance, Wasserstein distance is not possible\n",    "                      population_size=population_size,\n",    "                      summary_statistics=partial(make_sumstat_dict_nn, use_npe_summaries=True),\n",    "                      sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",    "                      #sampler=redis_sampler\n",    "                      )\n",    "\n",    "db_path = os.path.join(gp, f\"{results_path}/synthetic_{test_id}_test_nn_sumstats.db\")\n",    "if not os.path.exists(db_path):\n",    "    history_npe = abc_npe.new(\"sqlite:///\" + db_path,\n",    "                              make_sumstat_dict_nn(test_sim, use_npe_summaries=True))\n",    "\n",    "    # start the abc fitting\n",    "    abc_npe.run(min_acceptance_rate=1e-2, max_nr_populations=15)\n",    "    print('Done!')\n",    "else:\n",    "    history_npe = abc_npe.load(\"sqlite:///\" + db_path)\n",    "    if len(history_npe.all_runs()) > 1:\n",    "        # first run failed\n",    "        history_npe = abc_npe.load(\"sqlite:///\" + db_path, abc_id=len(history_npe.all_runs()))"   ],   "id": "29a244aeacb895ad",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "for hist, name in zip([history_abc, history_nn, history_npe], ['abc', 'abc_mean', 'abc_npe']):\n",    "    if hist is None:\n",    "        continue\n",    "    print(name, 'Generations:', hist.max_t)\n",    "    fig, ax = plt.subplots(1, len(param_names), tight_layout=True, figsize=(12, 4))\n",    "    for i, param in enumerate(limits.keys()):\n",    "        for t in range(hist.max_t + 1):\n",    "            df, w = hist.get_distribution(m=0, t=t)\n",    "            pyabc.visualization.plot_kde_1d(\n",    "                df,\n",    "                w,\n",    "                xmin=limits_log[param][0],\n",    "                xmax=limits_log[param][1],\n",    "                x=param,\n",    "                xname=log_param_names[i],\n",    "                ax=ax[i],\n",    "                label=f\"PDF t={t}\",\n",    "            )\n",    "        ax[i].set_xlim((limits_log[param][0]-0.2, limits_log[param][1]+0.2))\n",    "    plt.savefig(os.path.join(gp, f'{results_path}/synthetic_{test_id}_population_kdes_{name}.pdf'), bbox_inches='tight')\n",    "    plt.show()\n",    "\n",    "    fig, arr_ax = plt.subplots(1, 5, figsize=(12, 3), tight_layout=True)\n",    "    arr_ax = arr_ax.flatten()\n",    "    pyabc.visualization.plot_sample_numbers(hist, ax=arr_ax[0])\n",    "    arr_ax[0].get_legend().remove()\n",    "    pyabc.visualization.plot_walltime(hist, ax=arr_ax[1], unit='h')\n",    "    arr_ax[1].get_legend().remove()\n",    "    pyabc.visualization.plot_epsilons(hist, ax=arr_ax[2])\n",    "    pyabc.visualization.plot_effective_sample_sizes(hist, ax=arr_ax[3])\n",    "    pyabc.visualization.plot_acceptance_rates_trajectory(hist, ax=arr_ax[4])\n",    "    plt.savefig(os.path.join(gp, f'{results_path}/synthetic_{test_id}_diagnostics_{name}.pdf'), bbox_inches='tight')\n",    "    plt.show()"   ],   "id": "b7d320f9bb9b7e8c",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "markdown",   "source": "# Synthetic Tests",   "id": "287e3fde6457f51d"  },  {   "metadata": {},   "cell_type": "markdown",   "source": "## Compare Posterior Samples",   "id": "e3b932da8cac0311"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# get posterior samples\n",    "posterior_samples = {}\n",    "for hist, name in zip([history_abc, history_nn, history_npe], ['abc', 'abc_mean', 'abc_npe']):\n",    "    if hist is None:\n",    "        continue\n",    "    abc_df, abc_w = hist.get_distribution()\n",    "    posterior_samples[name] = pyabc.resample(abc_df[limits.keys()].values, abc_w, n=1000)\n",    "\n",    "# add bayesflow posterior samples\n",    "posterior_samples['npe'] = np.load(f'abc_results_{test_id}/posterior_samples_npe.npy')"   ],   "id": "488d22682851a587",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "labels_colors = {\n",    "    'abc': ('ABC with hand-crafted summaries', '#9AB8D7'),\n",    "    'abc_mean': ('ABC with posterior mean summaries', '#C4B7D4'),\n",    "    'abc_npe': ('ABC with inference-tailored summaries', '#EEBC88'),\n",    "    'npe': ('NPE with jointly learned summaries', '#A7CE97')\n",    "}\n",    "\n",    "colors = [labels_colors[name][1] for name in posterior_samples.keys()]\n",    "labels = [labels_colors[name][0] for name in posterior_samples.keys()]"   ],   "id": "4f919a570c48e60d",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "fig = plot_posterior_1d(\n",    "    posterior_samples=posterior_samples,\n",    "    prior_draws=prior_draws,\n",    "    log_param_names=log_param_names,\n",    "    test_sim=test_sim,\n",    "    test_params=test_params,\n",    "    labels_colors=labels_colors,\n",    "    make_sumstat_dict_nn=make_sumstat_dict_nn,\n",    "    save_path=os.path.join(gp, f'{results_path}/synthetic_posterior_all_rows.pdf')\n",    ")"   ],   "id": "fb7c8bdd7244f0a7",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "prior_mean = np.mean(prior_draws, axis=0)\n",    "prior_std = np.std(prior_draws, axis=0)\n",    "\n",    "def compute_z_score(posterior_mean):\n",    "    return (posterior_mean - prior_mean) / prior_std\n",    "\n",    "def compute_contraction(posterior_std):\n",    "    return 1. - (posterior_std / prior_std)\n",    "\n",    "posterior_stats = {}\n",    "for name, ps in posterior_samples.items():\n",    "    posterior_stats[name] = {\n",    "        'mean': np.mean(ps, axis=0),\n",    "        'std': np.std(ps, axis=0),\n",    "        'median': np.median(ps, axis=0),\n",    "        'z_score': compute_z_score(np.mean(ps, axis=0)),\n",    "        'contraction': compute_contraction(np.std(ps, axis=0))\n",    "    }"   ],   "id": "87b27ca1a2a0f549",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "z_scores = [posterior_stats[name]['z_score'] for name in posterior_samples.keys()]\n",    "contractions = [posterior_stats[name]['contraction'] for name in posterior_samples.keys()]\n",    "\n",    "# Plotting Z-scores and contractions for both methods\n",    "fig, ax1 = plt.subplots(figsize=(8, 4), tight_layout=True)\n",    "\n",    "# Z-Scores for both methods\n",    "for i, z in enumerate(z_scores):\n",    "    ax1.bar(np.arange(len(param_names)) + 0.2 * i, z, width=0.15, align='center', color=colors[i],\n",    "            label=labels[i])\n",    "ax1.set_ylabel('Z-Score')\n",    "ax1.tick_params(axis='y')\n",    "ax1.set_xticks(np.arange(len(param_names)) + 0.3)\n",    "ax1.set_xticklabels(log_param_names)\n",    "ax1.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",    "ax1.grid()\n",    "\n",    "# Plot Contractions for both methods on secondary axis\n",    "ax2 = ax1.twinx()\n",    "for i, c in enumerate(contractions):\n",    "    ax2.plot(np.arange(0.3, len(param_names)), c, color=colors[i], marker='o', linestyle='--')\n",    "ax2.set_ylabel('--●--  Contraction')\n",    "ax2.tick_params(axis='y')\n",    "# get maximal y-limit of ax1\n",    "max_y = max(ax1.get_ylim()[1], ax2.get_ylim()[1])\n",    "min_y = min(ax1.get_ylim()[0], ax2.get_ylim()[0])\n",    "ax1.set_ylim(min_y, max_y)\n",    "ax2.set_ylim(min_y, max_y)\n",    "\n",    "# Combine legends\n",    "handles1, labels1 = ax1.get_legend_handles_labels()\n",    "fig.legend(handles1, labels1,\n",    "           loc='lower center', bbox_to_anchor=(0.5, -0.12), ncol=2)\n",    "fig.savefig(os.path.join(gp, f'{results_path}/synthetic_z_scores_contraction.pdf'), bbox_inches='tight')\n",    "plt.show()"   ],   "id": "c851aaf76d152b11",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "#ordering = [0,4,1,5,2,6,3,7]\n",    "ordering = np.concatenate([[i,i+4, i+8, i+12] for i in range(4)])\n",    "all_params = np.concatenate((posterior_samples['abc'],\n",    "                             posterior_samples['abc_mean'],\n",    "                             posterior_samples['abc_npe'],\n",    "                             posterior_samples['npe']), axis=-1)\n",    "log_param_names_plot = np.array(\n",    "    [f'{n} $\\qquad$ ABC hand-crafted' for n in log_param_names] +\n",    "    [f'ABC posterior mean' for n in log_param_names] +\n",    "    [f'ABC inference tailored' for n in log_param_names] +\n",    "    [f'NPE' for n in log_param_names]\n",    ")[ordering]\n",    "param_names_plot = np.array(\n",    "    [f'{n} $\\qquad$ ABC hand-crafted' for n in param_names] +\n",    "    [f'ABC posterior mean' for n in param_names] +\n",    "    [f'ABC inference tailored' for n in param_names] +\n",    "    [f'NPE' for n in param_names]\n",    ")[ordering]\n",    "color_list = colors*len(param_names)\n",    "\n",    "ax = sampling_parameter_cis(\n",    "    all_params[:, ordering],\n",    "    true_param=np.concatenate((test_params, test_params, test_params, test_params))[ordering] if test_params is not None else None,\n",    "    prior_bounds=limits_log.values() if test_params is not None else None,\n",    "    param_names=log_param_names_plot,\n",    "    alpha=[95, 90 , 80],\n",    "    color_list=color_list,\n",    "    show_median=False if test_params is not None else True,\n",    "    size=(7, 4),\n",    "    legend_bbox_to_anchor=(0.45,1) if test_params is not None else (0.31,1)\n",    ")\n",    "plt.savefig(os.path.join(gp, f'{results_path}/synthetic_posterior_credible_intervals_log.pdf'))\n",    "plt.show()\n",    "\n",    "all_params = np.power(10, all_params)\n",    "ax = sampling_parameter_cis(\n",    "    all_params[:, ordering],\n",    "    true_param=np.power(10, np.concatenate((test_params, test_params, test_params, test_params))[ordering]) if test_params is not None else None,\n",    "    param_names=param_names_plot,\n",    "    alpha=[95, 90 , 80],\n",    "    color_list=color_list,\n",    "    show_median=False if test_params is not None else True,\n",    "    size=(7, 4),\n",    "    legend_bbox_to_anchor=(0.99,0.35)\n",    ")\n",    "#plt.savefig(os.path.join(gp, f'{results_path}/synthetic_posterior_credible_intervals.pdf'))\n",    "plt.show()"   ],   "id": "4033f43895a45b0f",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "markdown",   "source": "## Compare Simulations from Posterior Samples",   "id": "6ed9a874bf49cd3b"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "file_name = os.path.join(gp, f'{results_path}/synthetic_posterior_simulations.pickle')\n",    "if os.path.exists(file_name):\n",    "    with open(file_name, 'rb') as f:\n",    "        posterior_simulations = pickle.load(f)\n",    "else:\n",    "    posterior_simulations = {}\n",    "    for name, ps in posterior_samples.items():\n",    "        @delayed\n",    "        def wrapper_fun(sample_i):\n",    "            _sim_dict = {key: p for key, p in zip(obs_pars.keys(), ps[sample_i])}\n",    "            _posterior_sim = model(_sim_dict)\n",    "            return _posterior_sim['sim']\n",    "\n",    "        sim_list = Parallel(n_jobs=n_procs, verbose=1)(wrapper_fun(i) for i in range(10))\n",    "        posterior_simulations[name] = np.concatenate(sim_list)\n",    "\n",    "    with open(file_name, 'wb') as f:\n",    "        pickle.dump(posterior_simulations, f)\n",    "\n",    "simulation_sumstats = {}\n",    "for name, ps in posterior_simulations.items():\n",    "    simulation_sumstats[name] = [make_sumstat_dict(p_sim[np.newaxis]) for p_sim in ps]\n",    "    for i in range(len(simulation_sumstats[name])):\n",    "        simulation_sumstats[name][i]['nn'] = make_sumstat_dict_nn(ps[i][np.newaxis], use_npe_summaries=True)['summary_net']"   ],   "id": "903ac7db2e60002a",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "def obj_func_comparison(sim: dict, obs: dict, return_marginal: bool = False, weights: Union[dict, list] = None) -> Union[float, np.ndarray]:\n",    "    total = np.zeros(len(sim.keys()))\n",    "    for k_i, key in enumerate(sim):\n",    "        if key == 'nn':\n",    "            # for the neural network summary statistics we use the Euclidean distance\n",    "            total[k_i] = euclidean_distance(sim['nn'], obs['nn'])\n",    "            continue # no weights applied\n",    "        else:\n",    "            total[k_i] = distances[key](sim, obs)\n",    "        if weights is not None:\n",    "            if isinstance(weights, dict):\n",    "                total[k_i] = total[k_i] * weights[key]\n",    "            elif isinstance(weights, list):\n",    "                total[k_i] = total[k_i] * weights[k_i]\n",    "            else:\n",    "                raise ValueError('Weights must be a list or a dictionary')\n",    "    if return_marginal:\n",    "        return total\n",    "    return total.sum()"   ],   "id": "56c4846465fcc2ef",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "test_sim_dict = make_sumstat_dict(test_sim)\n",    "test_sim_dict['nn'] = make_sumstat_dict_nn(test_sim, use_npe_summaries=True)['summary_net']"   ],   "id": "1de2616c39309571",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "plot_sumstats_distance_stats(obj_func_comparison,\n",    "                             test_sim_dict,\n",    "                             [ps for ps in simulation_sumstats.values()],\n",    "                             labels=labels,\n",    "                             title='Wasserstein Distance',\n",    "                             colors=colors,\n",    "                             ylog_scale=True,\n",    "                             path=os.path.join(gp, f'{results_path}/synthetic_sumstats_comparison.pdf')\n",    "                             )\n",    "\n",    "print(*test_sim_dict.keys())\n",    "print(adaptive_weights)\n",    "plot_sumstats_distance_stats(partial(obj_func_comparison, weights=adaptive_weights),\n",    "                             test_sim_dict,\n",    "                             [ps for ps in simulation_sumstats.values()],\n",    "                             labels=labels,\n",    "                             colors=colors,\n",    "                             ylog_scale=True,\n",    "                             #path=os.path.join(gp, f'{results_path}/synthetic_sumstats_comparison.pdf')\n",    "                             )"   ],   "id": "c2e7c4e0aacd9d18",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": "",   "id": "389bfdd06d9dbc1d",   "outputs": [],   "execution_count": null  } ], "metadata": {  "kernelspec": {   "display_name": "Python 3",   "language": "python",   "name": "python3"  },  "language_info": {   "codemirror_mode": {    "name": "ipython",    "version": 2   },   "file_extension": ".py",   "mimetype": "text/x-python",   "name": "python",   "nbconvert_exporter": "python",   "pygments_lexer": "ipython2",   "version": "2.7.6"  } }, "nbformat": 4, "nbformat_minor": 5}