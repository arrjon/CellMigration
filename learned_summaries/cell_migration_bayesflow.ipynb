{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference with Neural Posterior Estimation (NPE)",
   "id": "3d684170c7ac2952"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pyabc\n",
    "from fitmulticell import model as morpheus_model\n",
    "from fitmulticell.sumstat import SummaryStatistics\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import umap\n",
    "from sklearn.linear_model import Lasso\n",
    "import seaborn as sns\n",
    "\n",
    "from load_bayesflow_model import load_model, custom_loader, EnsembleTrainer\n",
    "from plotting_routines import plot_posterior_2d\n",
    "from summary_stats import reduce_to_coordinates\n",
    "\n",
    "import tensorflow as tf\n",
    "import bayesflow as bf\n",
    "from bayesflow.simulation import GenerativeModel, Prior, Simulator\n",
    "\n",
    "# get the job array id and number of processors\n",
    "job_array_id = 0 #int(os.environ.get('SLURM_ARRAY_TASK_ID', 0))\n",
    "n_procs = 10 #int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",
    "print(job_array_id)\n",
    "on_cluster = False"
   ],
   "id": "10b55f49c4d5c93c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if on_cluster:\n",
    "    gp = '/home/jarruda_hpc/CellMigration/synth_data_params_bayesflow'\n",
    "else:\n",
    "    gp = os.getcwd()\n",
    "\n",
    "# defining the mapping of parameter inside the model xml file. the dictionary name is for \n",
    "# parameter name, and the value are the mapping values, to get the map value for parameter \n",
    "# check here: https://fitmulticell.readthedocs.io/en/latest/example/minimal.html#Inference-problem-definition\n",
    "\n",
    "par_map = {\n",
    "    'gradient_strength': './CellTypes/CellType/Constant[@symbol=\"gradient_strength\"]',\n",
    "    'move.strength': './CellTypes/CellType/Constant[@symbol=\"move.strength\"]',\n",
    "    'move.duration.mean': './CellTypes/CellType/Constant[@symbol=\"move.duration.mean\"]',\n",
    "    'cell_nodes_real': './Global/Constant[@symbol=\"cell_nodes_real\"]',\n",
    "}\n",
    "\n",
    "model_path = gp + \"/cell_movement_v24.xml\"  # time step is 30sec, move.dir completely normalized, init move.dir rand in all directions\n",
    "# defining the summary statistics function\n",
    "max_sequence_length = 120\n",
    "min_sequence_length = 0\n",
    "only_longest_traj_per_cell = True  # mainly to keep the data batchable\n",
    "sumstat = SummaryStatistics(sum_stat_calculator=partial(reduce_to_coordinates,\n",
    "                                                        minimal_length=min_sequence_length,\n",
    "                                                        maximal_length=max_sequence_length,\n",
    "                                                        only_longest_traj_per_cell=only_longest_traj_per_cell))                    \n",
    "\n",
    "if on_cluster:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "    # todo: remember also change tiff path in model.xml!\n",
    "else:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "\n",
    "# parameter values used to generate the synthetic data\n",
    "obs_pars = {\n",
    "    'gradient_strength': 100.,  # strength of the gradient of chemotaxis (energy potential)\n",
    "    'move.strength': 10.,  # strength of directed motion (energy potential)\n",
    "    'move.duration.mean': 0.1,  # mean of exponential distribution (seconds)\n",
    "    'cell_nodes_real': 50.,  # area of the cell  (\\mu m^2)\n",
    "}\n",
    "\n",
    "obs_pars_log = {key: np.log10(val) for key, val in obs_pars.items()}\n",
    "limits = {'gradient_strength': (1, 10000), #(10 ** 4, 10 ** 8),\n",
    "          'move.strength': (1, 100),\n",
    "          'move.duration.mean': (1e-4, 30), #(math.log10((10 ** -2) * 30), math.log10((10 ** 4))), # smallest time step in simulation 5\n",
    "          'cell_nodes_real': (1, 300)}\n",
    "limits_log = {key: (np.log10(val[0]), np.log10(val[1])) for key, val in limits.items()}\n",
    "\n",
    "\n",
    "prior = pyabc.Distribution(**{key: pyabc.RV(\"uniform\", loc=lb, scale=ub-lb)\n",
    "                              for key, (lb, ub) in limits_log.items()})\n",
    "#param_names = list(obs_pars.keys())\n",
    "#log_param_names = [f'log_{p}' for p in param_names]\n",
    "param_names = ['$m_{\\\\text{dir}}$', '$m_{\\\\text{rand}}$', '$w$', '$a$']\n",
    "log_param_names = ['$\\log_{10}(m_{\\\\text{dir}})$', '$\\log_{10}(m_{\\\\text{rand}})$', '$\\log_{10}(w)$', '$\\log_{10}(a)$']\n",
    "print(obs_pars)"
   ],
   "id": "74390dfb0e802a77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prior_fun(batch_size: int) -> np.ndarray:\n",
    "    samples = []\n",
    "    for _ in range(batch_size):\n",
    "        samples.append(list(prior.rvs().values()))\n",
    "    return np.array(samples)\n",
    "\n",
    "\n",
    "def generate_population_data(param_batch: np.ndarray, cells_in_population: int, max_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate population data\n",
    "    :param param_batch:  batch of parameters\n",
    "    :param cells_in_population:  number of cells in a population (50)\n",
    "    :param max_length:  maximum length of the sequence\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_batch = []\n",
    "    for params in param_batch:\n",
    "        params_dict = {key: p for key, p in zip(obs_pars.keys(), params)}\n",
    "        sim = model.sample(params_dict)\n",
    "        data_batch.append(sim)  # generates a cell population in one experiment\n",
    "\n",
    "    data_batch_transformed = np.ones((param_batch.shape[0], cells_in_population, max_length, 2)) * np.nan\n",
    "    # each cell is of different length, each with x and y coordinates, make a tensor out of it\n",
    "    n_cells_not_visible = 0\n",
    "    for p_id, population_sim in enumerate(data_batch):\n",
    "        if len(population_sim) == 0:\n",
    "            # no cells were visible in the simulation\n",
    "            n_cells_not_visible += 1\n",
    "            continue\n",
    "        for c_id, cell_sim in enumerate(population_sim):\n",
    "            # pre-pad the data with zeros, but first write zeros as nans to compute the mean and std\n",
    "            data_batch_transformed[p_id, c_id, -len(cell_sim['x']):, 0] = cell_sim['x']\n",
    "            data_batch_transformed[p_id, c_id, -len(cell_sim['y']):, 1] = cell_sim['y']\n",
    "\n",
    "    if n_cells_not_visible > 0:\n",
    "        print(f'Simulation with no cells visible: {n_cells_not_visible}/{len(data_batch)}')\n",
    "    return data_batch_transformed"
   ],
   "id": "c606f75e3a4c0257",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "presimulate = False\n",
    "presimulation_path = 'presimulations'\n",
    "n_val_data = 100\n",
    "cells_in_population = 50\n",
    "n_params = len(obs_pars)\n",
    "batch_size = 32\n",
    "iterations_per_epoch = 100\n",
    "# 1000 batches to be generated, 10 epochs until the batch is used again\n",
    "epochs = 500\n",
    "\n",
    "# check if gpu is available\n",
    "print('gpu:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "bayesflow_prior = Prior(batch_prior_fun=prior_fun, param_names=param_names)\n",
    "bayes_simulator = Simulator(batch_simulator_fun=partial(generate_population_data,\n",
    "                                                        cells_in_population=cells_in_population,\n",
    "                                                        max_length=max_sequence_length))\n",
    "generative_model = GenerativeModel(prior=bayesflow_prior, simulator=bayes_simulator,\n",
    "                                   skip_test=True,  # once is enough, simulation takes time\n",
    "                                   name=\"Normalizing Flow Generative Model\")"
   ],
   "id": "cecd0d3e2713e759",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if presimulate:\n",
    "    print('presimulating')\n",
    "    from time import sleep\n",
    "    sleep(job_array_id)\n",
    "\n",
    "    # we create on batch per job and save it in a folder\n",
    "    epoch_id = job_array_id // iterations_per_epoch\n",
    "    generative_model.presimulate_and_save(\n",
    "        batch_size=batch_size,\n",
    "        folder_path=presimulation_path+f'/epoch_{epoch_id}',\n",
    "        iterations_per_epoch=1,\n",
    "        epochs=1,\n",
    "        extend_from=job_array_id,\n",
    "        disable_user_input=True\n",
    "    )\n",
    "    print('Done!')"
   ],
   "id": "958a416f9ac30668",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.exists(os.path.join(gp, 'validation_data.pickle')):\n",
    "    with open(os.path.join(gp, 'validation_data.pickle'), 'rb') as f:\n",
    "        valid_data = pickle.load(f)\n",
    "else:\n",
    "    print('Generating validation data')\n",
    "    valid_data = generative_model(n_val_data)\n",
    "    # save the data\n",
    "    with open(os.path.join(gp, 'validation_data.pickle'), 'wb') as f:\n",
    "        pickle.dump(valid_data, f)\n",
    "\n",
    "x_mean = np.nanmean(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "x_std = np.nanstd(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "p_mean = np.mean(valid_data['prior_draws'], axis=0)\n",
    "p_std = np.std(valid_data['prior_draws'], axis=0)\n",
    "print('Mean and std of data:', x_mean, x_std)\n",
    "print('Mean and std of parameters:', p_mean, p_std)"
   ],
   "id": "df1829ce49462436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "job_array_id = 3  # best: 2, ensemble: 3, only summary: 10\n",
    "trainer, map_idx_sim = load_model(\n",
    "    model_id=job_array_id, \n",
    "    x_mean=x_mean,\n",
    "    x_std=x_std,\n",
    "    p_mean=p_mean,\n",
    "    p_std=p_std,\n",
    "    generative_model=generative_model\n",
    ")"
   ],
   "id": "b81539411969d014",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check if the model is already trained\n",
    "if not os.path.exists(trainer.checkpoint_path) and not isinstance(trainer, EnsembleTrainer):\n",
    "    trainer._setup_optimizer(\n",
    "        optimizer=None,\n",
    "        epochs=epochs,\n",
    "        iterations_per_epoch=iterations_per_epoch\n",
    "    )\n",
    "\n",
    "    history = trainer.train_from_presimulation(\n",
    "        presimulation_path=presimulation_path,\n",
    "        optimizer=trainer.optimizer,\n",
    "        max_epochs=epochs,\n",
    "        early_stopping=True,\n",
    "        early_stopping_args={'patience': 17 - 2},\n",
    "        custom_loader=custom_loader,\n",
    "        validation_sims=valid_data\n",
    "    )\n",
    "    print('Training done!')\n",
    "else:\n",
    "    history = trainer.loss_history.get_plottable()\n",
    "\n",
    "bf.diagnostics.plot_losses(history['train_losses'], history['val_losses'], fig_size=(10, 6))\n",
    "print('Final validation loss:', np.min(history['val_losses']))"
   ],
   "id": "a7c0835963566e42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Diagnostic plots",
   "id": "c72ff040811c2643"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "valid_data_config = trainer.configurator(valid_data)",
   "id": "e6a730d966e83d26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "posterior_samples = trainer.amortizer.sample(valid_data_config, n_samples=1000)\n",
    "posterior_samples = posterior_samples * p_std + p_mean\n",
    "if isinstance(valid_data_config, list):  # for ensemble\n",
    "    prior_draws = valid_data_config[0][\"parameters\"] * p_std + p_mean\n",
    "else:\n",
    "    prior_draws = valid_data_config[\"parameters\"] * p_std + p_mean"
   ],
   "id": "f6ad80516d7191c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bf.diagnostics.plot_sbc_ecdf(posterior_samples, prior_draws, difference=True, param_names=log_param_names);\n",
    "#plt.savefig(f'{trainer.checkpoint_path}/sbc_ecdf.png')"
   ],
   "id": "bf28748d802ce799",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "bf.diagnostics.plot_recovery(posterior_samples, prior_draws, uncertainty_agg=None,\n",
    "                             param_names=log_param_names);\n",
    "#plt.savefig(f'{trainer.checkpoint_path}/recovery.png')"
   ],
   "id": "a8a443ecb123ad2e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "bf.diagnostics.plot_z_score_contraction(posterior_samples, prior_draws, param_names=log_param_names);",
   "id": "69522e989ab5747d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# regress summary statistics to parameters to identify the most important latent dimensions\n",
    "if isinstance(trainer, EnsembleTrainer):\n",
    "    assert False  # changes the valid_data_config to a list\n",
    "    summary_output = trainer.amortizer.summary_net(valid_data_config)\n",
    "    valid_data_config = valid_data_config[0]\n",
    "else:\n",
    "    summary_output = trainer.amortizer.summary_net(valid_data_config['summary_conditions'])\n",
    "\n",
    "if job_array_id == 10:\n",
    "    summary_output = summary_output * p_std + p_mean\n",
    "    fig = bf.diagnostics.plot_recovery(summary_output[:, np.newaxis], prior_draws, param_names=log_param_names)\n",
    "    plt.savefig(f'{trainer.checkpoint_path}/summary_space_recovery.png')\n",
    "\n",
    "# perform a regression of each parameter to the summary statistics\n",
    "regressors = []\n",
    "for i in range(valid_data_config['parameters'].shape[1]):\n",
    "    reg = Lasso(alpha=0.1).fit(summary_output, valid_data_config['parameters'][:, i])\n",
    "    regressors.append(reg.coef_)\n",
    "\n",
    "# Convert list of coefficients to a NumPy array for visualization\n",
    "coeff_matrix = np.array(regressors).T\n",
    "\n",
    "# Identify the parameter for which each latent dimension is most important\n",
    "dominant_param = np.argmax(np.abs(coeff_matrix), axis=1)\n",
    "\n",
    "# Group latent dimensions based on the dominant parameter\n",
    "grouped_indices = []\n",
    "for param_idx in range(n_params):\n",
    "    group = [i for i in range(summary_output.shape[1]) if dominant_param[i] == param_idx]\n",
    "    grouped_indices.extend(group)\n",
    "\n",
    "# Reorder the coefficient matrix based on the grouped indices\n",
    "grouped_coeff_matrix = coeff_matrix[grouped_indices, :]\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(\n",
    "    np.abs(grouped_coeff_matrix).T,\n",
    "    annot=True,\n",
    "    cmap=sns.color_palette(\"flare\", as_cmap=True),\n",
    "    yticklabels=log_param_names,\n",
    "    xticklabels=[f\"Dim {i}\" for i in range(summary_output.shape[1])],\n",
    "    cbar_kws={'label': 'Absolute Coefficient Value'}\n",
    ")\n",
    "#plt.title(\"Regression Coefficients: Latent Dimensions vs Model Parameters\")\n",
    "plt.ylabel(\"Model Parameters\")\n",
    "plt.xlabel(\"Latent Dimensions\")\n",
    "#plt.savefig(f'{trainer.checkpoint_path}/summary_space_regression_coefficients.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# apply a UMAP to the summary statistics\n",
    "reducer = umap.UMAP(random_state=42, n_jobs=1)\n",
    "embedding = reducer.fit_transform(summary_output)\n",
    "\n",
    "fix, ax = plt.subplots(1, n_params, sharey=True, sharex=True, figsize=(12, 3))\n",
    "for i in range(n_params):\n",
    "    # color code base on size of parameter\n",
    "    colors = valid_data_config['parameters'][:, i]\n",
    "    # min max scaling\n",
    "    colors = (colors - np.min(colors)) / (np.max(colors) - np.min(colors))\n",
    "    # map to colormap\n",
    "    colormap = plt.get_cmap('flare')\n",
    "    colors = colormap(colors)\n",
    "\n",
    "    ax[i].scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        # color code base on size of parameter\n",
    "        c=colors,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    ax[i].set_title(f\"{log_param_names[i]}\")\n",
    "\n",
    "# add colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=colormap, norm=plt.Normalize(vmin=np.min(colors), vmax=np.max(colors)))\n",
    "sm._A = []\n",
    "plt.colorbar(sm, ax=ax, label='Normalized Parameter Value')\n",
    "#plt.savefig(f'{trainer.checkpoint_path}/summary_space_umap.png')\n",
    "plt.show()"
   ],
   "id": "681603b5f2e87a03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test on synthetic data",
   "id": "26f7a6d31b5bf000"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# simulate test data\n",
    "for test_id in [0, 1, 2]:\n",
    "    print(test_id)\n",
    "    np.random.seed(test_id)\n",
    "    test_params = np.array(list(prior.rvs().values()))\n",
    "    if not os.path.exists(os.path.join(gp, f'test_sim_{test_id}.npy')):\n",
    "        test_sim_full = bayes_simulator(test_params[np.newaxis])\n",
    "        test_sim = test_sim_full['sim_data']\n",
    "        np.save(os.path.join(gp, f'test_sim_{test_id}.npy'), test_sim)\n",
    "    else:\n",
    "        test_sim = np.load(os.path.join(gp, f'test_sim_{test_id}.npy'))\n",
    "        test_sim_full = {'sim_data': test_sim}\n",
    "\n",
    "    test_posterior_samples = trainer.amortizer.sample(trainer.configurator(test_sim_full), n_samples=1000)\n",
    "    test_posterior_samples = test_posterior_samples * p_std + p_mean\n",
    "    test_posterior_samples_median = np.median(test_posterior_samples, axis=0)\n",
    "    # compute the log posterior of the test data\n",
    "    input_dict = {\n",
    "        'sim_data': np.repeat(test_sim, repeats=100, axis=0),\n",
    "        'parameters': test_posterior_samples\n",
    "    }\n",
    "\n",
    "    # save posterior samples to load for abc comparison\n",
    "    #np.save(f'abc_results_{test_id}/posterior_samples_npe.npy', test_posterior_samples)\n",
    "\n",
    "    fig = plot_posterior_2d(posterior_draws=test_posterior_samples,\n",
    "                        prior_draws=prior_draws[:test_posterior_samples.shape[0]],\n",
    "                        param_names=log_param_names,\n",
    "                        true_params=test_params)\n",
    "    plt.show()"
   ],
   "id": "aaf9d11a06bc442f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Real Data\n",
    "\n",
    "Here we always use the ensemble network!"
   ],
   "id": "dc88096666755040"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if not isinstance(trainer, EnsembleTrainer):\n",
    "    raise ValueError('To reproduce results use the ensemble network.')"
   ],
   "id": "420c2b14ccdeee35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from load_data import load_real_data\n",
    "\n",
    "wasserstein_distance_dict = {0: np.nan, 1: np.nan}\n",
    "samples_dict = {0: np.nan, 1: np.nan}\n",
    "prior_draws = prior_fun(1000)"
   ],
   "id": "88f5e4f6b323ebcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "real_data_id = [0, 1][1]\n",
    "_, real_data_full = load_real_data(data_id=real_data_id,\n",
    "                                   max_sequence_length=max_sequence_length,\n",
    "                                   cells_in_population=cells_in_population,\n",
    "                                   plot_data=True)\n",
    "print(real_data_full.shape)"
   ],
   "id": "748dffc6b156ef8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# make sure that the sub-selected data size is well mixed based on track lengths\n",
    "# import pandas as pd\n",
    "\n",
    "# track_length = []\n",
    "# for cell in real_data_full:\n",
    "#     new_track_length = max_sequence_length-np.isnan(cell[:, 0]).sum()\n",
    "#     track_length.append(new_track_length)\n",
    "# track_length = np.array(track_length)\n",
    "\n",
    "# # Convert to DataFrame for handling\n",
    "# data = pd.DataFrame({'track_length': track_length, 'id': np.arange(len(track_length))})\n",
    "#\n",
    "# # Define number of bins\n",
    "# num_bins = 10  # Adjust for resolution\n",
    "# data['bin'] = pd.cut(data['track_length'], bins=num_bins)\n",
    "#\n",
    "# # Sample an equal number of points from each bin\n",
    "# sampled_data = data.groupby('bin').apply(lambda x: x.sample(min(len(x), 5))).reset_index(drop=True)\n",
    "# real_data_50 = real_data_full[sampled_data['id']]"
   ],
   "id": "52a039b01673630d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Zelldichte macht was aus\n",
    "\n",
    "Wo wollen Zellen hin? welche parameter beeinflussen das ganze? Zelldichte?\n",
    "\n",
    "1,5mm\n",
    "\n",
    "1 (nicht so gut, extrema)\n",
    "739.79x279.74  microns\n",
    "20231x768 pixel\n",
    "\n",
    "\n",
    "2 (wesentlich mehr der Wahrheit)\n",
    "882.94x287.03 microns\n",
    "2424x788 pixel\n"
   ],
   "id": "3212d01370d344d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# batch the real data\n",
    "n_cell_in_batch = [len(real_data_full), cells_in_population][0]  # 0: joint, 1: combined posterior\n",
    "batches = [real_data_full[i * n_cell_in_batch:(i + 1) * n_cell_in_batch]\n",
    "           for i in range(len(real_data_full) // n_cell_in_batch)]\n",
    "#batches = [real_data_50]  # single posterior\n",
    "real_posterior_samples_full = []\n",
    "n_samples = prior_draws.shape[0] // len(batches)\n",
    "\n",
    "for b in tqdm(batches):\n",
    "    print(b.shape)\n",
    "    real_posterior_samples = trainer.amortizer.sample(trainer.configurator({'sim_data': b[np.newaxis]}), \n",
    "                                          n_samples=n_samples)\n",
    "    real_posterior_samples = real_posterior_samples * p_std + p_mean\n",
    "    real_posterior_samples_full.append(real_posterior_samples)\n",
    "\n",
    "real_posterior_samples = np.concatenate(real_posterior_samples_full)\n",
    "#np.save(f'abc_results_real/posterior_samples_npe.npy', real_posterior_samples)\n",
    "samples_dict[real_data_id] = real_posterior_samples"
   ],
   "id": "17248751d4533848",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = plot_posterior_2d(posterior_draws=real_posterior_samples,\n",
    "                        prior_draws=prior_draws[:real_posterior_samples.shape[0]],\n",
    "                        param_names=log_param_names)\n",
    "#plt.savefig(f'{trainer.checkpoint_path}/full_posterior.png')\n",
    "plt.show()"
   ],
   "id": "aeefa13b15ae4ce6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if trainer.amortizer.summary_loss is not None:\n",
    "    from matplotlib.cm import viridis\n",
    "    real_data_config = trainer.configurator({'sim_data': batches})\n",
    "    summary_statistics = trainer.amortizer.summary_net(valid_data_config['summary_conditions'])\n",
    "    summary_statistics_obs = trainer.amortizer.summary_net(real_data_config['summary_conditions'])\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    colors = viridis(np.linspace(0.1, 0.9, 2))\n",
    "    ax.scatter(\n",
    "        summary_statistics_obs[:, 0], summary_statistics_obs[:, 1], color=colors[0], label=r\"Observed: $h_{\\psi}(x_{obs})$\"\n",
    "    )\n",
    "    ax.scatter(summary_statistics[:, 0], summary_statistics[:, 1], color=colors[1], label=r\"Well-specified: $h_{\\psi}(x)$\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.2)\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "    \n",
    "    #fig.savefig(f'abc_results_real/Real_{real_data_id} Summary Latent Space.png', bbox_inches='tight')\n",
    "    \n",
    "    MMD_sampling_distribution, MMD_observed = trainer.mmd_hypothesis_test(\n",
    "        observed_data=real_data_config, \n",
    "        reference_data=valid_data_config,  # if not provided, will use the generative model\n",
    "        num_null_samples=500,\n",
    "        bootstrap=True  # if True, use the reference data as null samples\n",
    "    )\n",
    "    fig = bf.diagnostics.plot_mmd_hypothesis_test(MMD_sampling_distribution, MMD_observed)\n",
    "    #fig.savefig(f'abc_results_real/Real_{real_data_id} MMD.png', bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "id": "d79424027ea8180a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Different Number of Cells in Experiment",
   "id": "1d4036aaff03efd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# batch the real data\n",
    "n_cell_in_batch = 1\n",
    "batches = [real_data_full[i * n_cell_in_batch:(i + 1) * n_cell_in_batch]\n",
    "           for i in range(len(real_data_full) // n_cell_in_batch)]\n",
    "real_posterior_samples_full = []\n",
    "\n",
    "for b in tqdm(batches):\n",
    "    real_posterior_samples = trainer.amortizer.sample(trainer.configurator({'sim_data': b[np.newaxis]}),\n",
    "                                          n_samples=100)\n",
    "    real_posterior_samples = real_posterior_samples * p_std + p_mean\n",
    "    real_posterior_samples_full.append(real_posterior_samples)"
   ],
   "id": "b5283e6aa909489",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compute summary stats and correlation with individual parameters\n",
    "# might not be a really reliable estimate of the parameter though\n",
    "from summary_stats import velocity, angle_degree, MSD, turning_angle\n",
    "from scipy.stats import linregress\n",
    "from scipy.stats import median_abs_deviation\n",
    "\n",
    "track_length = []\n",
    "\n",
    "turning_angles = []\n",
    "angle_degrees = []\n",
    "msds = []\n",
    "velo = []\n",
    "\n",
    "chemokine_strength = []\n",
    "persistence_strength = []\n",
    "waiting_time = []\n",
    "cell_volume = []\n",
    "for ps, cell in zip(real_posterior_samples_full, real_data_full):\n",
    "    new_track_length = max_sequence_length-np.isnan(cell[:, 0]).sum()\n",
    "    if new_track_length > 2:\n",
    "        track_length.append(new_track_length)\n",
    "\n",
    "        turning_angles.append(np.nanmean(turning_angle(dict(x=cell[:, 0], y=cell[:, 1]))))\n",
    "        angle_degrees.append(np.nanmean(angle_degree(dict(x=cell[:, 0], y=cell[:, 1]))))\n",
    "        msds.append(np.nanmean(MSD(dict(x=cell[:, 0], y=cell[:, 1]), all_time_lags=False)))\n",
    "        velo.append(np.nanmean(velocity(dict(x=cell[:, 0], y=cell[:, 1]))))\n",
    "\n",
    "        chemokine_strength.append(np.mean(ps[:, 0]))\n",
    "        persistence_strength.append(np.mean(ps[:, 1]))\n",
    "        waiting_time.append(np.mean(ps[:, 2]))\n",
    "        cell_volume.append(np.mean(ps[:, 3]))\n",
    "\n",
    "track_length = np.array(track_length)\n",
    "turning_angles, angle_degrees, msds, velo = np.array(turning_angles), np.array(angle_degrees), np.array(msds), np.array(velo)\n",
    "chemokine_strength, persistence_strength, waiting_time, cell_volume = np.array(chemokine_strength), np.array(persistence_strength), np.array(waiting_time), np.array(cell_volume)\n",
    "\n",
    "list_params = [chemokine_strength, persistence_strength, waiting_time, cell_volume] #, track_length]\n",
    "list_stats = [turning_angles, angle_degrees, msds, velo, track_length]\n",
    "\n",
    "reg_output = {\n",
    "    'Turning Angle': [linregress(turning_angles, p) for p in list_params],\n",
    "    'Angle Degree': [linregress(angle_degrees, p) for p in list_params],\n",
    "    'MSD($\\\\tau=1$)': [linregress(msds, p) for p in list_params],\n",
    "    'Velocity': [linregress(velo, p) for p in list_params],\n",
    "}"
   ],
   "id": "82de886b49cb86f9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Plot the correlation\n",
    "fig, ax = plt.subplots(len(reg_output.keys()), 4, sharex='row', sharey='col', figsize=(12, 9), tight_layout=True)\n",
    "\n",
    "for j, (key, list_p) in enumerate(reg_output.items()):\n",
    "    for i in range(len(list_p)):\n",
    "        reg = list_p[i]\n",
    "\n",
    "        ax[j, i].scatter(list_stats[j], list_params[i], alpha=0.7, label='Cell' if i == 0 and j == 0 else None)\n",
    "        if reg.pvalue < 0.01:\n",
    "            ax[j, i].plot(list_stats[j], reg.slope*list_stats[j] + reg.intercept,\n",
    "                     color='red', label=f'Regression Line (significant correlation)' if i == 2 and j == 3 else None)\n",
    "        else:\n",
    "            ax[j, i].plot(list_stats[j], reg.slope*list_stats[j] + reg.intercept,\n",
    "                     color='gray', label=f'Regression Line (non-significant correlation)' if i == 3 and j == 3 else None)\n",
    "\n",
    "        ax[j, i].set_title(f\"Correlation {reg.rvalue:.4f}\\n(p-value: {reg.pvalue:.4f})\")\n",
    "        ax[j, i].set_xlabel(key)\n",
    "        ax[j, i].set_ylabel((log_param_names + ['Track Length'])[i])\n",
    "\n",
    "fig.legend(loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.03))\n",
    "#plt.savefig(f'abc_results_real/real_parameter_vs_stats.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "572dc890b0088ba8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# batch the real data\n",
    "n_cell_in_batch = np.arange(1, len(real_data_full))\n",
    "stats_n_cells = {}\n",
    "np.random.seed(0)\n",
    "\n",
    "real_posterior_samples_partial = []\n",
    "real_posterior_samples_artificial = []\n",
    "for n_cells in tqdm(n_cell_in_batch):\n",
    "    n_rand_index = np.random.choice(len(real_data_full), size=n_cells, replace=False)\n",
    "    partial_data = real_data_full[n_rand_index]\n",
    "\n",
    "    real_posterior_samples = trainer.amortizer.sample(trainer.configurator({'sim_data': partial_data[np.newaxis]}),\n",
    "                                                      n_samples=100)\n",
    "    real_posterior_samples = real_posterior_samples * p_std + p_mean\n",
    "    real_posterior_samples_partial.append(real_posterior_samples)\n",
    "\n",
    "    # fill with nan to get full data set\n",
    "    full_artificial_data = np.ones_like(real_data_full) * np.nan\n",
    "    full_artificial_data[:partial_data.shape[0]] = partial_data\n",
    "\n",
    "    real_posterior_samples = trainer.amortizer.sample(trainer.configurator({'sim_data': full_artificial_data[np.newaxis]}),\n",
    "                                                      n_samples=100)\n",
    "    real_posterior_samples = real_posterior_samples * p_std + p_mean\n",
    "    real_posterior_samples_artificial.append(real_posterior_samples)\n",
    "\n",
    "real_posterior_samples_partial = np.stack(real_posterior_samples_partial)\n",
    "real_posterior_samples_artificial = np.stack(real_posterior_samples_artificial)"
   ],
   "id": "2c4b9f4d313543e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "real_posterior_samples_partial_median = np.median(real_posterior_samples_partial, axis=1)\n",
    "real_posterior_samples_partial_std = median_abs_deviation(real_posterior_samples_partial, axis=1)\n",
    "\n",
    "real_posterior_samples_artificial_median = np.median(real_posterior_samples_artificial, axis=1)\n",
    "real_posterior_samples_artificial_std = median_abs_deviation(real_posterior_samples_artificial, axis=1)"
   ],
   "id": "b0e3b2b8aa6196c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 4, sharex='row', sharey='col', figsize=(12, 4), tight_layout=True)\n",
    "\n",
    "for i, p_name in enumerate(log_param_names):\n",
    "    reg = linregress(n_cell_in_batch, real_posterior_samples_partial_median[:, i])\n",
    "\n",
    "    ax[i].errorbar(n_cell_in_batch,  real_posterior_samples_partial_median[:, i],\n",
    "                  yerr=real_posterior_samples_partial_std[:, i],\n",
    "                  fmt='o',\n",
    "                  alpha=0.5, label='Estimate (median $\\pm$ absolute deviation)' if i == 0 else None)\n",
    "\n",
    "    reg = linregress(n_cell_in_batch, real_posterior_samples_partial_median[:, i])\n",
    "    #if reg.pvalue < 0.01:\n",
    "    ax[i].plot(n_cell_in_batch, reg.slope*n_cell_in_batch + reg.intercept,\n",
    "                     color='black', label=f'Regression Line' if i == 1 else None,\n",
    "                   zorder=4)\n",
    "\n",
    "\n",
    "    ax[i].set_title(f\"Correlation {reg.rvalue:.4f}\\n(p-value: {reg.pvalue:.4f})\")\n",
    "    ax[i].set_xlabel('Number of Cells')\n",
    "    ax[i].set_ylabel(f'Median of {log_param_names[i]}')\n",
    "    ax[i].set_ylim(limits_log[list(limits_log.keys())[i]])\n",
    "\n",
    "fig.legend(loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.1))\n",
    "plt.savefig(f'abc_results_real/real_ncells_vs_parameter_median.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "b09bb5e7f54affc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 4, sharex='row', sharey='col', figsize=(12, 4), tight_layout=True)\n",
    "\n",
    "for i, p_name in enumerate(log_param_names):\n",
    "    reg = linregress(n_cell_in_batch, real_posterior_samples_artificial_median[:, i])\n",
    "\n",
    "    ax[i].errorbar(n_cell_in_batch,  real_posterior_samples_artificial_median[:, i],\n",
    "                  yerr=real_posterior_samples_artificial_std[:, i],\n",
    "                  fmt='o',\n",
    "                  alpha=0.5, label='Estimate (median $\\pm$ absolute deviation)' if i == 0 else None)\n",
    "\n",
    "    reg = linregress(n_cell_in_batch, real_posterior_samples_artificial_median[:, i])\n",
    "    ax[i].plot(n_cell_in_batch, reg.slope*n_cell_in_batch + reg.intercept,\n",
    "                     color='black', label=f'Regression Line' if i == 1 else None,\n",
    "                   zorder=4)\n",
    "\n",
    "    ax[i].set_title(f\"Correlation {reg.rvalue:.4f}\\n(p-value: {reg.pvalue:.4f})\")\n",
    "    ax[i].set_xlabel('Number of Cells')\n",
    "    ax[i].set_ylabel(f'Median of {log_param_names[i]}')\n",
    "    ax[i].set_ylim(limits_log[list(limits_log.keys())[i]])\n",
    "\n",
    "fig.legend(loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.1))\n",
    "#plt.savefig(f'abc_results_real/real_ncells_padded_vs_parameter_median.png', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "6f7e1494de76d9cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "88d703598d0f0e1a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
