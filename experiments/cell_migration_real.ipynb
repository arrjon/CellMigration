{ "cells": [  {   "metadata": {},   "cell_type": "markdown",   "source": "# Inference on Real Data",   "id": "2885213ace1b9d2c"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "import argparse\n",    "import os\n",    "import pickle\n",    "from functools import partial\n",    "from typing import Union\n",    "\n",    "import matplotlib.pyplot as plt\n",    "from joblib import Parallel, delayed\n",    "import numpy as np\n",    "import pyabc\n",    "#from pyabc.sampler import RedisEvalParallelSampler\n",    "import scipy.stats as stats\n",    "import umap\n",    "from sklearn.metrics.pairwise import cosine_similarity\n",    "from fitmulticell import model as morpheus_model\n",    "from fitmulticell.sumstat import SummaryStatistics\n",    "from matplotlib.patches import Patch\n",    "\n",    "from load_bayesflow_model import load_model, EnsembleTrainer\n",    "from plotting_routines import sampling_parameter_cis, plot_posterior_1d, plot_sumstats_distance_stats\n",    "from summary_stats import compute_summary_stats, reduce_to_coordinates, span, euclidean_distance"   ],   "id": "3ce04deed61e1ae4",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# load real data in morpheus format\n",    "from load_data import load_real_data\n",    "\n",    "test_params = None\n",    "results_path = 'abc_results_real'"   ],   "id": "d3952320ae362061",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# get the job array id and number of processors\n",    "n_procs = 10 # int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",    "on_cluster = False\n",    "population_size = 1000\n",    "\n",    "if on_cluster:\n",    "    parser = argparse.ArgumentParser(description='Parse necessary arguments')\n",    "    parser.add_argument('-pt', '--port', type=str, default=\"50004\",\n",    "                        help='Which port should be use?')\n",    "    parser.add_argument('-ip', '--ip', type=str,\n",    "                        help='Dynamically passed - BW: Login Node 3')\n",    "    args = parser.parse_args()"   ],   "id": "abbff289ceca71a5",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "if on_cluster:\n",    "    gp = '/home/jarruda_hpc/CellMigration/synth_data_params_bayesflow'\n",    "else:\n",    "    gp = os.getcwd()\n",    "\n",    "par_map = {\n",    "    'gradient_strength': './CellTypes/CellType/Constant[@symbol=\"gradient_strength\"]',\n",    "    'move.strength': './CellTypes/CellType/Constant[@symbol=\"move.strength\"]',\n",    "    'move.duration.mean': './CellTypes/CellType/Constant[@symbol=\"move.duration.mean\"]',\n",    "    'cell_nodes_real': './Global/Constant[@symbol=\"cell_nodes_real\"]',\n",    "}\n",    "\n",    "dt = 30\n",    "model_path = gp + \"/cell_movement_v24.xml\"  # time step is 30sec, for inference\n",    "\n",    "# defining the summary statistics function\n",    "min_sequence_length = 0\n",    "max_sequence_length = 3600 // dt\n",    "only_longest_traj_per_cell = True  # mainly to keep the data batchable\n",    "cells_in_population = 143\n",    "\n",    "# defining the summary statistics function\n",    "def make_sumstat_dict(data: Union[dict, np.ndarray]) -> dict:\n",    "    if isinstance(data, dict):\n",    "        # get key\n",    "        key = list(data.keys())[0]\n",    "        data = data[key]\n",    "    data = data[0]  # only one full simulation\n",    "    assert data.ndim == 3\n",    "    # compute the summary statistics\n",    "    msd_list, ta_list, v_list, ad_list = compute_summary_stats(data, dt=dt)\n",    "    cleaned_dict = {\n",    "        'msd': np.array(msd_list).flatten(),\n",    "        'ta': np.array(ta_list).flatten(),\n",    "        'vel': np.array(v_list).flatten(),\n",    "        'ad': np.array(ad_list).flatten(),\n",    "    }\n",    "    return cleaned_dict\n",    "\n",    "\n",    "def prepare_sumstats(output_morpheus_model) -> dict:\n",    "    sim_coordinates = reduce_to_coordinates(output_morpheus_model, \n",    "                          minimal_length=min_sequence_length, \n",    "                          maximal_length=max_sequence_length,\n",    "                          only_longest_traj_per_cell=only_longest_traj_per_cell,\n",    "                          )\n",    "    \n",    "    # we now do exactly the same as in the BayesFlow workflow, but here we get only one sample at a time\n",    "    data_transformed = np.ones((1, cells_in_population, max_sequence_length, 3)) * np.nan\n",    "    # each cell is of different length, each with x and y coordinates, make a tensor out of it\n",    "    if len(sim_coordinates) != 0:\n",    "        # some cells were visible in the simulation\n",    "        for c_id, cell_sim in enumerate(sim_coordinates):\n",    "            # pre-pad the data with zeros, but first write zeros as nans to compute the mean and std\n",    "            data_transformed[0, c_id, -len(cell_sim['x']):, 0] = cell_sim['x']\n",    "            data_transformed[0, c_id, -len(cell_sim['y']):, 1] = cell_sim['y']\n",    "            data_transformed[0, c_id, -len(cell_sim['t']):, 2] = cell_sim['t']\n",    "    \n",    "    return {'sim': data_transformed}\n",    "\n",    "\n",    "sumstat = SummaryStatistics(sum_stat_calculator=prepare_sumstats)                    \n",    "\n",    "if on_cluster:\n",    "    # define the model object\n",    "    model = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)\n",    "\n",    "    # note: remember also change tiff path in model.xml!\n",    "else:\n",    "    # define the model object\n",    "    model = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)\n",    "\n",    "# parameter values used to generate the synthetic data\n",    "obs_pars = {\n",    "    'gradient_strength': 100.,  # strength of the gradient of chemotaxis\n",    "    'move.strength': 10.,  # strength of directed motion\n",    "    'move.duration.mean': 0.1,  # mean of exponential distribution (seconds)\n",    "    'cell_nodes_real': 50.,  # area of the cell (\\mu m^2), macrophages have a volume of 4990\\mu m^3 -> radius of 17 if they would are sphere\n",    "}\n",    "\n",    "\n",    "obs_pars_log = {key: np.log10(val) for key, val in obs_pars.items()}\n",    "limits = {'gradient_strength': (1, 10000), #(10 ** 4, 10 ** 8),\n",    "          'move.strength': (1, 100),\n",    "          'move.duration.mean': (1e-4, 30), #(math.log10((10 ** -2) * 30), math.log10((10 ** 4))), # smallest time step in simulation 5\n",    "          'cell_nodes_real': (1, 300)}\n",    "limits_log = {key: (np.log10(val[0]), np.log10(val[1])) for key, val in limits.items()}\n",    "\n",    "\n",    "prior = pyabc.Distribution(**{key: pyabc.RV(\"uniform\", loc=lb, scale=ub-lb)\n",    "                              for key, (lb, ub) in limits_log.items()})\n",    "param_names = ['$m_{\\\\text{dir}}$', '$m_{\\\\text{rand}}$', '$w$', '$a$']\n",    "log_param_names = ['$\\log_{10}(m_{\\\\text{dir}})$', '$\\log_{10}(m_{\\\\text{rand}})$',\n",    "                   '$\\log_{10}(w)$', '$\\log_{10}(a)$']\n",    "print(obs_pars)\n",    "print(limits_log)"   ],   "id": "74390dfb0e802a77",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": "prior_draws = np.array([list(prior.rvs().values()) for _ in range(1000)])",   "id": "ab3a9bf0fcadc7d2",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "def obj_func_wass_helper(sim: dict, obs: dict, key: str) -> float:\n",    "    x, y = np.array(sim[key]), np.array(obs[key])\n",    "    if x.size == 0 or y.size == 0:\n",    "        return np.inf\n",    "    return stats.wasserstein_distance(x, y)\n",    "\n",    "distances = {\n",    "    'msd': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='msd')),\n",    "    'ta': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='ta')),\n",    "    'vel': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='vel')),\n",    "    'ad': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='ad')),\n",    "}\n",    "\n",    "# adaptive distance\n",    "log_file_weights = f\"{results_path}/adaptive_distance_log_real.txt\"\n",    "\n",    "adaptive_wasserstein_distance = pyabc.distance.AdaptiveAggregatedDistance(\n",    "    distances=list(distances.values()),\n",    "    scale_function=span,\n",    "    adaptive=False,  # only pre-calibration\n",    "    log_file=log_file_weights\n",    ")"   ],   "id": "a9f793fd44037b02",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "_, real_data_full = load_real_data(data_id=1,\n",    "                                   max_sequence_length=max_sequence_length,\n",    "                                   cells_in_population=cells_in_population)\n",    "real_sim = real_data_full[np.newaxis]\n",    "\n",    "fig, ax = plt.subplots(nrows=1, ncols=1, layout='constrained', figsize=(4, 2))\n",    "\n",    "plt.plot(real_data_full[0, :, 0], real_data_full[0, :, 1], color='blue', alpha=0.7)\n",    "for cell_id in range(1, real_data_full.shape[0]):\n",    "    plt.plot(real_data_full[cell_id, :, 0], real_data_full[cell_id, :, 1],\n",    "             linewidth=0.5, color='blue', alpha=0.7)\n",    "    plt.scatter(real_data_full[cell_id, :, 0], real_data_full[cell_id, :, 1],\n",    "                s=1, color='blue', alpha=0.7)\n",    "\n",    "lims = ax.get_xlim(), ax.get_ylim()\n",    "img_path = 'Cell_migration_grid_v3_final2_invers.tiff'\n",    "img = plt.imread(img_path)\n",    "ax.imshow(img,\n",    "          origin='lower',\n",    "          #extent=[x_min, x_max, y_min, y_max],\n",    "          aspect='auto',          # preserve pixel squares\n",    "          zorder=0,\n",    "          interpolation=['bicubic', 'bessel', 'hermite', 'nearest', 'spline16', 'mitchell', 'bilinear', 'sinc', 'none', 'gaussian', 'blackman', 'spline36', 'hanning', 'kaiser', 'lanczos', 'antialiased', 'hamming', 'catrom', 'quadric'][14]) # keep grid sharp\n",    "ax.set_xlim(lims[0])\n",    "ax.set_ylim(lims[1])\n",    "ax.set_aspect('equal')\n",    "ax.tick_params(axis=\"x\", labelsize=10)\n",    "ax.tick_params(axis=\"y\", labelsize=10)\n",    "\n",    "plt.ylabel('$y$ Position (in $\\mu m$)', fontsize=12)\n",    "plt.xlabel('$x$ Position (in $\\mu m$)', fontsize=12)\n",    "#plt.savefig(f'{results_path}/real_data.pdf', bbox_inches='tight')\n",    "plt.show()"   ],   "id": "70ad546e370c2f10",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "markdown",   "source": "# Real Data Analysis",   "id": "9eae72208b78f06"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "#redis_sampler = RedisEvalParallelSampler(host=args.ip, port=args.port,\n",    "#                                         adapt_look_ahead_proposal=False,\n",    "#                                         look_ahead=False)\n",    "\n",    "abc = pyabc.ABCSMC(model, prior,\n",    "                   distance_function=adaptive_wasserstein_distance,\n",    "                   summary_statistics=make_sumstat_dict,\n",    "                   population_size=population_size,\n",    "                   sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",    "                   #sampler=redis_sampler\n",    "                   )\n",    "\n",    "db_path = os.path.join(gp, f\"{results_path}/real_test_wasserstein_sumstats.db\")\n",    "if not os.path.exists(db_path):\n",    "    history_abc_real = abc.new(\"sqlite:///\" + db_path, make_sumstat_dict(real_sim))\n",    "\n",    "    # start the abc fitting\n",    "    abc.run(min_acceptance_rate=1e-2, max_nr_populations=15)\n",    "    print('Done!')\n",    "else:\n",    "    history_abc_real = abc.load(\"sqlite:///\" + db_path)\n",    "    if len(history_abc_real.all_runs()) > 1:\n",    "        # first run failed\n",    "        history_abc_real = abc.load(\"sqlite:///\" + db_path, abc_id=len(history_abc_real.all_runs()))\n",    "adaptive_weights_real = list(pyabc.storage.load_dict_from_json(log_file_weights).values())[-1]"   ],   "id": "11038c6fa5bddaa2",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "if os.path.exists(os.path.join(gp, 'validation_data.pickle')):\n",    "    with open(os.path.join(gp, 'validation_data.pickle'), 'rb') as f:\n",    "        valid_data = pickle.load(f)\n",    "else:\n",    "    raise FileNotFoundError('Validation data not found')\n",    "\n",    "x_mean = np.nanmean(valid_data['sim_data'], axis=(0, 1, 2))\n",    "x_std = np.nanstd(valid_data['sim_data'], axis=(0, 1, 2))\n",    "p_mean = np.mean(valid_data['prior_draws'], axis=0)\n",    "p_std = np.std(valid_data['prior_draws'], axis=0)\n",    "print('Mean and std of data:', x_mean, x_std)\n",    "print('Mean and std of parameters:', p_mean, p_std)"   ],   "id": "5a3d173ec5e0ba1d",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# use trained neural net as summary statistics\n",    "def make_sumstat_dict_nn(data: Union[dict, np.ndarray], use_npe_summaries: bool = True, return_reduced: bool = False\n",    "                         ) -> dict:\n",    "    if use_npe_summaries:\n",    "        model_id = 0\n",    "    else:\n",    "        model_id = 10\n",    "    if isinstance(data, dict):\n",    "        # get key\n",    "        key = list(data.keys())[0]\n",    "        data = data[key]\n",    "\n",    "    trainer = load_model(\n",    "        model_id=model_id,\n",    "        x_mean=x_mean,\n",    "        x_std=x_std,\n",    "        p_mean=p_mean,\n",    "        p_std=p_std,\n",    "    )\n",    "\n",    "    # configures the input for the network\n",    "    config_input = trainer.configurator({\"sim_data\": data})\n",    "    # get the summary statistics\n",    "    if isinstance(trainer, EnsembleTrainer):\n",    "        out_dict = {\n",    "            'summary_net': trainer.amortizer.summary_net(config_input).flatten()\n",    "        }\n",    "    else:\n",    "        out_dict = {\n",    "            'summary_net': trainer.amortizer.summary_net(config_input['summary_conditions']).numpy().flatten()\n",    "        }\n",    "    if return_reduced:\n",    "        # coeff matrix from regression model\n",    "        coeff_matrix = np.array([[-0.,           0.16171273, -0.,         -0.05603923],\n",    "                         [-0.,         -0.,          0.31892324, -0.        ],\n",    "                         [-0.2107947,  -0.10119028, -0.,          0.        ],\n",    "                         [-0.44803216, -0.,         -0.0584819,   0.31803382],\n",    "                         [-0.07227701,  0.,          0.,         -0.52875216],\n",    "                         [-0.,          0.422123,   -0.02283603,  0.        ],\n",    "                         [ 0.30511962,  0.,          0.,          0.        ],\n",    "                         [-0.38812312,  0.0425146,  -0.00355393,  0.        ]])\n",    "\n",    "        out_dict['summary_pred'] = (coeff_matrix.T @ out_dict['summary_net']) * p_std + p_mean\n",    "    if model_id == 10:\n",    "        # renormalize the parameters\n",    "        out_dict['summary_net'] = out_dict['summary_net'] * p_std + p_mean\n",    "\n",    "    del trainer\n",    "    return out_dict\n",    "\n",    "\n",    "if on_cluster:\n",    "    # define the model object\n",    "    model_nn = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)\n",    "\n",    "    # note: remember also change tiff path in model.xml!\n",    "else:\n",    "    # define the model object\n",    "    model_nn = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)"   ],   "id": "ac8182d1023c7d0c",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "#redis_sampler = RedisEvalParallelSampler(host=args.ip, port=args.port,\n",    "#                                         adapt_look_ahead_proposal=False,\n",    "#                                         look_ahead=False)\n",    "\n",    "# abc with summary net trained with NPE\n",    "abc_npe = pyabc.ABCSMC(model_nn, prior, # here we use now the Euclidean distance, Wasserstein distance is not possible\n",    "                      population_size=population_size,\n",    "                      summary_statistics=partial(make_sumstat_dict_nn, use_npe_summaries=True),\n",    "                      sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",    "                      #sampler=redis_sampler\n",    "                      )\n",    "\n",    "db_path = os.path.join(gp, f\"{results_path}/real_test_nn_sumstats.db\")\n",    "if not os.path.exists(db_path):\n",    "    history_npe_real = abc_npe.new(\"sqlite:///\" + db_path,\n",    "                              make_sumstat_dict_nn(real_sim, use_npe_summaries=True))\n",    "    # start the abc fitting\n",    "    abc_npe.run(min_acceptance_rate=1e-2, max_nr_populations=15)\n",    "    print('Done!')\n",    "else:\n",    "    history_npe_real = abc_npe.load(\"sqlite:///\" + db_path)\n",    "    if len(history_npe_real.all_runs()) > 1:\n",    "        # first run failed\n",    "        history_npe_real = abc_npe.load(\"sqlite:///\" + db_path, abc_id=len(history_npe_real.all_runs()))"   ],   "id": "4e98e2bd7b87b144",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "for hist, name in zip([history_abc_real, history_npe_real], ['abc', 'abc_npe']):\n",    "    if hist is None:\n",    "        continue\n",    "    diff_time = hist.get_all_populations().population_end_time - hist.get_abc().start_time\n",    "    total_time = [diff.total_seconds() for diff in diff_time][-1] / 60 / 60\n",    "    print(name,\n",    "          'Generations:', len(hist.get_all_populations()['t']),  # including precalibration\n",    "          'Samples:', hist.get_all_populations()['samples'].sum(),\n",    "          'Time (h):', np.round(total_time, 2))\n",    "    fig, ax = plt.subplots(1, len(param_names),  layout='constrained', figsize=(10, 2))\n",    "    for i, param in enumerate(limits.keys()):\n",    "        for t in range(hist.max_t + 1):\n",    "            df, w = hist.get_distribution(m=0, t=t)\n",    "            pyabc.visualization.plot_kde_1d(\n",    "                df,\n",    "                w,\n",    "                xmin=limits_log[param][0],\n",    "                xmax=limits_log[param][1],\n",    "                x=param,\n",    "                xname=log_param_names[i],\n",    "                ax=ax[i],\n",    "                label=f\"PDF t={t}\",\n",    "            )\n",    "        ax[i].set_xlim((limits_log[param][0]-0.2, limits_log[param][1]+0.2))\n",    "    fig.set_constrained_layout(True)\n",    "    plt.savefig(os.path.join(gp, f'{results_path}/real_population_kdes_{name}.pdf'))\n",    "    plt.show()\n",    "\n",    "    fig, arr_ax = plt.subplots(1, 5, figsize=(10, 2.5), layout='constrained')\n",    "    arr_ax = arr_ax.flatten()\n",    "    pyabc.visualization.plot_sample_numbers(hist, ax=arr_ax[0])\n",    "    arr_ax[0].get_legend().remove()\n",    "    pyabc.visualization.plot_walltime(hist, ax=arr_ax[1], unit='h')\n",    "    arr_ax[1].get_legend().remove()\n",    "    pyabc.visualization.plot_epsilons(hist, ax=arr_ax[2])\n",    "    pyabc.visualization.plot_effective_sample_sizes(hist, ax=arr_ax[3])\n",    "    pyabc.visualization.plot_acceptance_rates_trajectory(hist, ax=arr_ax[4])\n",    "    fig.set_constrained_layout(True)\n",    "    plt.savefig(os.path.join(gp, f'{results_path}/real_diagnostics_{name}.pdf'))\n",    "    plt.show()"   ],   "id": "78393a7229bcda78",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# get posterior samples\n",    "posterior_samples = {}\n",    "for hist, name in zip([history_abc_real, history_npe_real], ['abc', 'abc_npe']):\n",    "    if hist is None:\n",    "        continue\n",    "    abc_df, abc_w = hist.get_distribution()\n",    "    posterior_samples[name] = pyabc.resample(abc_df[limits.keys()].values, abc_w, n=1000)\n",    "\n",    "# add bayesflow posterior samples\n",    "posterior_samples['npe'] = np.load(f'abc_results_real/posterior_samples_npe.npy')\n",    "posterior_samples['npe_ensemble'] = np.load(f'abc_results_real/posterior_samples_npe_ensemble.npy')"   ],   "id": "30b4ffca42deedfd",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "labels_colors = {\n",    "    'abc': ('ABC hand-crafted summaries', '#9AB8D7', 'ABC'),\n",    "    #'abc_mean': ('ABC posterior mean summaries', '#C4B7D4', 'ABC-PM'),\n",    "    'abc_npe': ('ABC inference-tailored summaries', '#EEBC88', 'ABC-NPE'),\n",    "    'npe': ('NPE jointly learned summaries', '#A7CE97', 'NPE'),\n",    "    'npe_ensemble': ('NPE-Ensemble', '#8EB699', 'NPE-E')\n",    "}\n",    "\n",    "colors = [labels_colors[name][1] for name in posterior_samples.keys()]\n",    "labels = [labels_colors[name][0] for name in posterior_samples.keys()]"   ],   "id": "16c0daa7e8d9f7e7",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "fig = plot_posterior_1d(\n",    "    posterior_samples=posterior_samples,\n",    "    prior_draws=prior_draws,\n",    "    log_param_names=log_param_names,\n",    "    test_sim=real_sim,\n",    "    test_params=None,\n",    "    labels_colors=labels_colors,\n",    "    #make_sumstat_dict_nn=make_sumstat_dict_nn,\n",    "    save_path=os.path.join(gp, f'{results_path}/real_posterior_all_rows.pdf')\n",    ")"   ],   "id": "d4caf077c66ab35f",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "ordering = np.concatenate([[i,i+4, i+8, i+12] for i in range(4)])\n",    "all_params = np.concatenate((posterior_samples['abc'],\n",    "                             posterior_samples['abc_npe'],\n",    "                             posterior_samples['npe'],\n",    "                             posterior_samples['npe_ensemble']), axis=-1)\n",    "log_param_names_plot = np.array(\n",    "    [f'{n} $\\qquad$ ABC hand-crafted' for n in log_param_names] +\n",    "    [f'ABC inference tailored' for n in log_param_names] +\n",    "    [f'NPE' for n in log_param_names] +\n",    "    [f'NPE Ensemble' for n in log_param_names]\n",    ")[ordering]\n",    "param_names_plot = np.array(\n",    "    [f'{n} $\\qquad$ ABC hand-crafted' for n in param_names] +\n",    "    [f'ABC inference tailored' for n in param_names] +\n",    "    [f'NPE' for n in param_names] +\n",    "    [f'NPE Ensemble' for n in param_names]\n",    ")[ordering]\n",    "color_list = colors*len(param_names)\n",    "\n",    "ax = sampling_parameter_cis(\n",    "    all_params[:, ordering],\n",    "    true_param=np.concatenate((test_params, test_params, test_params, test_params))[ordering] if test_params is not None else None,\n",    "    prior_bounds=limits_log.values() if test_params is not None else None,\n",    "    param_names=log_param_names_plot,\n",    "    alpha=[95, 90 , 80],\n",    "    color_list=color_list,\n",    "    show_median=False if test_params is not None else True,\n",    "    size=(7, 4),\n",    "    legend_bbox_to_anchor=(0.45,1) if test_params is not None else (0.31,1)\n",    ")\n",    "plt.savefig(os.path.join(gp, f'{results_path}/real_posterior_credible_intervals_log.pdf'))\n",    "plt.show()\n",    "\n",    "all_params = np.power(10, all_params)\n",    "ax = sampling_parameter_cis(\n",    "    all_params[:, ordering],\n",    "    true_param=np.power(10, np.concatenate((test_params, test_params, test_params, test_params))[ordering]) if test_params is not None else None,\n",    "    param_names=param_names_plot,\n",    "    alpha=[95, 90 , 80],\n",    "    color_list=color_list,\n",    "    show_median=False if test_params is not None else True,\n",    "    size=(7, 4),\n",    "    legend_bbox_to_anchor=(0.99,0.35)\n",    ")\n",    "plt.savefig(os.path.join(gp, f'{results_path}/real_posterior_credible_intervals.pdf'))\n",    "plt.show()"   ],   "id": "e441cbe9264bdc03",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "file_name1 = os.path.join(gp, f'{results_path}/real_posterior_simulations.pickle')\n",    "file_name2 = os.path.join(gp, f'{results_path}/real_posterior_simulations_sumstats.pickle')\n",    "n_sim = 100\n",    "if os.path.exists(file_name1):\n",    "    with open(file_name1, 'rb') as f:\n",    "        posterior_simulations = pickle.load(f)\n",    "    with open(file_name2, 'rb') as f:\n",    "        simulation_sumstats = pickle.load(f)\n",    "else:\n",    "    posterior_simulations = {}\n",    "    for name, ps in posterior_samples.items():\n",    "        print('Simulating', name)\n",    "        @delayed\n",    "        def wrapper_fun(sample_i):\n",    "            _sim_dict = {key: p for key, p in zip(obs_pars.keys(), ps[sample_i])}\n",    "            _posterior_sim = model(_sim_dict)\n",    "            return _posterior_sim['sim']\n",    "\n",    "        sim_list = Parallel(n_jobs=n_procs, verbose=1)(wrapper_fun(i) for i in range(n_sim))\n",    "        posterior_simulations[name] = np.concatenate(sim_list)\n",    "\n",    "    with open(file_name1, 'wb') as f:\n",    "        pickle.dump(posterior_simulations, f)\n",    "\n",    "    simulation_sumstats = {}\n",    "    for name, ps in posterior_simulations.items():\n",    "        print('Processing', name)\n",    "        simulation_sumstats[name] = [make_sumstat_dict(p_sim[np.newaxis]) for p_sim in ps]\n",    "        for i in range(len(simulation_sumstats[name])):\n",    "            simulation_sumstats[name][i]['nn'] = make_sumstat_dict_nn(ps[i][np.newaxis], use_npe_summaries=True)['summary_net']\n",    "\n",    "    with open(file_name2, 'wb') as f:\n",    "        pickle.dump(simulation_sumstats, f)"   ],   "id": "2d5bd8d8b13a0cd1",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True, sharey=True,\n",    "                       tight_layout=True, figsize=(4, 3))\n",    "\n",    "plt.plot(real_data_full[0, :, 0], real_data_full[0, :, 1], color='blue', alpha=0.7)\n",    "for cell_id in range(1, real_data_full.shape[0]):\n",    "    plt.plot(real_data_full[cell_id, :, 0], real_data_full[cell_id, :, 1],\n",    "             linewidth=0.5, color='blue', alpha=0.7)\n",    "    plt.scatter(real_data_full[cell_id, :, 0], real_data_full[cell_id, :, 1],\n",    "                s=1, color='blue', alpha=0.7)\n",    "\n",    "lims = ax.get_xlim(), ax.get_ylim()\n",    "img_path = 'Cell_migration_grid_v3_final2_invers.tiff'\n",    "img = plt.imread(img_path)\n",    "ax.imshow(img,\n",    "          origin='lower',\n",    "          #extent=[x_min, x_max, y_min, y_max],\n",    "          aspect='auto',          # preserve pixel squares\n",    "          zorder=0,\n",    "          interpolation=['bicubic', 'bessel', 'hermite', 'nearest', 'spline16', 'mitchell', 'bilinear', 'sinc', 'none', 'gaussian', 'blackman', 'spline36', 'hanning', 'kaiser', 'lanczos', 'antialiased', 'hamming', 'catrom', 'quadric'][14]) # keep grid sharp\n",    "ax.set_xlim(0, 1120)\n",    "ax.set_ylim(0, 1700)\n",    "ax.set_aspect('equal')\n",    "\n",    "plt.ylabel('$y$ Position (in $\\mu m$)')\n",    "plt.xlabel('$x$ Position (in $\\mu m$)')\n",    "plt.show()\n",    "\n",    "for name, ps in posterior_simulations.items():\n",    "    print(name)\n",    "    ps = ps[0]\n",    "    fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True, sharey=True,\n",    "                           tight_layout=True, figsize=(4, 3))\n",    "\n",    "    plt.plot(ps[0, :, 0], ps[0, :, 1], color='blue', alpha=0.7)\n",    "    for cell_id in range(1, ps.shape[0]):\n",    "        plt.plot(ps[cell_id, :, 0], ps[cell_id, :, 1],\n",    "                 linewidth=0.5, color='blue', alpha=0.7)\n",    "        plt.scatter(ps[cell_id, :, 0], ps[cell_id, :, 1],\n",    "                    s=1, color='blue', alpha=0.7)\n",    "\n",    "    lims = ax.get_xlim(), ax.get_ylim()\n",    "    img_path = 'Cell_migration_grid_v3_final2_invers.tiff'\n",    "    img = plt.imread(img_path)\n",    "    ax.imshow(img,\n",    "              origin='lower',\n",    "              #extent=[x_min, x_max, y_min, y_max],\n",    "              aspect='auto',          # preserve pixel squares\n",    "              zorder=0,\n",    "              interpolation=['bicubic', 'bessel', 'hermite', 'nearest', 'spline16', 'mitchell', 'bilinear', 'sinc', 'none', 'gaussian', 'blackman', 'spline36', 'hanning', 'kaiser', 'lanczos', 'antialiased', 'hamming', 'catrom', 'quadric'][14]) # keep grid sharp\n",    "    ax.set_xlim(0, 1120)\n",    "    ax.set_ylim(0, 1700)\n",    "    ax.set_aspect('equal')\n",    "\n",    "    plt.ylabel('$y$ Position (in $\\mu m$)')\n",    "    plt.xlabel('$x$ Position (in $\\mu m$)')\n",    "    plt.show()"   ],   "id": "bb2d5c8166e72ae6",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "def obj_func_comparison(sim: dict, obs: dict, return_marginal: bool = False, weights: Union[dict, list] = None) -> Union[float, np.ndarray]:\n",    "    total = np.zeros(len(sim.keys()))\n",    "    for k_i, key in enumerate(sim):\n",    "        if key == 'nn':\n",    "            # for the neural network summary statistics we use the Euclidean distance\n",    "            total[k_i] = euclidean_distance(sim['nn'], obs['nn'])\n",    "            continue # no weights applied\n",    "        elif key == 'umap':\n",    "            # distance already computed\n",    "            total[k_i] = obs['umap']\n",    "            continue # no weights applied\n",    "        else:\n",    "            total[k_i] = distances[key](sim, obs)\n",    "        if weights is not None:\n",    "            if isinstance(weights, dict):\n",    "                total[k_i] = total[k_i] * weights[key]\n",    "            elif isinstance(weights, list):\n",    "                total[k_i] = total[k_i] * weights[k_i]\n",    "            else:\n",    "                raise ValueError('Weights must be a list or a dictionary')\n",    "    if return_marginal:\n",    "        return total\n",    "    return total.sum()"   ],   "id": "2c7d9ed9e25e0f96",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "real_sim_dict = make_sumstat_dict(real_sim)\n",    "real_sim_dict['nn'] = make_sumstat_dict_nn(real_sim, use_npe_summaries=True)['summary_net']\n",    "real_sim_dict['umap'] = None"   ],   "id": "2d3b35b6007eb1fe",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# compute the UMAP embedding\n",    "all_trajec = np.concatenate((\n",    "    real_sim[0],\n",    "    posterior_simulations['abc'][0],\n",    "    posterior_simulations['abc_npe'][0],\n",    "    posterior_simulations['npe'][0]),\n",    "    axis=0)\n",    "all_trajec = np.concatenate([all_trajec[..., 0], all_trajec[..., 1], all_trajec[..., 2]], axis=1)\n",    "all_trajec[np.isnan(all_trajec)] = -1\n",    "\n",    "color_code = np.concatenate([\n",    "    np.zeros(real_sim[0].shape[0]),\n",    "    np.ones(posterior_simulations['abc'][0].shape[0]),\n",    "    np.ones(posterior_simulations['abc_npe'][0].shape[0])*2,\n",    "    np.ones(posterior_simulations['npe'][0].shape[0])*3\n",    "])\n",    "\n",    "# make a umap plot\n",    "colors_umap = ['#1f77b4', labels_colors['abc'][1], labels_colors['abc_npe'][1],\n",    "               labels_colors['npe'][1]]\n",    "reducer = umap.UMAP(random_state=0)\n",    "embedding = reducer.fit_transform(all_trajec)\n",    "\n",    "plt.figure(tight_layout=True, figsize=(5, 5))\n",    "plt.scatter(\n",    "    embedding[:, 0],\n",    "    embedding[:, 1],\n",    "    c=[colors_umap[int(i)] for i in color_code],\n",    "    alpha=0.5,\n",    ")\n",    "plt.gca().set_aspect('equal', 'datalim')\n",    "patches = [Patch(color=colors_umap[i], label=f'{[\"Real\", \"ABC\", \"ABC-NPE\", \"NPE\"][i]}')\n",    "           for i in range(len(colors_umap))]\n",    "plt.legend(handles=patches)\n",    "plt.show()"   ],   "id": "c3b561d39cd3e075",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# compute the UMAP embedding and similarity matrices\n",    "\n",    "# 1) Stack & flatten as you already do\n",    "all_trajec = np.concatenate((\n",    "    real_sim,\n",    "    posterior_simulations['abc'],\n",    "    posterior_simulations['abc_npe'],\n",    "    posterior_simulations['npe'],\n",    "    posterior_simulations['npe_ensemble']\n",    "), axis=0).reshape((4*n_sim+1)*143, 120, 3)\n",    "# flatten the 3 coords into a single feature-vector per trajectory\n",    "all_trajec = np.concatenate([\n",    "    all_trajec[..., 0], all_trajec[..., 1], all_trajec[..., 2]\n",    "], axis=1)\n",    "all_trajec[np.isnan(all_trajec)] = -1\n",    "\n",    "# how many samples per group?\n",    "n_real = real_sim[0].shape[0]\n",    "n_abc  = posterior_simulations['abc'].shape[1] * posterior_simulations['abc'].shape[0]\n",    "n_abc_npe = posterior_simulations['abc_npe'].shape[1] * posterior_simulations['abc_npe'].shape[0]\n",    "n_npe  = posterior_simulations['npe'].shape[1] * posterior_simulations['npe'].shape[0]\n",    "n_npe_ensemble = posterior_simulations['npe_ensemble'].shape[1] * posterior_simulations['npe_ensemble'].shape[0]\n",    "\n",    "# 2) Compute UMAP embedding once\n",    "reducer   = umap.UMAP(random_state=0, n_components=10)\n",    "embedding = reducer.fit_transform(all_trajec)  # shape=(N,n_components)\n",    "\n",    "# 3) Compute full similarity matrices\n",    "sim_emb  = cosine_similarity(embedding)       # (N × N) in UMAP space\n",    "\n",    "# 4) For each method, pull similarities to the real set\n",    "#    sim_to_real_method = sim_norm[0:n_real, idx_start:idx_end]\n",    "ranges = {\n",    "    'abc':       slice(n_real,  n_real + n_abc),\n",    "    'abc_npe':   slice(n_real + n_abc, n_real + n_abc + n_abc_npe),\n",    "    'npe':       slice(n_real + n_abc + n_abc_npe, n_real + n_abc + n_abc_npe + n_npe),\n",    "    'npe_ensemble': slice(n_real + n_abc + n_abc_npe + n_npe, None),\n",    "}\n",    "\n",    "data = {}\n",    "for name, sl in ranges.items():\n",    "    # all sim-vs-all real similarities\n",    "    data[name] = sim_emb[:n_real, sl]\n",    "\n",    "# append to summary stats\n",    "for method, sim in simulation_sumstats.items():\n",    "    for i, ss in enumerate(sim):\n",    "        simulation_sumstats[method][i]['umap'] = np.median(data[method][:, i*143:(i+1)*143])"   ],   "id": "a952b03893ec486e",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "plot_sumstats_distance_stats(obj_func_comparison,\n",    "                             real_sim_dict,\n",    "                             [ps for ps in simulation_sumstats.values()],\n",    "                             labels=labels,\n",    "                             title='Wasserstein Distance',\n",    "                             colors=colors,\n",    "                             ylog_scale=True,\n",    "                             path=os.path.join(gp, f'{results_path}/real_sumstats_wasserstein.pdf')\n",    "                             )\n",    "\n",    "print(*real_sim_dict.keys())\n",    "print(adaptive_weights_real)\n",    "plot_sumstats_distance_stats(partial(obj_func_comparison, weights=adaptive_weights_real),\n",    "                             real_sim_dict,\n",    "                             [ps for ps in simulation_sumstats.values()],\n",    "                             labels=labels,\n",    "                             colors=colors,\n",    "                             #path=os.path.join(gp, f'{results_path}/real_sumstats_wasserstein.pdf')\n",    "                             )"   ],   "id": "f271374ce5e04fea",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "#np.save('abc_results_real/posterior_samples.npy', posterior_samples['npe_ensemble'])\n",    "print('Median:', np.power(10, np.median(posterior_samples['npe_ensemble'], axis=0)))"   ],   "id": "d2273c06cf4f48f7",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": "",   "id": "10cbce0bec376e00",   "outputs": [],   "execution_count": null  } ], "metadata": {  "kernelspec": {   "display_name": "Python 3",   "language": "python",   "name": "python3"  },  "language_info": {   "codemirror_mode": {    "name": "ipython",    "version": 2   },   "file_extension": ".py",   "mimetype": "text/x-python",   "name": "python",   "nbconvert_exporter": "python",   "pygments_lexer": "ipython2",   "version": "2.7.6"  } }, "nbformat": 4, "nbformat_minor": 5}