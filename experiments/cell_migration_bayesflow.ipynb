{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference with Neural Posterior Estimation (NPE)",
   "id": "3d684170c7ac2952"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pyabc\n",
    "from fitmulticell import model as morpheus_model\n",
    "from fitmulticell.sumstat import SummaryStatistics\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import umap\n",
    "from sklearn.linear_model import Lasso\n",
    "import seaborn as sns\n",
    "\n",
    "from load_bayesflow_model import load_model, custom_loader, EnsembleTrainer\n",
    "from plotting_routines import plot_posterior_2d\n",
    "from summary_stats import reduce_to_coordinates, compute_rmse\n",
    "\n",
    "import tensorflow as tf\n",
    "import bayesflow as bf\n",
    "from bayesflow.simulation import GenerativeModel, Prior, Simulator\n",
    "\n",
    "# get the job array id and number of processors\n",
    "job_array_id = 0 #int(os.environ.get('SLURM_ARRAY_TASK_ID', 0))\n",
    "n_procs = 10 #int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",
    "print(job_array_id)\n",
    "on_cluster = False"
   ],
   "id": "10b55f49c4d5c93c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if on_cluster:\n",
    "    gp = '/home/jarruda_hpc/CellMigration/synth_data_params_bayesflow'\n",
    "else:\n",
    "    gp = os.getcwd()\n",
    "\n",
    "# defining the mapping of parameter inside the model xml file. the dictionary name is for \n",
    "# parameter name, and the value are the mapping values, to get the map value for parameter \n",
    "# check here: https://fitmulticell.readthedocs.io/en/latest/example/minimal.html#Inference-problem-definition\n",
    "\n",
    "par_map = {\n",
    "    'gradient_strength': './CellTypes/CellType/Constant[@symbol=\"gradient_strength\"]',\n",
    "    'move.strength': './CellTypes/CellType/Constant[@symbol=\"move.strength\"]',\n",
    "    'move.duration.mean': './CellTypes/CellType/Constant[@symbol=\"move.duration.mean\"]',\n",
    "    'cell_nodes_real': './Global/Constant[@symbol=\"cell_nodes_real\"]',\n",
    "}\n",
    "\n",
    "model_path = gp + \"/cell_movement_v24.xml\"  # time step is 30sec, move.dir completely normalized, init move.dir rand in all directions\n",
    "# defining the summary statistics function\n",
    "max_sequence_length = 120\n",
    "min_sequence_length = 0\n",
    "only_longest_traj_per_cell = True  # mainly to keep the data batchable\n",
    "sumstat = SummaryStatistics(sum_stat_calculator=partial(reduce_to_coordinates,\n",
    "                                                        minimal_length=min_sequence_length,\n",
    "                                                        maximal_length=max_sequence_length,\n",
    "                                                        only_longest_traj_per_cell=only_longest_traj_per_cell))                    \n",
    "\n",
    "if on_cluster:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "    # note: remember also change tiff path in model.xml!\n",
    "else:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "\n",
    "# parameter values used to generate the synthetic data\n",
    "obs_pars = {\n",
    "    'gradient_strength': 100.,  # strength of the gradient of chemotaxis (energy potential)\n",
    "    'move.strength': 10.,  # strength of directed motion (energy potential)\n",
    "    'move.duration.mean': 0.1,  # mean of exponential distribution (seconds)\n",
    "    'cell_nodes_real': 50.,  # area of the cell  (\\mu m^2)\n",
    "}\n",
    "\n",
    "obs_pars_log = {key: np.log10(val) for key, val in obs_pars.items()}\n",
    "limits = {'gradient_strength': (1, 10000), #(10 ** 4, 10 ** 8),\n",
    "          'move.strength': (1, 100),\n",
    "          'move.duration.mean': (1e-4, 30), #(math.log10((10 ** -2) * 30), math.log10((10 ** 4))), # smallest time step in simulation 5\n",
    "          'cell_nodes_real': (1, 300)}\n",
    "limits_log = {key: (np.log10(val[0]), np.log10(val[1])) for key, val in limits.items()}\n",
    "\n",
    "\n",
    "prior = pyabc.Distribution(**{key: pyabc.RV(\"uniform\", loc=lb, scale=ub-lb)\n",
    "                              for key, (lb, ub) in limits_log.items()})\n",
    "param_names = ['$m_{\\\\text{dir}}$', '$m_{\\\\text{rand}}$', '$w$', '$a$']\n",
    "log_param_names = ['$\\log_{10}(m_{\\\\text{dir}})$', '$\\log_{10}(m_{\\\\text{rand}})$', '$\\log_{10}(w)$', '$\\log_{10}(a)$']\n",
    "print(obs_pars)"
   ],
   "id": "74390dfb0e802a77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prior_fun(batch_size: int) -> np.ndarray:\n",
    "    samples = []\n",
    "    for _ in range(batch_size):\n",
    "        samples.append(list(prior.rvs().values()))\n",
    "    return np.array(samples)\n",
    "\n",
    "\n",
    "def generate_population_data(param_batch: np.ndarray, cells_in_population: int, max_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate population data\n",
    "    :param param_batch:  batch of parameters\n",
    "    :param cells_in_population:  number of cells in a population (50)\n",
    "    :param max_length:  maximum length of the sequence\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_batch = []\n",
    "    for params in param_batch:\n",
    "        params_dict = {key: p for key, p in zip(obs_pars.keys(), params)}\n",
    "        sim = model.sample(params_dict)\n",
    "        data_batch.append(sim)  # generates a cell population in one experiment\n",
    "\n",
    "    data_batch_transformed = np.ones((param_batch.shape[0], cells_in_population, max_length, 3)) * np.nan\n",
    "    # each cell is of different length, each with x and y coordinates, make a tensor out of it\n",
    "    n_cells_not_visible = 0\n",
    "    for p_id, population_sim in enumerate(data_batch):\n",
    "        if len(population_sim) == 0:\n",
    "            # no cells were visible in the simulation\n",
    "            n_cells_not_visible += 1\n",
    "            continue\n",
    "        for c_id, cell_sim in enumerate(population_sim):\n",
    "            # pre-pad the data with zeros, but first write zeros as nans to compute the mean and std\n",
    "            data_batch_transformed[p_id, c_id, -len(cell_sim['x']):, 0] = cell_sim['x']\n",
    "            data_batch_transformed[p_id, c_id, -len(cell_sim['y']):, 1] = cell_sim['y']\n",
    "            data_batch_transformed[p_id, c_id, -len(cell_sim['t']):, 2] = cell_sim['t']\n",
    "\n",
    "    if n_cells_not_visible > 0:\n",
    "        print(f'Simulation with no cells visible: {n_cells_not_visible}/{len(data_batch)}')\n",
    "    return data_batch_transformed"
   ],
   "id": "c606f75e3a4c0257",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "presimulate = False\n",
    "presimulation_path = 'presimulations'\n",
    "n_val_data = 100\n",
    "cells_in_population = 143\n",
    "n_params = len(obs_pars)\n",
    "batch_size = 32\n",
    "iterations_per_epoch = 100\n",
    "# 1000 batches to be generated, 10 epochs until the batch is used again\n",
    "epochs = 500\n",
    "\n",
    "# check if gpu is available\n",
    "print('gpu:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "bayesflow_prior = Prior(batch_prior_fun=prior_fun, param_names=param_names)\n",
    "bayes_simulator = Simulator(batch_simulator_fun=partial(generate_population_data,\n",
    "                                                        cells_in_population=cells_in_population,\n",
    "                                                        max_length=max_sequence_length))\n",
    "generative_model = GenerativeModel(prior=bayesflow_prior, simulator=bayes_simulator,\n",
    "                                   skip_test=True,  # once is enough, simulation takes time\n",
    "                                   name=\"Normalizing Flow Generative Model\")"
   ],
   "id": "cecd0d3e2713e759",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if presimulate:\n",
    "    print('presimulating')\n",
    "    from time import sleep\n",
    "    sleep(job_array_id)\n",
    "\n",
    "    # we create on batch per job and save it in a folder\n",
    "    epoch_id = job_array_id // iterations_per_epoch\n",
    "    generative_model.presimulate_and_save(\n",
    "        batch_size=batch_size,\n",
    "        folder_path=presimulation_path+f'/epoch_{epoch_id}',\n",
    "        iterations_per_epoch=1,\n",
    "        epochs=1,\n",
    "        extend_from=job_array_id,\n",
    "        disable_user_input=True\n",
    "    )\n",
    "    print('Done!')"
   ],
   "id": "958a416f9ac30668",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.exists(os.path.join(gp, 'validation_data.pickle')):\n",
    "    with open(os.path.join(gp, 'validation_data.pickle'), 'rb') as f:\n",
    "        valid_data = pickle.load(f)\n",
    "else:\n",
    "    print('Generating validation data')\n",
    "    valid_data = generative_model(n_val_data)\n",
    "    # save the data\n",
    "    with open(os.path.join(gp, 'validation_data.pickle'), 'wb') as f:\n",
    "        pickle.dump(valid_data, f)\n",
    "\n",
    "x_mean = np.nanmean(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "x_std = np.nanstd(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "p_mean = np.mean(valid_data['prior_draws'], axis=0)\n",
    "p_std = np.std(valid_data['prior_draws'], axis=0)\n",
    "print('Mean and std of data:', x_mean, x_std)\n",
    "print('Mean and std of parameters:', p_mean, p_std)"
   ],
   "id": "df1829ce49462436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = 0  # best: 0, ensemble: 3, only summary: 10\n",
    "trainer = load_model(\n",
    "    model_id=model_id,\n",
    "    x_mean=x_mean,\n",
    "    x_std=x_std,\n",
    "    p_mean=p_mean,\n",
    "    p_std=p_std,\n",
    "    generative_model=generative_model\n",
    ")"
   ],
   "id": "b81539411969d014",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check if the model is already trained\n",
    "if not os.path.exists(trainer.checkpoint_path) and not isinstance(trainer, EnsembleTrainer):\n",
    "    trainer._setup_optimizer(\n",
    "        optimizer=None,\n",
    "        epochs=epochs,\n",
    "        iterations_per_epoch=iterations_per_epoch\n",
    "    )\n",
    "\n",
    "    history = trainer.train_from_presimulation(\n",
    "        presimulation_path=presimulation_path,\n",
    "        optimizer=trainer.optimizer,\n",
    "        max_epochs=epochs,\n",
    "        early_stopping=True,\n",
    "        early_stopping_args={'patience': 17 - 2},\n",
    "        custom_loader=custom_loader,\n",
    "        validation_sims=valid_data\n",
    "    )\n",
    "    print('Training done!')\n",
    "else:\n",
    "    history = trainer.loss_history.get_plottable()\n",
    "\n",
    "bf.diagnostics.plot_losses(history['train_losses'], history['val_losses'], fig_size=(10, 6))\n",
    "print('Final validation loss:', np.min(history['val_losses']))\n",
    "\n",
    "# model 0: -0.6\n",
    "# model 1: -0.8\n",
    "# model 2: -0.7"
   ],
   "id": "a7c0835963566e42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Diagnostic plots",
   "id": "c72ff040811c2643"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "valid_data_config = trainer.configurator(valid_data)",
   "id": "4f6939fee4bd108a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if model_id != 10:\n",
    "    posterior_samples = trainer.amortizer.sample(valid_data_config, n_samples=1000)\n",
    "    posterior_samples = posterior_samples * p_std + p_mean\n",
    "    if isinstance(valid_data_config, list):  # for ensemble\n",
    "        prior_draws = valid_data_config[0][\"parameters\"] * p_std + p_mean\n",
    "    else:\n",
    "        prior_draws = valid_data_config[\"parameters\"] * p_std + p_mean\n",
    "\n",
    "    print('RMSE:', compute_rmse(prior_draws, posterior_samples))\n",
    "    print('RMSE average:', np.mean(compute_rmse(prior_draws, posterior_samples)))\n",
    "\n",
    "    # model 0: 0.202\n",
    "    # model 1: 0.196\n",
    "    # model 2: 0.208\n",
    "    # ensemble: 0.202\n",
    "\n",
    "    bf.diagnostics.plot_sbc_ecdf(posterior_samples, prior_draws, difference=True, param_names=log_param_names);\n",
    "    plt.savefig(f'{trainer.checkpoint_path}/sbc_ecdf.pdf')\n",
    "\n",
    "    bf.diagnostics.plot_recovery(posterior_samples, prior_draws, param_names=log_param_names);\n",
    "    plt.savefig(f'{trainer.checkpoint_path}/recovery.pdf')\n",
    "\n",
    "    bf.diagnostics.plot_z_score_contraction(posterior_samples, prior_draws, param_names=log_param_names);"
   ],
   "id": "f6ad80516d7191c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# regress summary statistics to parameters to identify the most important latent dimensions\n",
    "if isinstance(trainer, EnsembleTrainer):\n",
    "    assert False  # changes the valid_data_config to a list\n",
    "    summary_output = trainer.amortizer.summary_net(valid_data_config)\n",
    "    valid_data_config = valid_data_config[0]\n",
    "else:\n",
    "    summary_output = trainer.amortizer.summary_net(valid_data_config['summary_conditions'])\n",
    "\n",
    "if model_id == 10:\n",
    "    summary_output = summary_output * p_std + p_mean\n",
    "    fig = bf.diagnostics.plot_recovery(summary_output[:, np.newaxis], prior_draws, param_names=log_param_names)\n",
    "    plt.savefig(f'{trainer.checkpoint_path}/summary_space_recovery.pdf')\n",
    "    print('RMSE:', compute_rmse(prior_draws, summary_output[:, np.newaxis]).mean())\n",
    "\n",
    "# perform a regression of each parameter to the summary statistics\n",
    "regressors = []\n",
    "for i in range(valid_data_config['parameters'].shape[1]):\n",
    "    reg = Lasso(alpha=0.1).fit(summary_output, valid_data_config['parameters'][:, i])\n",
    "    regressors.append(reg.coef_)\n",
    "\n",
    "# Convert list of coefficients to a NumPy array for visualization\n",
    "coeff_matrix = np.array(regressors).T\n",
    "\n",
    "# Identify the parameter for which each latent dimension is most important\n",
    "dominant_param = np.argmax(np.abs(coeff_matrix), axis=1)\n",
    "\n",
    "# Group latent dimensions based on the dominant parameter\n",
    "grouped_indices = []\n",
    "for param_idx in range(n_params):\n",
    "    group = [i for i in range(summary_output.shape[1]) if dominant_param[i] == param_idx]\n",
    "    grouped_indices.extend(group)\n",
    "\n",
    "# Reorder the coefficient matrix based on the grouped indices\n",
    "grouped_coeff_matrix = coeff_matrix[grouped_indices, :]\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(\n",
    "    np.abs(grouped_coeff_matrix).T,\n",
    "    annot=True,\n",
    "    cmap=sns.color_palette(\"flare\", as_cmap=True),\n",
    "    yticklabels=log_param_names,\n",
    "    xticklabels=[f\"Dim {i}\" for i in range(summary_output.shape[1])],\n",
    "    cbar_kws={'label': 'Absolute Coefficient Value'}\n",
    ")\n",
    "#plt.title(\"Regression Coefficients: Latent Dimensions vs Model Parameters\")\n",
    "plt.ylabel(\"Model Parameters\")\n",
    "plt.xlabel(\"Latent Dimensions\")\n",
    "plt.savefig(f'{trainer.checkpoint_path}/summary_space_regression_coefficients.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# apply a UMAP to the summary statistics\n",
    "reducer = umap.UMAP(random_state=42, n_jobs=1)\n",
    "embedding = reducer.fit_transform(summary_output)\n",
    "\n",
    "fix, ax = plt.subplots(1, n_params, sharey=True, sharex=True, figsize=(12, 3))\n",
    "for i in range(n_params):\n",
    "    # color code base on size of parameter\n",
    "    colors = valid_data_config['parameters'][:, i]\n",
    "    # min max scaling\n",
    "    colors = (colors - np.min(colors)) / (np.max(colors) - np.min(colors))\n",
    "    # map to colormap\n",
    "    colormap = plt.get_cmap('flare')\n",
    "    colors = colormap(colors)\n",
    "\n",
    "    ax[i].scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        # color code base on size of parameter\n",
    "        c=colors,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    ax[i].set_title(f\"{log_param_names[i]}\")\n",
    "\n",
    "# add colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=colormap, norm=plt.Normalize(vmin=np.min(colors), vmax=np.max(colors)))\n",
    "sm._A = []\n",
    "plt.colorbar(sm, ax=ax, label='Normalized Parameter Value')\n",
    "plt.savefig(f'{trainer.checkpoint_path}/summary_space_umap.pdf')\n",
    "plt.show()"
   ],
   "id": "681603b5f2e87a03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Test on synthetic data\n",
    "\n",
    "Here we use the best model."
   ],
   "id": "26f7a6d31b5bf000"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# simulate test data\n",
    "for test_id in [0, 1, 2]:\n",
    "    print(test_id)\n",
    "    np.random.seed(42+test_id)\n",
    "    test_params = np.array(list(prior.rvs().values()))\n",
    "    if not os.path.exists(os.path.join(gp, f'test_sim_{test_id}.npy')):\n",
    "        test_sim_full = bayes_simulator(test_params[np.newaxis])\n",
    "        test_sim = test_sim_full['sim_data']\n",
    "        np.save(os.path.join(gp, f'test_sim_{test_id}.npy'), test_sim)\n",
    "    else:\n",
    "        test_sim = np.load(os.path.join(gp, f'test_sim_{test_id}.npy'))\n",
    "        test_sim_full = {'sim_data': test_sim}\n",
    "\n",
    "    test_posterior_samples = trainer.amortizer.sample(trainer.configurator(test_sim_full), n_samples=1000)\n",
    "    test_posterior_samples = test_posterior_samples * p_std + p_mean\n",
    "    test_posterior_samples_median = np.median(test_posterior_samples, axis=0)\n",
    "    # compute the log posterior of the test data\n",
    "    input_dict = {\n",
    "        'sim_data': np.repeat(test_sim, repeats=100, axis=0),\n",
    "        'parameters': test_posterior_samples\n",
    "    }\n",
    "\n",
    "    # save posterior samples to load for abc comparison (only for the best model)\n",
    "    #np.save(f'abc_results_{test_id}/posterior_samples_npe.npy', test_posterior_samples)\n",
    "\n",
    "    fig = plot_posterior_2d(posterior_draws=test_posterior_samples,\n",
    "                        prior_draws=prior_draws[:test_posterior_samples.shape[0]],\n",
    "                        param_names=log_param_names,\n",
    "                        true_params=test_params)\n",
    "    plt.show()"
   ],
   "id": "aaf9d11a06bc442f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Real Data",
   "id": "dc88096666755040"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from load_data import load_real_data\n",
    "\n",
    "wasserstein_distance_dict = {0: np.nan, 1: np.nan}\n",
    "samples_dict = {0: np.nan, 1: np.nan}\n",
    "prior_draws = prior_fun(1000)"
   ],
   "id": "88f5e4f6b323ebcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_, real_data_full = load_real_data(data_id=1,\n",
    "                                   max_sequence_length=max_sequence_length,\n",
    "                                   cells_in_population=cells_in_population,\n",
    "                                   plot_data=True)\n",
    "print(real_data_full.shape)"
   ],
   "id": "748dffc6b156ef8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Zelldichte macht was aus\n",
    "\n",
    "Wo wollen Zellen hin? welche parameter beeinflussen das ganze? Zelldichte?\n",
    "\n",
    "1,5mm\n",
    "\n",
    "1 (nicht so gut, extrema)\n",
    "739.79x279.74  microns\n",
    "20231x768 pixel\n",
    "\n",
    "\n",
    "2 (wesentlich mehr der Wahrheit)\n",
    "882.94x287.03 microns\n",
    "2424x788 pixel\n"
   ],
   "id": "3212d01370d344d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# batch the real data\n",
    "real_posterior_samples = trainer.amortizer.sample(trainer.configurator({'sim_data': real_data_full[np.newaxis]}),\n",
    "                                          n_samples=prior_draws.shape[0])\n",
    "real_posterior_samples = real_posterior_samples * p_std + p_mean\n",
    "\n",
    "if isinstance(trainer, EnsembleTrainer):\n",
    "    np.save(f'abc_results_real/posterior_samples_npe_ensemble.npy', real_posterior_samples)\n",
    "else:\n",
    "    np.save(f'abc_results_real/posterior_samples_npe.npy', real_posterior_samples)"
   ],
   "id": "17248751d4533848",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig = plot_posterior_2d(posterior_draws=real_posterior_samples,\n",
    "                        prior_draws=prior_draws[:real_posterior_samples.shape[0]],\n",
    "                        param_names=log_param_names)\n",
    "plt.savefig(f'{trainer.checkpoint_path}/full_posterior.png')\n",
    "plt.show()"
   ],
   "id": "aeefa13b15ae4ce6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if trainer.amortizer.summary_loss is not None:\n",
    "    from matplotlib.cm import viridis\n",
    "    real_data_config = trainer.configurator({'sim_data': real_data_full[np.newaxis]})\n",
    "    summary_statistics = trainer.amortizer.summary_net(valid_data_config['summary_conditions'])\n",
    "    summary_statistics_obs = trainer.amortizer.summary_net(real_data_config['summary_conditions'])\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    colors = viridis(np.linspace(0.1, 0.9, 2))\n",
    "    ax.scatter(\n",
    "        summary_statistics_obs[:, 0], summary_statistics_obs[:, 1], color=colors[0], label=r\"Observed: $h_{\\psi}(x_{obs})$\"\n",
    "    )\n",
    "    ax.scatter(summary_statistics[:, 0], summary_statistics[:, 1], color=colors[1], label=r\"Well-specified: $h_{\\psi}(x)$\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.2)\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "    \n",
    "    fig.savefig(f'abc_results_real/Real Summary Latent Space.pdf', bbox_inches='tight')\n",
    "    \n",
    "    MMD_sampling_distribution, MMD_observed = trainer.mmd_hypothesis_test(\n",
    "        observed_data=real_data_config, \n",
    "        reference_data=valid_data_config,  # if not provided, will use the generative model\n",
    "        num_null_samples=500,\n",
    "        bootstrap=True  # if True, use the reference data as null samples\n",
    "    )\n",
    "    fig = bf.diagnostics.plot_mmd_hypothesis_test(MMD_sampling_distribution, MMD_observed)\n",
    "    fig.savefig(f'abc_results_real/Real MMD.pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "id": "d79424027ea8180a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Different Number of Cells in Experiment",
   "id": "1d4036aaff03efd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.stats import linregress\n",
    "from scipy.stats import median_abs_deviation"
   ],
   "id": "80307fddc94faeb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# batch the real data\n",
    "n_cell_in_batch = np.arange(1, len(real_data_full))\n",
    "stats_n_cells = {}\n",
    "np.random.seed(0)\n",
    "\n",
    "real_posterior_samples_partial = []\n",
    "real_posterior_samples_artificial = []\n",
    "for n_cells in tqdm(n_cell_in_batch):\n",
    "    n_rand_index = np.random.choice(len(real_data_full), size=n_cells, replace=False)\n",
    "    partial_data = real_data_full[n_rand_index]\n",
    "\n",
    "    real_posterior_samples = trainer.amortizer.sample(trainer.configurator({'sim_data': partial_data[np.newaxis]}),\n",
    "                                                      n_samples=100)\n",
    "    real_posterior_samples = real_posterior_samples * p_std + p_mean\n",
    "    real_posterior_samples_partial.append(real_posterior_samples)\n",
    "\n",
    "real_posterior_samples_partial = np.stack(real_posterior_samples_partial)"
   ],
   "id": "2c4b9f4d313543e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "real_posterior_samples_partial_median = np.median(real_posterior_samples_partial, axis=1)\n",
    "real_posterior_samples_partial_std = median_abs_deviation(real_posterior_samples_partial, axis=1)"
   ],
   "id": "b0e3b2b8aa6196c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 4, sharex='row', sharey='col', figsize=(12, 4), tight_layout=True)\n",
    "\n",
    "for i, p_name in enumerate(log_param_names):\n",
    "    reg = linregress(n_cell_in_batch, real_posterior_samples_partial_median[:, i])\n",
    "\n",
    "    ax[i].errorbar(n_cell_in_batch,  real_posterior_samples_partial_median[:, i],\n",
    "                  yerr=real_posterior_samples_partial_std[:, i],\n",
    "                  fmt='o',\n",
    "                  alpha=0.5, label='Estimate (median $\\pm$ absolute deviation)' if i == 0 else None)\n",
    "\n",
    "    reg = linregress(n_cell_in_batch, real_posterior_samples_partial_median[:, i])\n",
    "    #if reg.pvalue < 0.01:\n",
    "    ax[i].plot(n_cell_in_batch, reg.slope*n_cell_in_batch + reg.intercept,\n",
    "                     color='black', label=f'Regression Line' if i == 1 else None,\n",
    "                   zorder=4)\n",
    "\n",
    "\n",
    "    ax[i].set_title(f\"Correlation {reg.rvalue:.4f}\\n(p-value: {reg.pvalue:.4f})\")\n",
    "    ax[i].set_xlabel('Number of Cells')\n",
    "    ax[i].set_ylabel(f'Median of {log_param_names[i]}')\n",
    "    ax[i].set_ylim(limits_log[list(limits_log.keys())[i]])\n",
    "\n",
    "fig.legend(loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.1))\n",
    "plt.savefig(f'abc_results_real/real_ncells_vs_parameter_median.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "b09bb5e7f54affc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "88d703598d0f0e1a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
