{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference with Neural Posterior Estimation (NPE)",
   "id": "3d684170c7ac2952"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pickle\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pyabc\n",
    "from fitmulticell import model as morpheus_model\n",
    "from fitmulticell.sumstat import SummaryStatistics\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import umap\n",
    "from sklearn.linear_model import Lasso\n",
    "import seaborn as sns\n",
    "\n",
    "from load_bayesflow_model import load_model, custom_loader, EnsembleTrainer\n",
    "from plotting_routines import plot_posterior_2d\n",
    "from summary_stats import reduce_to_coordinates, compute_rmse\n",
    "\n",
    "import tensorflow as tf\n",
    "import bayesflow as bf\n",
    "from bayesflow.simulation import GenerativeModel, Prior, Simulator\n",
    "\n",
    "# get the job array id and number of processors\n",
    "job_array_id = 0 #int(os.environ.get('SLURM_ARRAY_TASK_ID', 0))\n",
    "n_procs = 10 #int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",
    "print(job_array_id)\n",
    "on_cluster = False"
   ],
   "id": "10b55f49c4d5c93c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if on_cluster:\n",
    "    gp = '/home/jarruda_hpc/CellMigration/synth_data_params_bayesflow'\n",
    "else:\n",
    "    gp = os.getcwd()\n",
    "\n",
    "# defining the mapping of parameter inside the model xml file. the dictionary name is for \n",
    "# parameter name, and the value are the mapping values, to get the map value for parameter \n",
    "# check here: https://fitmulticell.readthedocs.io/en/latest/example/minimal.html#Inference-problem-definition\n",
    "\n",
    "par_map = {\n",
    "    'gradient_strength': './CellTypes/CellType/Constant[@symbol=\"gradient_strength\"]',\n",
    "    'move.strength': './CellTypes/CellType/Constant[@symbol=\"move.strength\"]',\n",
    "    'move.duration.rate': './CellTypes/CellType/Constant[@symbol=\"move.duration.rate\"]',\n",
    "    'cell_nodes_real': './Global/Constant[@symbol=\"cell_nodes_real\"]',\n",
    "}\n",
    "\n",
    "model_path = gp + \"/cell_movement_v24.xml\"  # time step is 30sec, move.dir completely normalized, init move.dir rand in all directions\n",
    "# defining the summary statistics function\n",
    "max_sequence_length = 120\n",
    "min_sequence_length = 0\n",
    "only_longest_traj_per_cell = True  # mainly to keep the data batchable\n",
    "sumstat = SummaryStatistics(sum_stat_calculator=partial(reduce_to_coordinates,\n",
    "                                                        minimal_length=min_sequence_length,\n",
    "                                                        maximal_length=max_sequence_length,\n",
    "                                                        only_longest_traj_per_cell=only_longest_traj_per_cell))                    \n",
    "\n",
    "if on_cluster:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "    # note: remember also change tiff path in model.xml!\n",
    "else:\n",
    "    # define the model object\n",
    "    model = morpheus_model.MorpheusModel(\n",
    "        model_path, par_map=par_map, par_scale=\"log10\",\n",
    "        show_stdout=False, show_stderr=False,\n",
    "        clean_simulation=True,\n",
    "        raise_on_error=False, sumstat=sumstat)\n",
    "\n",
    "\n",
    "# parameter values used to generate the synthetic data\n",
    "obs_pars = {\n",
    "    'gradient_strength': 100.,  # strength of the gradient of chemotaxis (energy potential)\n",
    "    'move.strength': 10.,  # strength of directed motion (energy potential)\n",
    "    'move.duration.rate': 0.1,  # rate of exponential distribution (1/seconds)\n",
    "    'cell_nodes_real': 50.,  # area of the cell  (\\mu m^2)\n",
    "}\n",
    "\n",
    "obs_pars_log = {key: np.log10(val) for key, val in obs_pars.items()}\n",
    "limits = {'gradient_strength': (1, 10000), #(10 ** 4, 10 ** 8),\n",
    "          'move.strength': (1, 100),\n",
    "          'move.duration.rate': (1e-4, 30), #(math.log10((10 ** -2) * 30), math.log10((10 ** 4))), # smallest time step in simulation 5\n",
    "          'cell_nodes_real': (1, 300)}\n",
    "limits_log = {key: (np.log10(val[0]), np.log10(val[1])) for key, val in limits.items()}\n",
    "\n",
    "\n",
    "prior = pyabc.Distribution(**{key: pyabc.RV(\"uniform\", loc=lb, scale=ub-lb)\n",
    "                              for key, (lb, ub) in limits_log.items()})\n",
    "param_names = ['$m_{\\\\text{dir}}$', '$m_{\\\\text{rand}}$', '$\\lambda$', '$a$']\n",
    "log_param_names = ['$\\log_{10}(m_{\\\\text{dir}})$', '$\\log_{10}(m_{\\\\text{rand}})$', '$\\log_{10}(\\lambda)$', '$\\log_{10}(a)$']\n",
    "print(obs_pars)"
   ],
   "id": "74390dfb0e802a77",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def prior_fun(batch_size: int) -> np.ndarray:\n",
    "    samples = []\n",
    "    for _ in range(batch_size):\n",
    "        samples.append(list(prior.rvs().values()))\n",
    "    return np.array(samples)\n",
    "\n",
    "\n",
    "def generate_population_data(param_batch: np.ndarray, cells_in_population: int, max_length: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate population data\n",
    "    :param param_batch:  batch of parameters\n",
    "    :param cells_in_population:  number of cells in a population (50)\n",
    "    :param max_length:  maximum length of the sequence\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_batch = []\n",
    "    for params in param_batch:\n",
    "        params_dict = {key: p for key, p in zip(obs_pars.keys(), params)}\n",
    "        sim = model.sample(params_dict)\n",
    "        data_batch.append(sim)  # generates a cell population in one experiment\n",
    "\n",
    "    data_batch_transformed = np.ones((param_batch.shape[0], cells_in_population, max_length, 3)) * np.nan\n",
    "    # each cell is of different length, each with x and y coordinates, make a tensor out of it\n",
    "    n_cells_not_visible = 0\n",
    "    for p_id, population_sim in enumerate(data_batch):\n",
    "        if len(population_sim) == 0:\n",
    "            # no cells were visible in the simulation\n",
    "            n_cells_not_visible += 1\n",
    "            continue\n",
    "        for c_id, cell_sim in enumerate(population_sim):\n",
    "            # pre-pad the data with zeros, but first write zeros as nans to compute the mean and std\n",
    "            data_batch_transformed[p_id, c_id, -len(cell_sim['x']):, 0] = cell_sim['x']\n",
    "            data_batch_transformed[p_id, c_id, -len(cell_sim['y']):, 1] = cell_sim['y']\n",
    "            data_batch_transformed[p_id, c_id, -len(cell_sim['t']):, 2] = cell_sim['t']\n",
    "\n",
    "    if n_cells_not_visible > 0:\n",
    "        print(f'Simulation with no cells visible: {n_cells_not_visible}/{len(data_batch)}')\n",
    "    return data_batch_transformed"
   ],
   "id": "c606f75e3a4c0257",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "presimulate = False\n",
    "presimulation_path = 'presimulations'\n",
    "n_val_data = 300\n",
    "cells_in_population = 143\n",
    "n_params = len(obs_pars)\n",
    "batch_size = 32\n",
    "iterations_per_epoch = 100\n",
    "# 1000 batches to be generated, 10 epochs until the batch is used again\n",
    "epochs = 500  # 50 effective epochs\n",
    "\n",
    "# check if gpu is available\n",
    "print('gpu:', tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "bayesflow_prior = Prior(batch_prior_fun=prior_fun, param_names=param_names)\n",
    "bayes_simulator = Simulator(batch_simulator_fun=partial(generate_population_data,\n",
    "                                                        cells_in_population=cells_in_population,\n",
    "                                                        max_length=max_sequence_length))\n",
    "generative_model = GenerativeModel(prior=bayesflow_prior, simulator=bayes_simulator,\n",
    "                                   skip_test=True,  # once is enough, simulation takes time\n",
    "                                   name=\"Normalizing Flow Generative Model\")"
   ],
   "id": "cecd0d3e2713e759",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if presimulate:\n",
    "    print('presimulating')\n",
    "    from time import sleep\n",
    "    sleep(job_array_id)\n",
    "\n",
    "    # we create on batch per job and save it in a folder\n",
    "    epoch_id = job_array_id // iterations_per_epoch\n",
    "    generative_model.presimulate_and_save(\n",
    "        batch_size=batch_size,\n",
    "        folder_path=presimulation_path+f'/epoch_{epoch_id}',\n",
    "        iterations_per_epoch=1,\n",
    "        epochs=1,\n",
    "        extend_from=job_array_id,\n",
    "        disable_user_input=True\n",
    "    )\n",
    "    print('Done!')"
   ],
   "id": "958a416f9ac30668",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if os.path.exists(os.path.join(gp, 'validation_data.pickle')):  # this was used in training\n",
    "    with open(os.path.join(gp, 'validation_data.pickle'), 'rb') as f:\n",
    "        valid_data = pickle.load(f)\n",
    "else:\n",
    "    print('Generating validation data')\n",
    "    valid_data = generative_model(n_val_data)\n",
    "    # save the data\n",
    "    with open(os.path.join(gp, 'validation_data_1000.pickle'), 'wb') as f:\n",
    "        pickle.dump(valid_data, f)\n",
    "\n",
    "x_mean = np.nanmean(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "x_std = np.nanstd(valid_data['sim_data'], axis=(0, 1, 2))\n",
    "p_mean = np.mean(valid_data['prior_draws'], axis=0)\n",
    "p_std = np.std(valid_data['prior_draws'], axis=0)\n",
    "print('Mean and std of data:', x_mean, x_std)\n",
    "print('Mean and std of parameters:', p_mean, p_std)\n",
    "\n",
    "if os.path.exists(os.path.join(gp, 'validation_data_1000.pickle')):\n",
    "    with open(os.path.join(gp, 'validation_data_1000.pickle'), 'rb') as f:\n",
    "        valid_data = pickle.load(f)\n",
    "\n",
    "for k, v in valid_data.items():\n",
    "    if not v is None:\n",
    "        valid_data[k] = v[:n_val_data]"
   ],
   "id": "df1829ce49462436",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_id = 0 # best: 0, ensemble: 3, only summary: 10\n",
    "trainer = load_model(\n",
    "    model_id=model_id,\n",
    "    x_mean=x_mean,\n",
    "    x_std=x_std,\n",
    "    p_mean=p_mean,\n",
    "    p_std=p_std,\n",
    "    generative_model=generative_model\n",
    ")"
   ],
   "id": "b81539411969d014",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check if the model is already trained\n",
    "if not os.path.exists(trainer.checkpoint_path) and not isinstance(trainer, EnsembleTrainer):\n",
    "    trainer._setup_optimizer(\n",
    "        optimizer=None,\n",
    "        epochs=epochs,\n",
    "        iterations_per_epoch=iterations_per_epoch\n",
    "    )\n",
    "\n",
    "    history = trainer.train_from_presimulation(\n",
    "        presimulation_path=presimulation_path,\n",
    "        optimizer=trainer.optimizer,\n",
    "        max_epochs=epochs,\n",
    "        early_stopping=True,\n",
    "        early_stopping_args={'patience': 17 - 2},\n",
    "        custom_loader=custom_loader,\n",
    "        validation_sims=valid_data\n",
    "    )\n",
    "    print('Training done!')\n",
    "else:\n",
    "    history = trainer.loss_history.get_plottable()\n",
    "\n",
    "if 'train_losses' in history.keys():\n",
    "    bf.diagnostics.plot_losses(history['train_losses'], history['val_losses'], fig_size=(10, 6))\n",
    "    print('Final validation loss:', np.min(history['val_losses']))\n",
    "\n",
    "# model 0: -0.6\n",
    "# model 1: -0.8\n",
    "# model 2: -0.7"
   ],
   "id": "a7c0835963566e42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "valid_data_config = trainer.configurator(valid_data)\n",
    "\n",
    "if isinstance(trainer, EnsembleTrainer):\n",
    "    prior_draws = valid_data_config[0][\"parameters\"] * p_std + p_mean\n",
    "else:\n",
    "    prior_draws = valid_data_config[\"parameters\"] * p_std + p_mean"
   ],
   "id": "14ce9fcfe5385f2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Diagnostic plots",
   "id": "c72ff040811c2643"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.stats import median_abs_deviation\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "def plot_recovery(\n",
    "    post_samples,\n",
    "    prior_samples,\n",
    "    point_agg=np.median,\n",
    "    uncertainty_agg=median_abs_deviation,\n",
    "    param_names=None,\n",
    "    fig_size=None,\n",
    "    label_fontsize=16,\n",
    "    title_fontsize=18,\n",
    "    metric_fontsize=16,\n",
    "    tick_fontsize=12,\n",
    "    add_corr=True,\n",
    "    add_r2=True,\n",
    "    color=\"#8f2727\",\n",
    "    n_col=None,\n",
    "    n_row=None,\n",
    "    xlabel=\"Ground truth\",\n",
    "    ylabel=\"Estimated\",\n",
    "    color_weights=None,          # 1D array of shape (n_data_sets,) with values in [0,1]\n",
    "    color_cmap=\"cividis\",\n",
    "    add_colorbar=True,\n",
    "    colorbar_label=\"mean error\",\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Recovery plot with optional per-dataset color from values in [0,1].\"\"\"\n",
    "\n",
    "    est = point_agg(post_samples, axis=1)\n",
    "    if uncertainty_agg is not None:\n",
    "        u = uncertainty_agg(post_samples, axis=1)\n",
    "\n",
    "    n_datasets, n_params = prior_samples.shape\n",
    "    if param_names is None:\n",
    "        param_names = [f\"$\\\\theta_{{{i}}}$\" for i in range(1, n_params + 1)]\n",
    "\n",
    "    if n_row is None and n_col is None:\n",
    "        n_row = int(np.ceil(n_params / 6))\n",
    "        n_col = int(np.ceil(n_params / n_row))\n",
    "    elif n_row is None and n_col is not None:\n",
    "        n_row = int(np.ceil(n_params / n_col))\n",
    "    elif n_row is not None and n_col is None:\n",
    "        n_col = int(np.ceil(n_params / n_row))\n",
    "\n",
    "    if fig_size is None:\n",
    "        fig_size = (int(4 * n_col), int(4 * n_row))\n",
    "    f, axarr = plt.subplots(n_row, n_col, figsize=fig_size)\n",
    "\n",
    "    # Color mapping\n",
    "    use_colormap = color_weights is not None\n",
    "    if use_colormap:\n",
    "        cw = np.asarray(color_weights, dtype=float)\n",
    "        if cw.shape[0] != n_datasets:\n",
    "            raise ValueError(\"color_weights must have length equal to n_data_sets\")\n",
    "        cmap = plt.get_cmap(color_cmap)\n",
    "        norm = plt.Normalize(vmin=0, vmax=0.25)\n",
    "        colors_arr = cmap(norm(cw))\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "\n",
    "    axarr = np.atleast_1d(axarr)\n",
    "    axiter = axarr.flat if (n_col > 1 or n_row > 1) else axarr\n",
    "\n",
    "    for i, ax in enumerate(axiter):\n",
    "        if i >= n_params:\n",
    "            break\n",
    "\n",
    "        x = prior_samples[:, i]\n",
    "        y = est[:, i]\n",
    "\n",
    "        if uncertainty_agg is None:\n",
    "            if use_colormap:\n",
    "                _ = ax.scatter(x, y, c=cw, cmap=color_cmap, alpha=0.8, **kwargs)\n",
    "            else:\n",
    "                _ = ax.scatter(x, y, alpha=0.5, color=color, **kwargs)\n",
    "        else:\n",
    "            if use_colormap:\n",
    "                for j in range(n_datasets):\n",
    "                    ax.errorbar(\n",
    "                        x[j],\n",
    "                        y[j],\n",
    "                        yerr=u[j, i],\n",
    "                        fmt=\"o\",\n",
    "                        color=colors_arr[j],\n",
    "                        alpha=0.8,\n",
    "                        capsize=2,\n",
    "                        **kwargs,\n",
    "                    )\n",
    "            else:\n",
    "                _ = ax.errorbar(x, y, yerr=u[:, i], fmt=\"o\", alpha=0.5, color=color, **kwargs)\n",
    "\n",
    "        lower = min(x.min(), y.min())\n",
    "        upper = max(x.max(), y.max())\n",
    "        eps = (upper - lower) * 0.1\n",
    "        ax.set_xlim([lower - eps, upper + eps])\n",
    "        ax.set_ylim([lower - eps, upper + eps])\n",
    "        ax.plot(\n",
    "            [ax.get_xlim()[0], ax.get_xlim()[1]],\n",
    "            [ax.get_ylim()[0], ax.get_ylim()[1]],\n",
    "            color=\"black\",\n",
    "            alpha=0.9,\n",
    "            linestyle=\"dashed\",\n",
    "        )\n",
    "\n",
    "        if add_r2:\n",
    "            r2 = r2_score(x, y)\n",
    "            ax.text(0.1, 0.9, \"$R^2$ = {:.3f}\".format(r2), transform=ax.transAxes, size=metric_fontsize)\n",
    "        if add_corr:\n",
    "            corr = np.corrcoef(x, y)[0, 1]\n",
    "            ax.text(0.1, 0.8, \"$r$ = {:.3f}\".format(corr), transform=ax.transAxes, size=metric_fontsize)\n",
    "        ax.set_title(param_names[i], fontsize=title_fontsize)\n",
    "\n",
    "        sns.despine(ax=ax)\n",
    "        ax.grid(alpha=0.5)\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=tick_fontsize)\n",
    "        ax.tick_params(axis=\"both\", which=\"minor\", labelsize=tick_fontsize)\n",
    "\n",
    "    bottom_row = axarr if n_row == 1 else axarr[0] if n_col == 1 else axarr[n_row - 1, :]\n",
    "    for _ax in bottom_row:\n",
    "        _ax.set_xlabel(xlabel, fontsize=label_fontsize)\n",
    "\n",
    "    if n_row == 1:\n",
    "        axarr[0].set_ylabel(ylabel, fontsize=label_fontsize)\n",
    "    else:\n",
    "        for _ax in axarr[:, 0]:\n",
    "            _ax.set_ylabel(ylabel, fontsize=label_fontsize)\n",
    "\n",
    "    for _ax in list(axiter)[n_params:]:\n",
    "        _ax.remove()\n",
    "\n",
    "    if use_colormap and add_colorbar:\n",
    "        cbar = f.colorbar(sm, ax=axarr.ravel().tolist(), pad=-0.43)\n",
    "        cbar.set_label(colorbar_label, fontsize=metric_fontsize)\n",
    "\n",
    "    f.tight_layout()\n",
    "    return f"
   ],
   "id": "570810ce377e489",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if model_id != 10:\n",
    "    posterior_samples = []\n",
    "    for n in tqdm(range(n_val_data//100)):  # batching due to memory issues\n",
    "        temp_data = {'summary_conditions': valid_data_config['summary_conditions'][n*100:(n+1)*100]}\n",
    "        posterior_samples_temp = trainer.amortizer.sample(temp_data, n_samples=1000)\n",
    "        posterior_samples.append(posterior_samples_temp)\n",
    "    posterior_samples = np.concatenate(posterior_samples, axis=0)\n",
    "    #posterior_samples = trainer.amortizer.sample(valid_data_config, n_samples=1000)\n",
    "    posterior_samples = posterior_samples * p_std + p_mean\n",
    "\n",
    "    print('RMSE:', compute_rmse(prior_draws, posterior_samples))\n",
    "    print('RMSE average:', np.mean(compute_rmse(prior_draws, posterior_samples)))\n",
    "\n",
    "    # model 0: 0.202\n",
    "    # model 1: 0.196\n",
    "    # model 2: 0.208\n",
    "    # ensemble: 0.202\n",
    "\n",
    "    prior_range = np.max(prior_draws, axis=0)-np.min(prior_draws, axis=0)\n",
    "    color_weights = np.median(np.mean(np.abs(posterior_samples-prior_draws[:, None])/prior_range, axis=-1), axis=1)\n",
    "    #fig = bf.diagnostics.plot_recovery(posterior_samples, prior_draws, param_names=log_param_names, add_corr=False)\n",
    "    fig = plot_recovery(posterior_samples, prior_draws, param_names=log_param_names, add_corr=False,\n",
    "                        color_weights=color_weights)\n",
    "    print('error corr with m_dir', np.corrcoef(color_weights, prior_draws[:, 0])[0, 1])\n",
    "    print('error corr with m_rand', np.corrcoef(color_weights, prior_draws[:, 1])[0, 1])\n",
    "    print('error corr with lambda', np.corrcoef(color_weights, prior_draws[:, 2])[0, 1])\n",
    "    print('error corr with a', np.corrcoef(color_weights, prior_draws[:, 3])[0, 1])\n",
    "    #ax = fig.get_axes()\n",
    "    #ax[2].axvspan(xmin=np.log10(1/30), xmax=limits_log['move.duration.rate'][1],\n",
    "    #              color='black', alpha=0.3, label='rates are smaller than the visible window')\n",
    "    fig.savefig(f'{trainer.checkpoint_path}/recovery.pdf')\n",
    "\n",
    "    fig = bf.diagnostics.plot_sbc_ecdf(posterior_samples, prior_draws, difference=True, param_names=log_param_names)\n",
    "    plt.savefig(f'{trainer.checkpoint_path}/sbc_ecdf.pdf')\n",
    "\n",
    "    #fig = bf.diagnostics.plot_z_score_contraction(posterior_samples, prior_draws, param_names=log_param_names)"
   ],
   "id": "f6ad80516d7191c8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('error corr with m_dir', np.corrcoef(color_weights, prior_draws[:, 0])[0, 1])\n",
    "print('error corr with m_rand', np.corrcoef(color_weights, prior_draws[:, 1])[0, 1])\n",
    "print('error corr with lambda', np.corrcoef(color_weights, prior_draws[:, 2])[0, 1])\n",
    "print('error corr with a', np.corrcoef(color_weights, prior_draws[:, 3])[0, 1])"
   ],
   "id": "6b441c20d3cdfc6c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# # extract only those parameters where lambda is smaller than 1/30\n",
    "# valid_indices = prior_draws[:, 2] < np.log10(1/30)\n",
    "# posterior_samples_valid = posterior_samples[valid_indices]\n",
    "# prior_draws_valid = prior_draws[valid_indices]\n",
    "#\n",
    "# bad_value = 0.5\n",
    "# valid_indices = (np.median(posterior_samples_valid[:, :, 2], axis=1) - prior_draws_valid[:, 2])**2 > bad_value\n",
    "# posterior_samples_valid_bad = posterior_samples_valid[valid_indices]\n",
    "# prior_draws_valid_bad = prior_draws_valid[valid_indices]\n",
    "#\n",
    "# valid_indices = (np.median(posterior_samples_valid[:, :, 2], axis=1) - prior_draws_valid[:, 2])**2 <= bad_value\n",
    "# posterior_samples_valid_good = posterior_samples_valid[valid_indices]\n",
    "# prior_draws_valid_good = prior_draws_valid[valid_indices]\n",
    "#\n",
    "# # generate both figures\n",
    "# fig_bad = bf.diagnostics.plot_recovery(\n",
    "#     posterior_samples_valid_bad, prior_draws_valid_bad,\n",
    "#     param_names=log_param_names, add_corr=False, add_r2=False, color='blue'\n",
    "# )\n",
    "# fig_good = bf.diagnostics.plot_recovery(\n",
    "#     posterior_samples_valid_good, prior_draws_valid_good,\n",
    "#     param_names=log_param_names, add_corr=False, add_r2=False\n",
    "# )\n",
    "#\n",
    "# from matplotlib.lines import Line2D\n",
    "# from matplotlib.collections import LineCollection\n",
    "#\n",
    "# # make sure both are rendered\n",
    "# fig_bad.canvas.draw()\n",
    "# fig_good.canvas.draw()\n",
    "#\n",
    "# for ax_bad, ax_good in zip(fig_bad.axes, fig_good.axes):\n",
    "#     # copy marker only Line2D artists\n",
    "#     for ln in ax_bad.lines:\n",
    "#         if not isinstance(ln, Line2D):\n",
    "#             continue\n",
    "#         x, y = ln.get_xdata(orig=False), ln.get_ydata(orig=False)\n",
    "#         if x is None or y is None or len(x) == 0:\n",
    "#             continue\n",
    "#         marker = ln.get_marker()\n",
    "#         ls = ln.get_linestyle()\n",
    "#         # treat as points if it has a marker or no visible line\n",
    "#         if marker not in [None, \"\"] or ls in [\"\", \"None\", None]:\n",
    "#             ax_good.plot(\n",
    "#                 x, y,\n",
    "#                 linestyle=\"None\",\n",
    "#                 marker=marker or \"o\",\n",
    "#                 markersize=ln.get_markersize(),\n",
    "#                 markerfacecolor=ln.get_markerfacecolor(),\n",
    "#                 markeredgecolor=ln.get_markeredgecolor(),\n",
    "#                 alpha=ln.get_alpha() if ln.get_alpha() is not None else 0.8,\n",
    "#                 zorder=6,\n",
    "#             )\n",
    "#\n",
    "#     # optional copy of simple line collections if needed\n",
    "#     for lc in ax_bad.collections:\n",
    "#         if not isinstance(lc, LineCollection):\n",
    "#             continue\n",
    "#         for seg in lc.get_segments():\n",
    "#             ax_good.plot(seg[:, 0], seg[:, 1],\n",
    "#                          linewidth=(lc.get_linewidths()[0] if len(lc.get_linewidths()) else 1.0),\n",
    "#                          color=(lc.get_colors()[0] if len(lc.get_colors()) else None),\n",
    "#                          alpha=lc.get_alpha() if lc.get_alpha() is not None else 0.8,\n",
    "#                          zorder=4)\n",
    "#\n",
    "# plt.close(fig_bad)\n",
    "# fig_good.tight_layout()\n",
    "# fig_good.canvas.draw()"
   ],
   "id": "a2dadfce9a123611",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# regress summary statistics to parameters to identify the most important latent dimensions\n",
    "if isinstance(trainer, EnsembleTrainer):\n",
    "    assert False  # changes the valid_data_config to a list\n",
    "    summary_output = trainer.amortizer.summary_net(valid_data_config)\n",
    "    valid_data_config = valid_data_config[0]\n",
    "else:\n",
    "    summary_output = trainer.amortizer.summary_net(valid_data_config['summary_conditions'])\n",
    "\n",
    "if model_id == 10:\n",
    "    summary_output = summary_output * p_std + p_mean\n",
    "\n",
    "    prior_range = np.max(prior_draws, axis=0)-np.min(prior_draws, axis=0)\n",
    "    color_weights = np.median(np.mean(np.abs(summary_output[:, np.newaxis]-prior_draws[:, None])/prior_range, axis=-1), axis=1)\n",
    "    #fig = bf.diagnostics.plot_recovery(posterior_samples, prior_draws, param_names=log_param_names, add_corr=False)\n",
    "    fig = plot_recovery(summary_output[:, np.newaxis], prior_draws, param_names=log_param_names, add_corr=False,\n",
    "                        color_weights=color_weights)\n",
    "    #fig = bf.diagnostics.plot_recovery(summary_output[:, np.newaxis], prior_draws, param_names=log_param_names)\n",
    "    #ax = fig.get_axes()\n",
    "    #ax[2].axvspan(xmin=np.log10(1/30), xmax=limits_log['move.duration.rate'][1],\n",
    "    #              color='black', alpha=0.3, label='rates are smaller than the visible window')\n",
    "    plt.savefig(f'{trainer.checkpoint_path}/summary_space_recovery.pdf')\n",
    "    print('RMSE:', compute_rmse(prior_draws, summary_output[:, np.newaxis]).mean())\n",
    "\n",
    "# perform a regression of each parameter to the summary statistics\n",
    "regressors = []\n",
    "for i in range(valid_data_config['parameters'].shape[1]):\n",
    "    reg = Lasso(alpha=0.1).fit(summary_output, valid_data_config['parameters'][:, i])\n",
    "    regressors.append(reg.coef_)\n",
    "\n",
    "# Convert list of coefficients to a NumPy array for visualization\n",
    "coeff_matrix = np.array(regressors).T\n",
    "\n",
    "# Identify the parameter for which each latent dimension is most important\n",
    "dominant_param = np.argmax(np.abs(coeff_matrix), axis=1)\n",
    "\n",
    "# Group latent dimensions based on the dominant parameter\n",
    "grouped_indices = []\n",
    "for param_idx in range(n_params):\n",
    "    group = [i for i in range(summary_output.shape[1]) if dominant_param[i] == param_idx]\n",
    "    grouped_indices.extend(group)\n",
    "\n",
    "# Reorder the coefficient matrix based on the grouped indices\n",
    "grouped_coeff_matrix = coeff_matrix[grouped_indices, :]\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(8, 3), layout='constrained')\n",
    "ax = sns.heatmap(\n",
    "    np.abs(grouped_coeff_matrix).T,\n",
    "    annot=True,\n",
    "    cmap=sns.color_palette(\"flare\", as_cmap=True),\n",
    "    yticklabels=log_param_names,\n",
    "    xticklabels=[f\"Dim {i}\" for i in range(summary_output.shape[1])],\n",
    "    cbar_kws={'label': 'Absolute Coefficient Value'}\n",
    ")\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.ax.yaxis.set_tick_params(labelsize=10)\n",
    "#plt.title(\"Regression Coefficients: Latent Dimensions vs Model Parameters\")\n",
    "plt.ylabel(\"Model Parameters\", fontsize=14)\n",
    "plt.xlabel(\"Latent Dimensions\", fontsize=14)\n",
    "axes = plt.gcf().get_axes()\n",
    "for a in axes:\n",
    "    a.tick_params(axis=\"x\", labelsize=12)\n",
    "    a.tick_params(axis=\"y\", labelsize=12)\n",
    "\n",
    "plt.savefig(f'{trainer.checkpoint_path}/summary_space_regression_coefficients.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# apply a UMAP to the summary statistics\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "embedding = reducer.fit_transform(summary_output)\n",
    "\n",
    "fix, ax = plt.subplots(1, n_params, sharey=True, sharex=True, figsize=(8, 3))\n",
    "for i in range(n_params):\n",
    "    # color code base on size of parameter\n",
    "    colors = valid_data['prior_draws'][:, i]\n",
    "    # min max scaling\n",
    "    colors = (colors - np.min(colors)) / (np.max(colors) - np.min(colors))\n",
    "    # map to colormap\n",
    "    colormap = plt.get_cmap('flare')\n",
    "    colors = colormap(colors)\n",
    "\n",
    "    ax[i].scatter(\n",
    "        embedding[:, 0],\n",
    "        embedding[:, 1],\n",
    "        # color code base on size of parameter\n",
    "        c=colors,\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    ax[i].set_title(f\"{log_param_names[i]}\")\n",
    "\n",
    "# add colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=colormap, norm=plt.Normalize(vmin=np.min(colors), vmax=np.max(colors)))\n",
    "sm._A = []\n",
    "plt.colorbar(sm, ax=ax, label='Normalized Parameter Value')\n",
    "plt.savefig(f'{trainer.checkpoint_path}/summary_space_umap.pdf')\n",
    "plt.show()"
   ],
   "id": "681603b5f2e87a03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Test on synthetic data",
   "id": "26f7a6d31b5bf000"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# simulate test data\n",
    "for test_id in [0, 1, 2]:\n",
    "    print(test_id)\n",
    "    np.random.seed(42+test_id)\n",
    "    test_params = np.array(list(prior.rvs().values()))\n",
    "    if not os.path.exists(os.path.join(gp, f'test_sim_{test_id}.npy')):\n",
    "        test_sim_full = bayes_simulator(test_params[np.newaxis])\n",
    "        test_sim = test_sim_full['sim_data']\n",
    "        np.save(os.path.join(gp, f'test_sim_{test_id}.npy'), test_sim)\n",
    "    else:\n",
    "        test_sim = np.load(os.path.join(gp, f'test_sim_{test_id}.npy'))\n",
    "        test_sim_full = {'sim_data': test_sim}\n",
    "\n",
    "    test_posterior_samples = trainer.amortizer.sample(trainer.configurator(test_sim_full), n_samples=1000)\n",
    "    test_posterior_samples = test_posterior_samples * p_std + p_mean\n",
    "    test_posterior_samples_median = np.median(test_posterior_samples, axis=0)\n",
    "    # compute the log posterior of the test data\n",
    "    input_dict = {\n",
    "        'sim_data': np.repeat(test_sim, repeats=100, axis=0),\n",
    "        'parameters': test_posterior_samples\n",
    "    }\n",
    "\n",
    "    # save posterior samples to load for abc comparison (only for the best model)\n",
    "    #np.save(f'abc_results_{test_id}/posterior_samples_npe.npy', test_posterior_samples)\n",
    "\n",
    "    if not isinstance(trainer, EnsembleTrainer):\n",
    "        summary_output = trainer.amortizer.summary_net(trainer.configurator(test_sim_full)).numpy().flatten()\n",
    "        summary_pred = (coeff_matrix.T @ summary_output) * p_std + p_mean\n",
    "    else:\n",
    "        summary_pred = None\n",
    "\n",
    "    fig = plot_posterior_2d(posterior_draws=test_posterior_samples,\n",
    "                        prior_draws=prior_draws[:test_posterior_samples.shape[0]],\n",
    "                        param_names=log_param_names,\n",
    "                        reference_params=summary_pred,\n",
    "                        true_params=test_params)\n",
    "    plt.show()"
   ],
   "id": "aaf9d11a06bc442f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Real Data",
   "id": "dc88096666755040"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from load_data import load_real_data\n",
    "\n",
    "wasserstein_distance_dict = {0: np.nan, 1: np.nan}\n",
    "samples_dict = {0: np.nan, 1: np.nan}\n",
    "prior_draws = prior_fun(1000)"
   ],
   "id": "88f5e4f6b323ebcd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "_, real_data_full = load_real_data(data_id=1,\n",
    "                                   max_sequence_length=max_sequence_length,\n",
    "                                   cells_in_population=cells_in_population,\n",
    "                                   plot_data=True)\n",
    "print(real_data_full.shape)"
   ],
   "id": "748dffc6b156ef8e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Zelldichte macht was aus\n",
    "\n",
    "Wo wollen Zellen hin? welche parameter beeinflussen das ganze? Zelldichte?\n",
    "\n",
    "1,5mm\n",
    "\n",
    "1 (nicht so gut, extrema)\n",
    "739.79x279.74  microns\n",
    "20231x768 pixel\n",
    "\n",
    "\n",
    "2 (wesentlich mehr der Wahrheit)\n",
    "882.94x287.03 microns\n",
    "2424x788 pixel\n"
   ],
   "id": "3212d01370d344d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# batch the real data\n",
    "real_posterior_samples = trainer.amortizer.sample(trainer.configurator({'sim_data': real_data_full[np.newaxis]}),\n",
    "                                          n_samples=prior_draws.shape[0])\n",
    "real_posterior_samples = real_posterior_samples * p_std + p_mean\n",
    "\n",
    "# if isinstance(trainer, EnsembleTrainer):\n",
    "#     np.save(f'abc_results_real/posterior_samples_npe_ensemble.npy', real_posterior_samples)\n",
    "# else:\n",
    "#     np.save(f'abc_results_real/posterior_samples_npe.npy', real_posterior_samples)\n",
    "\n",
    "fig = plot_posterior_2d(posterior_draws=real_posterior_samples,\n",
    "                        prior_draws=prior_draws[:real_posterior_samples.shape[0]],\n",
    "                        param_names=log_param_names)\n",
    "#plt.savefig(f'{trainer.checkpoint_path}/full_posterior.png')\n",
    "plt.show()"
   ],
   "id": "17248751d4533848",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if isinstance(trainer, EnsembleTrainer):\n",
    "    log_posteriors_mean = trainer.amortizer.log_posterior_mean_diff(\n",
    "        trainer.configurator({'sim_data': real_data_full[np.newaxis]}),\n",
    "        n_samples=100\n",
    "    )\n",
    "\n",
    "    # heuristic to check model misspecification\n",
    "    print('Max KL Divergence:', np.max(log_posteriors_mean))\n",
    "    print(f'Much smaller than parameter dimension: {np.max(log_posteriors_mean) / n_params}')\n",
    "    print(log_posteriors_mean)"
   ],
   "id": "f7bff6cf098c63f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if trainer.amortizer.summary_loss is not None:\n",
    "    from matplotlib.cm import viridis\n",
    "    real_data_config = trainer.configurator({'sim_data': real_data_full[np.newaxis]})\n",
    "    summary_statistics = trainer.amortizer.summary_net(valid_data_config['summary_conditions'])\n",
    "    summary_statistics_obs = trainer.amortizer.summary_net(real_data_config['summary_conditions'])\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "    colors = viridis(np.linspace(0.1, 0.9, 2))\n",
    "    ax.scatter(\n",
    "        summary_statistics_obs[:, 0], summary_statistics_obs[:, 1], color=colors[0], label=r\"Observed: $h_{\\psi}(x_{obs})$\"\n",
    "    )\n",
    "    ax.scatter(summary_statistics[:, 0], summary_statistics[:, 1], color=colors[1], label=r\"Well-specified: $h_{\\psi}(x)$\")\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.2)\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "    \n",
    "    fig.savefig(f'abc_results_real/Real Summary Latent Space.pdf', bbox_inches='tight')\n",
    "    \n",
    "    MMD_sampling_distribution, MMD_observed = trainer.mmd_hypothesis_test(\n",
    "        observed_data=real_data_config, \n",
    "        reference_data=valid_data_config,  # if not provided, will use the generative model\n",
    "        num_null_samples=500,\n",
    "        bootstrap=True  # if True, use the reference data as null samples\n",
    "    )\n",
    "    fig = bf.diagnostics.plot_mmd_hypothesis_test(MMD_sampling_distribution, MMD_observed)\n",
    "    fig.savefig(f'abc_results_real/Real MMD.pdf', bbox_inches='tight')\n",
    "    plt.show()"
   ],
   "id": "d79424027ea8180a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Different Number of Cells in Experiment",
   "id": "1d4036aaff03efd9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from scipy.stats import linregress\n",
    "from scipy.stats import median_abs_deviation"
   ],
   "id": "80307fddc94faeb2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# batch the real data\n",
    "n_cell_in_batch = np.arange(1, len(real_data_full))\n",
    "stats_n_cells = {}\n",
    "np.random.seed(0)\n",
    "\n",
    "real_posterior_samples_partial = []\n",
    "real_posterior_samples_artificial = []\n",
    "for n_cells in tqdm(n_cell_in_batch):\n",
    "    n_rand_index = np.random.choice(len(real_data_full), size=n_cells, replace=False)\n",
    "    partial_data = real_data_full[n_rand_index]\n",
    "\n",
    "    real_posterior_samples = trainer.amortizer.sample(trainer.configurator({'sim_data': partial_data[np.newaxis]}),\n",
    "                                                      n_samples=100)\n",
    "    real_posterior_samples = real_posterior_samples * p_std + p_mean\n",
    "    real_posterior_samples_partial.append(real_posterior_samples)\n",
    "\n",
    "real_posterior_samples_partial = np.stack(real_posterior_samples_partial)"
   ],
   "id": "2c4b9f4d313543e6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "real_posterior_samples_partial_median = np.median(real_posterior_samples_partial, axis=1)\n",
    "real_posterior_samples_partial_std = median_abs_deviation(real_posterior_samples_partial, axis=1)"
   ],
   "id": "b0e3b2b8aa6196c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(10, 2.5), layout='constrained')\n",
    "\n",
    "for i, p_name in enumerate(log_param_names):\n",
    "    reg = linregress(n_cell_in_batch, real_posterior_samples_partial_median[:, i])\n",
    "\n",
    "    ax[i].errorbar(n_cell_in_batch,  real_posterior_samples_partial_median[:, i],\n",
    "                  yerr=real_posterior_samples_partial_std[:, i],\n",
    "                  fmt='o',\n",
    "                  alpha=0.5, label='Estimate (median $\\pm$ absolute deviation)' if i == 0 else None)\n",
    "\n",
    "    reg = linregress(n_cell_in_batch, real_posterior_samples_partial_median[:, i])\n",
    "    #if reg.pvalue < 0.01:\n",
    "    ax[i].plot(n_cell_in_batch, reg.slope*n_cell_in_batch + reg.intercept,\n",
    "               color='black', label=f'Regression Line' if i == 1 else None, zorder=4)\n",
    "\n",
    "    #ax[i].set_title(f\"Correlation {reg.rvalue:.4f}\\n(p-value: {reg.pvalue:.4f})\")\n",
    "    ax[i].set_xlabel('Number of Cells')\n",
    "    ax[i].set_title(f'{log_param_names[i]}')\n",
    "    ax[i].set_ylim(limits_log[list(limits_log.keys())[i]])\n",
    "\n",
    "fig.legend(loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.15))\n",
    "plt.savefig(f'abc_results_real/real_ncells_vs_parameter_median.pdf', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "id": "b09bb5e7f54affc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "88d703598d0f0e1a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
