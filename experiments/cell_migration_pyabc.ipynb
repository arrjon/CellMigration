{ "cells": [  {   "metadata": {},   "cell_type": "markdown",   "source": "# Inference with Approximate Bayesian Computation (ABC)",   "id": "2885213ace1b9d2c"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "import argparse\n",    "import os\n",    "import pickle\n",    "from functools import partial\n",    "from typing import Union\n",    "\n",    "import matplotlib.pyplot as plt\n",    "import pandas as pd\n",    "from joblib import Parallel, delayed\n",    "import numpy as np\n",    "import pyabc\n",    "#from pyabc.sampler import RedisEvalParallelSampler\n",    "import scipy.stats as stats\n",    "import umap\n",    "from sklearn.metrics.pairwise import cosine_similarity\n",    "from fitmulticell import model as morpheus_model\n",    "from fitmulticell.sumstat import SummaryStatistics\n",    "\n",    "from load_bayesflow_model import load_model, EnsembleTrainer\n",    "from plotting_routines import sampling_parameter_cis, plot_posterior_1d, plot_sumstats_distance_stats\n",    "from summary_stats import compute_summary_stats, reduce_to_coordinates, span, euclidean_distance"   ],   "id": "3ce04deed61e1ae4",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# get the job array id and number of processors\n",    "test_id = 1 #int(os.environ.get('SLURM_ARRAY_TASK_ID', 0))\n",    "n_procs = 10 # int(os.environ.get('SLURM_CPUS_PER_TASK', 1))\n",    "print('test_id', test_id)\n",    "on_cluster = False"   ],   "id": "81c3fdf26f2b28f1",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "population_size = 1000\n",    "\n",    "if on_cluster:\n",    "    parser = argparse.ArgumentParser(description='Parse necessary arguments')\n",    "    parser.add_argument('-pt', '--port', type=str, default=\"50004\",\n",    "                        help='Which port should be use?')\n",    "    parser.add_argument('-ip', '--ip', type=str,\n",    "                        help='Dynamically passed - BW: Login Node 3')\n",    "    args = parser.parse_args()"   ],   "id": "abbff289ceca71a5",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "if on_cluster:\n",    "    gp = '/home/jarruda_hpc/CellMigration/synth_data_params_bayesflow'\n",    "else:\n",    "    gp = os.getcwd()\n",    "\n",    "par_map = {\n",    "    'gradient_strength': './CellTypes/CellType/Constant[@symbol=\"gradient_strength\"]',\n",    "    'move.strength': './CellTypes/CellType/Constant[@symbol=\"move.strength\"]',\n",    "    'move.duration.rate': './CellTypes/CellType/Constant[@symbol=\"move.duration.rate\"]',\n",    "    'cell_nodes_real': './Global/Constant[@symbol=\"cell_nodes_real\"]',\n",    "}\n",    "\n",    "dt = 30\n",    "model_path = gp + \"/cell_movement_v24.xml\"  # time step is 30sec, for inference\n",    "\n",    "# defining the summary statistics function\n",    "min_sequence_length = 0\n",    "max_sequence_length = 3600 // dt\n",    "only_longest_traj_per_cell = True  # mainly to keep the data batchable\n",    "cells_in_population = 143\n",    "\n",    "def make_sumstat_dict(data: Union[dict, np.ndarray], return_pred=False) -> dict:\n",    "    if isinstance(data, dict):\n",    "        # get key\n",    "        key = list(data.keys())[0]\n",    "        data = data[key]\n",    "    data = data[0]  # only one full simulation\n",    "    assert data.ndim == 3\n",    "    # compute the summary statistics\n",    "    msd_list, ta_list, v_list, ad_list = compute_summary_stats(data, dt=dt)\n",    "    cleaned_dict = {\n",    "        'msd': np.array(msd_list).flatten(),\n",    "        'ta': np.array(ta_list).flatten(),\n",    "        'vel': np.array(v_list).flatten(),\n",    "        'ad': np.array(ad_list).flatten(),\n",    "    }\n",    "    if return_pred:\n",    "        # compute the prediction of the regression model\n",    "        coeff_matrix = np.array([[-0.,           0.16171273, -0.,         -0.05603923],\n",    "                                 [-0.,         -0.,          0.31892324, -0.        ],\n",    "                                 [-0.2107947,  -0.10119028, -0.,          0.        ],\n",    "                                 [-0.44803216, -0.,         -0.0584819,   0.31803382],\n",    "                                 [-0.07227701,  0.,          0.,         -0.52875216],\n",    "                                 [-0.,          0.422123,   -0.02283603,  0.        ],\n",    "                                 [ 0.30511962,  0.,          0.,          0.        ],\n",    "                                 [-0.38812312,  0.0425146,  -0.00355393,  0.        ]])\n",    "        cleaned_dict['summary_pred'] = (coeff_matrix.T @ np.array(list(cleaned_dict.values())).flatten()) * \\\n",    "                                       np.std(list(cleaned_dict.values())) + \\\n",    "                                       np.mean(list(cleaned_dict.values()))\n",    "    return cleaned_dict\n",    "\n",    "\n",    "def prepare_sumstats(output_morpheus_model) -> dict:\n",    "    sim_coordinates = reduce_to_coordinates(output_morpheus_model, \n",    "                          minimal_length=min_sequence_length, \n",    "                          maximal_length=max_sequence_length,\n",    "                          only_longest_traj_per_cell=only_longest_traj_per_cell,\n",    "                          )\n",    "    \n",    "    # we now do exactly the same as in the BayesFlow workflow, but here we get only one sample at a time\n",    "    data_transformed = np.ones((1, cells_in_population, max_sequence_length, 3)) * np.nan\n",    "    # each cell is of different length, each with x and y coordinates, make a tensor out of it\n",    "    if len(sim_coordinates) != 0:\n",    "        # some cells were visible in the simulation\n",    "        for c_id, cell_sim in enumerate(sim_coordinates):\n",    "            # pre-pad the data with zeros, but first write zeros as nans to compute the mean and std\n",    "            data_transformed[0, c_id, -len(cell_sim['x']):, 0] = cell_sim['x']\n",    "            data_transformed[0, c_id, -len(cell_sim['y']):, 1] = cell_sim['y']\n",    "            data_transformed[0, c_id, -len(cell_sim['t']):, 2] = cell_sim['t']\n",    "    \n",    "    return {'sim': data_transformed}\n",    "\n",    "\n",    "sumstat = SummaryStatistics(sum_stat_calculator=prepare_sumstats)                    \n",    "\n",    "if on_cluster:\n",    "    # define the model object\n",    "    model = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)\n",    "\n",    "    # note: remember also change tiff path in model.xml!\n",    "else:\n",    "    # define the model object\n",    "    model = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)\n",    "\n",    "# parameter values used to generate the synthetic data\n",    "obs_pars = {\n",    "    'gradient_strength': 100.,  # strength of the gradient of chemotaxis\n",    "    'move.strength': 10.,  # strength of directed motion\n",    "    'move.duration.rate': 0.1,  # rate of exponential distribution (1/seconds)\n",    "    'cell_nodes_real': 50.,  # area of the cell (\\mu m^2), macrophages have a volume of 4990\\mu m^3 -> radius of 17 if they would are sphere\n",    "}\n",    "\n",    "\n",    "obs_pars_log = {key: np.log10(val) for key, val in obs_pars.items()}\n",    "limits = {'gradient_strength': (1, 10000), #(10 ** 4, 10 ** 8),\n",    "          'move.strength': (1, 100),\n",    "          'move.duration.rate': (1e-4, 30), #(math.log10((10 ** -2) * 30), math.log10((10 ** 4))), # smallest time step in simulation 5\n",    "          'cell_nodes_real': (1, 300)}\n",    "limits_log = {key: (np.log10(val[0]), np.log10(val[1])) for key, val in limits.items()}\n",    "\n",    "\n",    "prior = pyabc.Distribution(**{key: pyabc.RV(\"uniform\", loc=lb, scale=ub-lb)\n",    "                              for key, (lb, ub) in limits_log.items()})\n",    "\n",    "param_names = ['$m_{\\\\text{dir}}$', '$m_{\\\\text{rand}}$', '$\\lambda$', '$a$']\n",    "log_param_names = ['$\\log_{10}(m_{\\\\text{dir}})$', '$\\log_{10}(m_{\\\\text{rand}})$',\n",    "                   '$\\log_{10}(\\lambda)$', '$\\log_{10}(a)$']\n",    "print(obs_pars)\n",    "print(limits_log)"   ],   "id": "74390dfb0e802a77",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "sigma0 = 550\n",    "space_x0 = 1173/2\n",    "space_y0 = 1500/1.31/2\n",    "x0, y0 = 1173/2, (1500+1500/2+270)/1.31\n",    "u1 = lambda space_x, space_y: 7/(2*np.pi*(sigma0**2)) *np.exp(-1/2*(((space_x)-(x0))**2+ ((space_y)-(y0))**2)/(sigma0**2))\n",    "\n",    "# plot the function\n",    "fig = plt.figure()\n",    "ax = fig.add_subplot(111, projection='3d')\n",    "x = np.linspace(0, 1173 , 100)\n",    "y = np.linspace(0, 2500 , 100)\n",    "X, Y = np.meshgrid(x, y)\n",    "Z = u1(X, Y)\n",    "ax.plot_surface(X, Y, Z, alpha=0.5)\n",    "# plot start points\n",    "ax.scatter(space_x0, space_y0, u1(space_x0, space_y0), color='r', s=100)\n",    "\n",    "ax.set_xlabel('space_x')\n",    "ax.set_ylabel('space_y')\n",    "plt.show()\n",    "\n",    "space_x0, space_y0, u1(space_x0, space_y0), u1(x0, y0)"   ],   "id": "be4eaa8d21c3bfff",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# load test data\n",    "np.random.seed(42+test_id)\n",    "test_params = np.array(list(prior.rvs().values()))\n",    "if not os.path.exists(os.path.join(gp, f'test_sim_{test_id}.npy')):\n",    "    raise FileNotFoundError('Test data not found')\n",    "else:\n",    "    test_sim = np.load(os.path.join(gp, f'test_sim_{test_id}.npy'))\n",    "    test_sim_full = {'sim_data': test_sim}\n",    "results_path = f'abc_results_{test_id}'\n",    "test_sim.shape"   ],   "id": "b90db5d16d7fa2b",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": "prior_draws = np.array([list(prior.rvs().values()) for _ in range(1000)])",   "id": "ab3a9bf0fcadc7d2",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "time_step = 10\n",    "time_points = np.arange(10, test_sim.shape[2]+1, step=time_step)\n",    "statistics_time = []\n",    "for t in time_points:\n",    "    msd_list, ta_list, v_list, ad_list = compute_summary_stats(test_sim[0, :, :t], dt=dt)\n",    "    d = np.mean(msd_list), np.std(msd_list)\n",    "    ta = np.mean(ta_list), np.std(ta_list)\n",    "    v = np.mean(v_list), np.std(v_list)\n",    "    ad = np.mean(ad_list), np.std(ad_list)\n",    "    statistics_time.append((d, ta, v, ad))\n",    "\n",    "# plot statistics over time\n",    "fig, ax = plt.subplots(1, 4, layout='constrained', figsize=(10, 1.5))\n",    "for i in range(4):\n",    "    ax[i].errorbar(time_points*dt/60, [s[i][0] for s in statistics_time],\n",    "                   yerr=[s[i][1] for s in statistics_time], fmt='-o')\n",    "    if i == 0:\n",    "        ax[i].set_ylabel(r'Displacement')\n",    "    elif i == 1:\n",    "        ax[i].set_ylabel(r'Turning Angle')\n",    "    elif i == 2:\n",    "        ax[i].set_ylabel(r'Velocity')\n",    "    elif i == 3:\n",    "        ax[i].set_ylabel(r'Angle Degree')\n",    "    ax[i].set_xlabel('Time (min)')\n",    "plt.savefig(os.path.join(gp, f'{results_path}/test_sim_statistics_time_{test_id}.pdf'))\n",    "plt.show()"   ],   "id": "42a255b1ffe209f6",   "outputs": [],   "execution_count": null  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-01-07T10:49:32.834405Z",     "start_time": "2025-01-07T10:49:32.246623Z"    }   },   "cell_type": "markdown",   "source": "## ABC with Wasserstein distance",   "id": "1145158513a20ad6"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "def obj_func_wass_helper(sim: dict, obs: dict, key: str) -> float:\n",    "    x, y = np.array(sim[key]), np.array(obs[key])\n",    "    if x.size == 0 or y.size == 0:\n",    "        return np.inf\n",    "    return stats.wasserstein_distance(x, y)\n",    "\n",    "distances = {\n",    "    'msd': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='msd')),\n",    "    'ta': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='ta')),\n",    "    'vel': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='vel')),\n",    "    'ad': pyabc.distance.FunctionDistance(partial(obj_func_wass_helper, key='ad')),\n",    "}\n",    "\n",    "# adaptive distance\n",    "log_file_weights = f\"{results_path}/adaptive_distance_log_{test_id}.txt\"\n",    "adaptive_wasserstein_distance = pyabc.distance.AdaptiveAggregatedDistance(\n",    "    distances=list(distances.values()),\n",    "    scale_function=span,\n",    "    adaptive=False,  # only pre-calibration\n",    "    log_file=log_file_weights\n",    ")"   ],   "id": "1636e7efc136f2e0",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "#redis_sampler = RedisEvalParallelSampler(host=args.ip, port=args.port,\n",    "#                                         adapt_look_ahead_proposal=False,\n",    "#                                         look_ahead=False)\n",    "\n",    "abc = pyabc.ABCSMC(model, prior,\n",    "                   distance_function=adaptive_wasserstein_distance,\n",    "                   summary_statistics=make_sumstat_dict,\n",    "                   population_size=population_size,\n",    "                   sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",    "                   #sampler=redis_sampler\n",    "                   )\n",    "\n",    "db_path = os.path.join(gp, f\"{results_path}/synthetic_{test_id}_test_wasserstein_sumstats.db\")\n",    "if not os.path.exists(db_path):\n",    "    history_abc = abc.new(\"sqlite:///\" + db_path, make_sumstat_dict(test_sim))\n",    "\n",    "    # start the abc fitting\n",    "    abc.run(min_acceptance_rate=1e-2, max_nr_populations=15)\n",    "    print('Done!')\n",    "else:\n",    "    history_abc = abc.load(\"sqlite:///\" + db_path)\n",    "    if len(history_abc.all_runs()) > 1:\n",    "        history_abc = abc.load(\"sqlite:///\" + db_path, abc_id=len(history_abc.all_runs()))\n",    "adaptive_weights = list(pyabc.storage.load_dict_from_json(log_file_weights).values())[-1]"   ],   "id": "efcbb86db464a88d",   "outputs": [],   "execution_count": null  },  {   "metadata": {    "ExecuteTime": {     "end_time": "2025-01-07T10:49:48.876700Z",     "start_time": "2025-01-07T10:49:35.742219Z"    }   },   "cell_type": "markdown",   "source": "## ABC with neural network summary statistics",   "id": "df1829ce49462436"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "if os.path.exists(os.path.join(gp, 'validation_data.pickle')):\n",    "    with open(os.path.join(gp, 'validation_data.pickle'), 'rb') as f:\n",    "        valid_data = pickle.load(f)\n",    "else:\n",    "    raise FileNotFoundError('Validation data not found')\n",    "\n",    "x_mean = np.nanmean(valid_data['sim_data'], axis=(0, 1, 2))\n",    "x_std = np.nanstd(valid_data['sim_data'], axis=(0, 1, 2))\n",    "p_mean = np.mean(valid_data['prior_draws'], axis=0)\n",    "p_std = np.std(valid_data['prior_draws'], axis=0)\n",    "print('Mean and std of data:', x_mean, x_std)\n",    "print('Mean and std of parameters:', p_mean, p_std)"   ],   "id": "a7c0835963566e42",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# use trained neural net as summary statistics\n",    "def make_sumstat_dict_nn(data: Union[dict, np.ndarray], use_npe_summaries: bool = True, return_reduced: bool = False\n",    "                         ) -> dict:\n",    "    if use_npe_summaries:\n",    "        model_id = 0\n",    "    else:\n",    "        model_id = 10\n",    "    if isinstance(data, dict):\n",    "        # get key\n",    "        key = list(data.keys())[0]\n",    "        data = data[key]\n",    "\n",    "    trainer = load_model(\n",    "        model_id=model_id,\n",    "        x_mean=x_mean,\n",    "        x_std=x_std,\n",    "        p_mean=p_mean,\n",    "        p_std=p_std,\n",    "    )\n",    "\n",    "    # configures the input for the network\n",    "    config_input = trainer.configurator({\"sim_data\": data})\n",    "    # get the summary statistics\n",    "    if isinstance(trainer, EnsembleTrainer):\n",    "        out_dict = {\n",    "            'summary_net': trainer.amortizer.summary_net(config_input).flatten()\n",    "        }\n",    "    else:\n",    "        out_dict = {\n",    "            'summary_net': trainer.amortizer.summary_net(config_input['summary_conditions']).numpy().flatten()\n",    "        }\n",    "    if return_reduced:\n",    "        # coeff matrix from regression model\n",    "        coeff_matrix = np.array([[-0.,           0.16171273, -0.,         -0.05603923],\n",    "                         [-0.,         -0.,          0.31892324, -0.        ],\n",    "                         [-0.2107947,  -0.10119028, -0.,          0.        ],\n",    "                         [-0.44803216, -0.,         -0.0584819,   0.31803382],\n",    "                         [-0.07227701,  0.,          0.,         -0.52875216],\n",    "                         [-0.,          0.422123,   -0.02283603,  0.        ],\n",    "                         [ 0.30511962,  0.,          0.,          0.        ],\n",    "                         [-0.38812312,  0.0425146,  -0.00355393,  0.        ]])\n",    "\n",    "        out_dict['summary_pred'] = (coeff_matrix.T @ out_dict['summary_net']) * p_std + p_mean\n",    "    if model_id == 10:\n",    "        # renormalize the parameters\n",    "        out_dict['summary_net'] = out_dict['summary_net'] * p_std + p_mean\n",    "\n",    "    del trainer\n",    "    return out_dict\n",    "\n",    "\n",    "if on_cluster:\n",    "    # define the model object\n",    "    model_nn = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        executable=\"ulimit -s unlimited; /home/jarruda_hpc/CellMigration/morpheus-2.3.7\",\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)\n",    "\n",    "    # note: remember also change tiff path in model.xml!\n",    "else:\n",    "    # define the model object\n",    "    model_nn = morpheus_model.MorpheusModel(\n",    "        model_path, par_map=par_map, par_scale=\"log10\",\n",    "        show_stdout=False, show_stderr=False,\n",    "        clean_simulation=True,\n",    "        raise_on_error=False, sumstat=sumstat)"   ],   "id": "bf7b71404095f91e",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# %%time\n",    "# print(make_sumstat_dict_nn(test_sim), test_params)\n",    "#\n",    "# p = 1\n",    "# summary_error = (np.abs(make_sumstat_dict_nn(test_sim, use_npe_summaries=False)['summary_net']-test_params)**p).sum() ** (1 / p)\n",    "# print(make_sumstat_dict_nn(test_sim, use_npe_summaries=False), test_params)\n",    "# print('error:', summary_error)"   ],   "id": "8522eceb33f5a581",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# abc with summary net trained on posterior mean\n",    "abc_nn = pyabc.ABCSMC(model_nn, prior, # here we use now the Euclidean distance, Wasserstein distance is not possible\n",    "                      population_size=population_size,\n",    "                      summary_statistics=partial(make_sumstat_dict_nn, use_npe_summaries=False),\n",    "                      sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",    "                      #sampler=redis_sampler\n",    "                      )\n",    "\n",    "db_path = os.path.join(gp, f\"{results_path}/synthetic_{test_id}_test_nn_sumstats_posterior_mean.db\")\n",    "\n",    "if not os.path.exists(db_path):\n",    "    history_nn = abc_nn.new(\"sqlite:///\" + db_path, make_sumstat_dict_nn(test_sim))\n",    "\n",    "    # start the abc fitting\n",    "    abc_nn.run(min_acceptance_rate=1e-2, max_nr_populations=15)\n",    "    print('Done!')\n",    "else:\n",    "    history_nn = abc_nn.load(\"sqlite:///\" + db_path)\n",    "    if len(history_nn.all_runs()) > 1:\n",    "        history_nn = abc_nn.load(\"sqlite:///\" + db_path, abc_id=len(history_nn.all_runs()))"   ],   "id": "82568acc7945ee86",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "#redis_sampler = RedisEvalParallelSampler(host=args.ip, port=args.port,\n",    "#                                         adapt_look_ahead_proposal=False,\n",    "#                                         look_ahead=False)\n",    "\n",    "# abc with summary net trained with NPE\n",    "abc_npe = pyabc.ABCSMC(model_nn, prior, # here we use now the Euclidean distance, Wasserstein distance is not possible\n",    "                      population_size=population_size,\n",    "                      summary_statistics=partial(make_sumstat_dict_nn, use_npe_summaries=True),\n",    "                      sampler=pyabc.sampler.MulticoreEvalParallelSampler(n_procs=n_procs)\n",    "                      #sampler=redis_sampler\n",    "                      )\n",    "\n",    "db_path = os.path.join(gp, f\"{results_path}/synthetic_{test_id}_test_nn_sumstats.db\")\n",    "if not os.path.exists(db_path):\n",    "    history_npe = abc_npe.new(\"sqlite:///\" + db_path,\n",    "                              make_sumstat_dict_nn(test_sim, use_npe_summaries=True))\n",    "\n",    "    # start the abc fitting\n",    "    abc_npe.run(min_acceptance_rate=1e-2, max_nr_populations=15)\n",    "    print('Done!')\n",    "else:\n",    "    history_npe = abc_npe.load(\"sqlite:///\" + db_path)\n",    "    if len(history_npe.all_runs()) > 1:\n",    "        # first run failed\n",    "        history_npe = abc_npe.load(\"sqlite:///\" + db_path, abc_id=len(history_npe.all_runs()))"   ],   "id": "29a244aeacb895ad",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "for hist, name in zip([history_abc, history_nn, history_npe], ['abc', 'abc_mean', 'abc_npe']):\n",    "    if hist is None:\n",    "        continue\n",    "    diff_time = hist.get_all_populations().population_end_time - hist.get_abc().start_time\n",    "    total_time = [diff.total_seconds() for diff in diff_time][-1] / 60 / 60\n",    "    print(name,\n",    "          'Generations:', len(hist.get_all_populations()['t']),  # including precalibration\n",    "          'Samples:', hist.get_all_populations()['samples'].sum(),\n",    "          'Time (h):', np.round(total_time, 2))\n",    "    fig, ax = plt.subplots(1, len(param_names), layout='constrained', figsize=(10, 2))\n",    "    for i, param in enumerate(limits.keys()):\n",    "        for t in range(hist.max_t + 1):\n",    "            df, w = hist.get_distribution(m=0, t=t)\n",    "            if 'move.duration.mean' in df.columns:\n",    "                df['move.duration.rate'] = df['move.duration.mean']  # there was mistake in the name\n",    "            pyabc.visualization.plot_kde_1d(\n",    "                df,\n",    "                w,\n",    "                xmin=limits_log[param][0],\n",    "                xmax=limits_log[param][1],\n",    "                x=param,\n",    "                xname=log_param_names[i],\n",    "                ax=ax[i],\n",    "                label=f\"PDF t={t}\",\n",    "            )\n",    "        ax[i].set_xlim((limits_log[param][0]-0.2, limits_log[param][1]+0.2))\n",    "    fig.set_constrained_layout(True)\n",    "    plt.savefig(os.path.join(gp, f'{results_path}/synthetic_{test_id}_population_kdes_{name}.pdf'), bbox_inches='tight')\n",    "    plt.show()\n",    "\n",    "    fig, arr_ax = plt.subplots(1, 5, figsize=(10, 2.5), layout='constrained')\n",    "    arr_ax = arr_ax.flatten()\n",    "    pyabc.visualization.plot_sample_numbers(hist, ax=arr_ax[0])\n",    "    arr_ax[0].get_legend().remove()\n",    "    pyabc.visualization.plot_walltime(hist, ax=arr_ax[1], unit='h')\n",    "    arr_ax[1].get_legend().remove()\n",    "    pyabc.visualization.plot_epsilons(hist, ax=arr_ax[2])\n",    "    pyabc.visualization.plot_effective_sample_sizes(hist, ax=arr_ax[3])\n",    "    pyabc.visualization.plot_acceptance_rates_trajectory(hist, ax=arr_ax[4])\n",    "    fig.set_constrained_layout(True)\n",    "    plt.savefig(os.path.join(gp, f'{results_path}/synthetic_{test_id}_diagnostics_{name}.pdf'), bbox_inches='tight')\n",    "    plt.show()"   ],   "id": "b7d320f9bb9b7e8c",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# fig, ax = plt.subplots(3, len(param_names), sharey='col', sharex='col', layout='constrained', figsize=(10, 3))\n",    "# for i, param in enumerate(limits.keys()):\n",    "#     j = 0\n",    "#     for t in range(-1, hist.max_t + 1):\n",    "#         df, w = hist.get_distribution(m=0, t=t)\n",    "#         if not t in [-1, 6, 14]:\n",    "#             continue\n",    "#         if 'move.duration.mean' in df.columns:\n",    "#             df['move.duration.rate'] = df['move.duration.mean']  # there was mistake in the name\n",    "#         if t == -1:\n",    "#             # add prior\n",    "#             df_prior = prior.rvs(size=1000)\n",    "#             df = pd.DataFrame(df_prior)\n",    "#             w = np.ones(len(df)) / len(df)\n",    "#         artists = pyabc.visualization.plot_kde_1d(\n",    "#             df,\n",    "#             w,\n",    "#             xmin=limits_log[param][0],\n",    "#             xmax=limits_log[param][1],\n",    "#             x=param,\n",    "#             xname=log_param_names[i],\n",    "#             ax=ax[j, i],\n",    "#             label=f\"PDF t={t}\",\n",    "#         )\n",    "#         line = ax[j, i].get_lines()[-1]\n",    "#         line.set_color('#674E85')\n",    "#         #line.set_linewidth(2)\n",    "#         ax[j, i].set_xlim((limits_log[param][0]-0.2, limits_log[param][1]+0.2))\n",    "#         if t != 14:\n",    "#             ax[j, i].set_xlabel('')\n",    "#         if t == -1:\n",    "#             #ax[j, i].set_title(f'Prior')\n",    "#             ax[j, i].set_ylabel(\"Prior\")\n",    "#         elif t == 14:\n",    "#             ax[j, i].set_ylabel(f'Final\\nPosterior')\n",    "#         else:\n",    "#             ax[j, i].set_ylabel(f'Posterior\\n{t+1}. Gen.')\n",    "#         if i != 0:\n",    "#             ax[j, i].set_ylabel(\"\")\n",    "#         j += 1\n",    "# for a in ax.flatten():\n",    "#     # set font size\n",    "#     a.tick_params(labelsize=10)\n",    "#     a.xaxis.label.set_size(12)\n",    "#     a.yaxis.label.set_size(12)\n",    "# fig.set_constrained_layout(True)\n",    "# plt.savefig(os.path.join(gp, f'{results_path}/synthetic_{test_id}_population_kdes_{name}_list.pdf'), bbox_inches='tight')\n",    "# plt.show()"   ],   "id": "6f6d275cd044b5ae",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "markdown",   "source": "# Synthetic Tests",   "id": "287e3fde6457f51d"  },  {   "metadata": {},   "cell_type": "markdown",   "source": "## Compare Posterior Samples",   "id": "e3b932da8cac0311"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# get posterior samples\n",    "posterior_samples = {}\n",    "for hist, name in zip([history_abc, history_nn, history_npe], ['abc', 'abc_mean', 'abc_npe']):\n",    "    if hist is None:\n",    "        continue\n",    "    abc_df, abc_w = hist.get_distribution()\n",    "    if 'move.duration.mean' in abc_df.columns:\n",    "        abc_df['move.duration.rate'] = abc_df['move.duration.mean']  # there was mistake in the name\n",    "    posterior_samples[name] = pyabc.resample(abc_df[limits.keys()].values, abc_w, n=1000)\n",    "\n",    "# add bayesflow posterior samples\n",    "posterior_samples['npe'] = np.load(f'abc_results_{test_id}/posterior_samples_npe.npy')\n",    "\n",    "# pickle samples\n",    "#with open(f'posterior_samples_{test_id}.pickle', 'wb') as f:\n",    "#        pickle.dump(posterior_samples, f)"   ],   "id": "488d22682851a587",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "markdown",   "source": "",   "id": "9c2c91341abdcd52"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "labels_colors = {\n",    "    'abc': ('ABC hand-crafted summaries', '#9AB8D7', 'ABC'),\n",    "    'abc_mean': ('ABC posterior mean summaries', '#C4B7D4', 'ABC-PM'),\n",    "    'abc_npe': ('ABC inference-tailored summaries', '#EEBC88', 'ABC-NPE'),\n",    "    'npe': ('NPE jointly learned summaries', '#A7CE97', 'NPE')\n",    "}\n",    "\n",    "colors = [labels_colors[name][1] for name in posterior_samples.keys()]\n",    "labels = [labels_colors[name][0] for name in posterior_samples.keys()]"   ],   "id": "4f919a570c48e60d",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "fig = plot_posterior_1d(\n",    "    posterior_samples=posterior_samples,\n",    "    prior_draws=prior_draws,\n",    "    log_param_names=log_param_names,\n",    "    test_sim=test_sim,\n",    "    test_params=test_params,\n",    "    labels_colors=labels_colors,\n",    "    make_sumstat_dict_nn=make_sumstat_dict_nn,\n",    "    save_path=os.path.join(gp, f'{results_path}/synthetic_posterior_all_rows.pdf')\n",    ")"   ],   "id": "fb7c8bdd7244f0a7",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "prior_mean = np.mean(prior_draws, axis=0)\n",    "prior_std = np.std(prior_draws, axis=0)\n",    "\n",    "def compute_z_score(posterior_mean):\n",    "    return (posterior_mean - prior_mean) / prior_std\n",    "\n",    "def compute_contraction(posterior_std):\n",    "    return 1. - (posterior_std / prior_std)\n",    "\n",    "posterior_stats = {}\n",    "for name, ps in posterior_samples.items():\n",    "    posterior_stats[name] = {\n",    "        'mean': np.mean(ps, axis=0),\n",    "        'std': np.std(ps, axis=0),\n",    "        'median': np.median(ps, axis=0),\n",    "        'z_score': compute_z_score(np.mean(ps, axis=0)),\n",    "        'contraction': compute_contraction(np.std(ps, axis=0))\n",    "    }"   ],   "id": "87b27ca1a2a0f549",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "z_scores = [posterior_stats[name]['z_score'] for name in posterior_samples.keys()]\n",    "contractions = [posterior_stats[name]['contraction'] for name in posterior_samples.keys()]\n",    "\n",    "# Plotting Z-scores and contractions for both methods\n",    "fig, ax1 = plt.subplots(figsize=(8, 4), tight_layout=True)\n",    "\n",    "# Z-Scores for both methods\n",    "for i, z in enumerate(z_scores):\n",    "    ax1.bar(np.arange(len(param_names)) + 0.2 * i, z, width=0.15, align='center', color=colors[i],\n",    "            label=labels[i])\n",    "ax1.set_ylabel('Z-Score')\n",    "ax1.tick_params(axis='y')\n",    "ax1.set_xticks(np.arange(len(param_names)) + 0.3)\n",    "ax1.set_xticklabels(log_param_names)\n",    "ax1.axhline(0, color='gray', linestyle='--', linewidth=0.8)\n",    "ax1.grid()\n",    "\n",    "# Plot Contractions for both methods on secondary axis\n",    "ax2 = ax1.twinx()\n",    "for i, c in enumerate(contractions):\n",    "    ax2.plot(np.arange(0.3, len(param_names)), c, color=colors[i], marker='o', linestyle='--')\n",    "ax2.set_ylabel('--●--  Contraction')\n",    "ax2.tick_params(axis='y')\n",    "# get maximal y-limit of ax1\n",    "max_y = max(ax1.get_ylim()[1], ax2.get_ylim()[1])\n",    "min_y = min(ax1.get_ylim()[0], ax2.get_ylim()[0])\n",    "ax1.set_ylim(min_y, max_y)\n",    "ax2.set_ylim(min_y, max_y)\n",    "\n",    "# Combine legends\n",    "handles1, labels1 = ax1.get_legend_handles_labels()\n",    "fig.legend(handles1, labels1,\n",    "           loc='lower center', bbox_to_anchor=(0.5, -0.12), ncol=2)\n",    "fig.savefig(os.path.join(gp, f'{results_path}/synthetic_z_scores_contraction_{test_id}.pdf'), bbox_inches='tight')\n",    "plt.show()"   ],   "id": "c851aaf76d152b11",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "#ordering = [0,4,1,5,2,6,3,7]\n",    "ordering = np.concatenate([[i,i+4, i+8, i+12] for i in range(4)])\n",    "all_params = np.concatenate((posterior_samples['abc'],\n",    "                             posterior_samples['abc_mean'],\n",    "                             posterior_samples['abc_npe'],\n",    "                             posterior_samples['npe']), axis=-1)\n",    "log_param_names_plot = np.array(\n",    "    [f'{n} $\\qquad$ ABC hand-crafted' for n in log_param_names] +\n",    "    [f'ABC posterior mean' for n in log_param_names] +\n",    "    [f'ABC inference tailored' for n in log_param_names] +\n",    "    [f'NPE' for n in log_param_names]\n",    ")[ordering]\n",    "param_names_plot = np.array(\n",    "    [f'{n} $\\qquad$ ABC hand-crafted' for n in param_names] +\n",    "    [f'ABC posterior mean' for n in param_names] +\n",    "    [f'ABC inference tailored' for n in param_names] +\n",    "    [f'NPE' for n in param_names]\n",    ")[ordering]\n",    "color_list = colors*len(param_names)\n",    "\n",    "ax = sampling_parameter_cis(\n",    "    all_params[:, ordering],\n",    "    true_param=np.concatenate((test_params, test_params, test_params, test_params))[ordering] if test_params is not None else None,\n",    "    prior_bounds=limits_log.values() if test_params is not None else None,\n",    "    param_names=log_param_names_plot,\n",    "    alpha=[95, 90 , 80],\n",    "    color_list=color_list,\n",    "    show_median=False if test_params is not None else True,\n",    "    size=(7, 4),\n",    "    legend_bbox_to_anchor=(0.45,1) if test_params is not None else (0.31,1)\n",    ")\n",    "plt.savefig(os.path.join(gp, f'{results_path}/synthetic_posterior_credible_intervals_log_{test_id}.pdf'))\n",    "plt.show()\n",    "\n",    "all_params = np.power(10, all_params)\n",    "ax = sampling_parameter_cis(\n",    "    all_params[:, ordering],\n",    "    true_param=np.power(10, np.concatenate((test_params, test_params, test_params, test_params))[ordering]) if test_params is not None else None,\n",    "    param_names=param_names_plot,\n",    "    alpha=[95, 90 , 80],\n",    "    color_list=color_list,\n",    "    show_median=False if test_params is not None else True,\n",    "    size=(7, 4),\n",    "    legend_bbox_to_anchor=(0.99,0.35)\n",    ")\n",    "#plt.savefig(os.path.join(gp, f'{results_path}/synthetic_posterior_credible_intervals.pdf'))\n",    "plt.show()"   ],   "id": "4033f43895a45b0f",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "markdown",   "source": "## Compare Simulations from Posterior Samples",   "id": "6ed9a874bf49cd3b"  },  {   "metadata": {},   "cell_type": "code",   "source": [    "%%time\n",    "file_name = os.path.join(gp, f'{results_path}/synthetic_posterior_simulations.pickle')\n",    "n_sim = 100\n",    "if os.path.exists(file_name):\n",    "    with open(file_name, 'rb') as f:\n",    "        posterior_simulations = pickle.load(f)\n",    "else:\n",    "    posterior_simulations = {}\n",    "    for name, ps in posterior_samples.items():\n",    "        print('Simulating', name)\n",    "        @delayed\n",    "        def wrapper_fun(sample_i):\n",    "            _sim_dict = {key: p for key, p in zip(obs_pars.keys(), ps[sample_i])}\n",    "            _posterior_sim = model(_sim_dict)\n",    "            return _posterior_sim['sim']\n",    "\n",    "        sim_list = Parallel(n_jobs=6, verbose=1)(wrapper_fun(i) for i in range(n_sim))\n",    "        posterior_simulations[name] = np.concatenate(sim_list)\n",    "\n",    "    with open(file_name, 'wb') as f:\n",    "       pickle.dump(posterior_simulations, f)\n",    "\n",    "simulation_sumstats = {}\n",    "for name, ps in posterior_simulations.items():\n",    "    print('Processing', name)\n",    "    simulation_sumstats[name] = [make_sumstat_dict(p_sim[np.newaxis]) for p_sim in ps]\n",    "    for i in range(len(simulation_sumstats[name])):\n",    "        simulation_sumstats[name][i]['nn'] = make_sumstat_dict_nn(ps[i][np.newaxis], use_npe_summaries=True)['summary_net']"   ],   "id": "903ac7db2e60002a",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "def obj_func_comparison(sim: dict, obs: dict, return_marginal: bool = False, weights: Union[dict, list] = None) -> Union[float, np.ndarray]:\n",    "    total = np.zeros(len(sim.keys()))\n",    "    for k_i, key in enumerate(sim):\n",    "        if key == 'nn':\n",    "            # for the neural network summary statistics we use the Euclidean distance\n",    "            total[k_i] = euclidean_distance(sim['nn'], obs['nn'])\n",    "            continue # no weights applied\n",    "        elif key == 'umap':\n",    "            # distance already computed\n",    "            total[k_i] = obs['umap']\n",    "            continue # no weights applied\n",    "        else:\n",    "            total[k_i] = distances[key](sim, obs)\n",    "        if weights is not None:\n",    "            if isinstance(weights, dict):\n",    "                total[k_i] = total[k_i] * weights[key]\n",    "            elif isinstance(weights, list):\n",    "                total[k_i] = total[k_i] * weights[k_i]\n",    "            else:\n",    "                raise ValueError('Weights must be a list or a dictionary')\n",    "    if return_marginal:\n",    "        return total\n",    "    return total.sum()"   ],   "id": "56c4846465fcc2ef",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "test_sim_dict = make_sumstat_dict(test_sim)\n",    "test_sim_dict['nn'] = make_sumstat_dict_nn(test_sim, use_npe_summaries=True)['summary_net']\n",    "test_sim_dict['umap'] = None  # will be computed later"   ],   "id": "1de2616c39309571",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "# compute the UMAP embedding and similarity matrices\n",    "\n",    "# 1) Stack & flatten as you already do\n",    "all_trajec = np.concatenate((\n",    "    test_sim,\n",    "    posterior_simulations['abc'],\n",    "    posterior_simulations['abc_mean'],\n",    "    posterior_simulations['abc_npe'],\n",    "    posterior_simulations['npe'],\n",    "), axis=0).reshape((4*n_sim+1)*143, 120, 3)\n",    "# flatten the 3 coords into a single feature-vector per trajectory\n",    "all_trajec = np.concatenate([\n",    "    all_trajec[..., 0], all_trajec[..., 1], all_trajec[..., 2]\n",    "], axis=1)\n",    "all_trajec[np.isnan(all_trajec)] = -1\n",    "\n",    "# how many samples per group?\n",    "n_real = test_sim[0].shape[0]\n",    "n_abc  = posterior_simulations['abc'].shape[1] * posterior_simulations['abc'].shape[0]\n",    "n_abc_mean  = posterior_simulations['abc_mean'].shape[1] * posterior_simulations['abc_mean'].shape[0]\n",    "n_abc_npe = posterior_simulations['abc_npe'].shape[1] * posterior_simulations['abc_npe'].shape[0]\n",    "n_npe  = posterior_simulations['npe'].shape[1] * posterior_simulations['npe'].shape[0]\n",    "\n",    "# 2) Compute UMAP embedding once\n",    "reducer   = umap.UMAP(random_state=0, n_components=10)\n",    "embedding = reducer.fit_transform(all_trajec)  # shape=(N,n_components)\n",    "\n",    "# 3) Compute full similarity matrices\n",    "sim_emb  = cosine_similarity(embedding)       # (N × N) in UMAP space\n",    "\n",    "# 4) For each method, pull similarities to the real set\n",    "#    sim_to_real_method = sim_norm[0:n_real, idx_start:idx_end]\n",    "ranges = {\n",    "    'abc':       slice(n_real,  n_real + n_abc),\n",    "    'abc_mean':  slice(n_real + n_abc, n_real + n_abc + n_abc_mean),\n",    "    'abc_npe':   slice(n_real + n_abc + n_abc_mean, n_real + n_abc + n_abc_mean + n_abc_npe),\n",    "    'npe':       slice(n_real + n_abc + n_abc_mean + n_abc_npe, None),\n",    "}\n",    "\n",    "data = {}\n",    "for name, sl in ranges.items():\n",    "    # all sim-vs-all real similarities\n",    "    data[name] = sim_emb[:n_real, sl]\n",    "\n",    "# append to summary stats\n",    "for method, sim in simulation_sumstats.items():\n",    "    for i, ss in enumerate(sim):\n",    "        simulation_sumstats[method][i]['umap'] = np.median(data[method][:, i*143:(i+1)*143])"   ],   "id": "c3dc7b59b55c0cfd",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "plot_sumstats_distance_stats(obj_func_comparison,\n",    "                             test_sim_dict,\n",    "                             [ps for ps in simulation_sumstats.values()],\n",    "                             labels=labels,\n",    "                             title='Wasserstein Distance',\n",    "                             colors=colors,\n",    "                             ylog_scale=True,\n",    "                             path=os.path.join(gp, f'{results_path}/synthetic_sumstats_comparison_{test_id}.pdf')\n",    "                             )\n",    "\n",    "print(*test_sim_dict.keys())\n",    "print(adaptive_weights)\n",    "plot_sumstats_distance_stats(partial(obj_func_comparison, weights=adaptive_weights),\n",    "                             test_sim_dict,\n",    "                             [ps for ps in simulation_sumstats.values()],\n",    "                             labels=labels,\n",    "                             colors=colors,\n",    "                             ylog_scale=True,\n",    "                             #path=os.path.join(gp, f'{results_path}/synthetic_sumstats_comparison.pdf')\n",    "                             )"   ],   "id": "646badb0b5c20dca",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "from itertools import combinations\n",    "from scipy.stats import mannwhitneyu\n",    "from statsmodels.stats.multitest import multipletests\n",    "from matplotlib.patches import Patch\n",    "\n",    "def plot_sumstats_distance_stats_test(obj_func_comparison: callable,\n",    "                                 test_sim_dict: dict,\n",    "                                 sumstats_list: list[list],\n",    "                                 labels: list[str] = None,\n",    "                                 colors: list[str] = None,\n",    "                                 ylog_scale: bool = False,\n",    "                                 title: str = 'Weighted\\nWasserstein Distance',\n",    "                                 path: str = None,\n",    "                                 fdr_q: float = 0.05,\n",    "                                 correct_across_all: bool = False,\n",    "                                 print_results: bool = True):\n",    "    \"\"\"\n",    "    Violin plots of distances with pairwise Mann Whitney U tests (FDR corrected)\n",    "    \"\"\"\n",    "    # compute marginal distances\n",    "    marginal_distances_list = []\n",    "    for sumstats in sumstats_list:\n",    "        md = np.zeros((len(sumstats), len(test_sim_dict)))\n",    "        for i, st in enumerate(sumstats):\n",    "            md[i, :] = obj_func_comparison(test_sim_dict, st, return_marginal=True)\n",    "        marginal_distances_list.append(md)\n",    "    marginal_distances_list = np.array(marginal_distances_list)  # (n_methods, n_sim, statistics)\n",    "    print(marginal_distances_list.shape)\n",    "\n",    "    n_stats   = len(test_sim_dict)\n",    "    n_methods = len(sumstats_list)\n",    "    name_plots = ['Displacement', 'Turning Angle', 'Velocity', 'Angle Degree', 'NPE Summary', 'UMAP']\n",    "    order = [0, 2, 3, 1, 4, 5]\n",    "\n",    "    # pairwise Mann Whitney tests\n",    "    method_pairs = list(combinations(range(n_methods), 2))\n",    "    raw_p = {g: [] for g in range(n_stats)}\n",    "    store = {g: [] for g in range(n_stats)}\n",    "\n",    "    for grp_idx in range(n_stats):\n",    "        group_data = marginal_distances_list[:, :, grp_idx]\n",    "        for (i, j) in method_pairs:\n",    "            x = np.asarray(group_data[i], float)\n",    "            y = np.asarray(group_data[j], float)\n",    "            x = x[np.isfinite(x)]\n",    "            y = y[np.isfinite(y)]\n",    "            if len(x) < 2 or len(y) < 2:\n",    "                U, p, r_rb = np.nan, np.nan, np.nan\n",    "            else:\n",    "                res = mannwhitneyu(x, y, alternative='two-sided', method='auto')\n",    "                U, p = res.statistic, res.pvalue\n",    "                r_rb = 1.0 - 2.0 * U / (len(x) * len(y))\n",    "            raw_p[grp_idx].append(p)\n",    "            store[grp_idx].append(((i, j), U, p, r_rb))\n",    "\n",    "    # FDR correction\n",    "    if correct_across_all:\n",    "        all_p = np.array([p for g in range(n_stats) for p in raw_p[g]], float)\n",    "        mask = np.isfinite(all_p)\n",    "        adj_all = np.full_like(all_p, np.nan)\n",    "        if mask.any():\n",    "            _, p_corr, _, _ = multipletests(all_p[mask], alpha=fdr_q, method='fdr_bh')\n",    "            adj_all[mask] = p_corr\n",    "        adj_p = {}\n",    "        idx = 0\n",    "        for g in range(n_stats):\n",    "            k = len(raw_p[g])\n",    "            adj_p[g] = list(adj_all[idx:idx+k])\n",    "            idx += k\n",    "    else:\n",    "        adj_p = {}\n",    "        for g in range(n_stats):\n",    "            pvec = np.array(raw_p[g], float)\n",    "            mask = np.isfinite(pvec)\n",    "            p_adj = np.full_like(pvec, np.nan)\n",    "            if mask.any():\n",    "                _, p_corr, _, _ = multipletests(pvec[mask], alpha=fdr_q, method='fdr_bh')\n",    "                p_adj[mask] = p_corr\n",    "            adj_p[g] = list(p_adj)\n",    "\n",    "    # optional printing\n",    "    if print_results:\n",    "        fam = \"all groups\" if correct_across_all else \"per group\"\n",    "        print(f\"Mann Whitney U tests with BH FDR at q={fdr_q} corrected {fam}\")\n",    "        for g in range(n_stats):\n",    "            print(f\"\\nGroup {g}  {name_plots[g]}\")\n",    "            for k, ((i, j), U, p, r_rb) in enumerate(store[g]):\n",    "                p_adj = adj_p[g][k]\n",    "                sig = \"yes\" if np.isfinite(p_adj) and p_adj < fdr_q else \"no\"\n",    "                print(f\"  {labels[i]} vs {labels[j]}  U={U:.3f}  p={p:.3e}  p_adj={p_adj:.3e}  r_rb={r_rb:.3f}  sig={sig}\")\n",    "\n",    "    # plotting\n",    "    fig, axes = plt.subplots(1, n_stats, figsize=(10, 2), layout=\"constrained\")\n",    "    if n_stats == 1:\n",    "        axes = [axes]\n",    "\n",    "    for ax, grp_idx in zip(axes, order):\n",    "        data = marginal_distances_list[:, :, grp_idx]\n",    "        cleaned = [np.asarray(g)[np.isfinite(g)] for g in data]\n",    "\n",    "        parts = ax.violinplot(cleaned, showmeans=False, showextrema=False)\n",    "        for pc, col in zip(parts['bodies'], colors):\n",    "            pc.set_facecolor(col)\n",    "            pc.set_edgecolor('black')\n",    "            pc.set_alpha(0.8)\n",    "\n",    "        # draw medians\n",    "        medians = [np.median(d) if len(d) else np.nan for d in cleaned]\n",    "        ax.scatter(np.arange(1, n_methods+1), medians, color='black', s=10, zorder=3)\n",    "\n",    "        ax.set_xlabel(name_plots[grp_idx], fontsize=12)\n",    "        ax.set_xticks(np.arange(1, n_methods+1))\n",    "        ax.set_xticklabels([], rotation=45, ha='right')\n",    "        if ylog_scale and grp_idx != 5:\n",    "            ax.set_yscale('log')\n",    "        if grp_idx < 4:\n",    "            ax.set_ylim(0.0001, 20)\n",    "            if grp_idx != 0:\n",    "                ax.set_yticks([])\n",    "        if grp_idx == 4:\n",    "            ax.set_ylim(0.1, 20)\n",    "\n",    "        # significance annotations\n",    "        y_max = max((np.max(c) for c in cleaned if c.size), default=np.nan)\n",    "        if not np.isfinite(y_max):\n",    "            continue\n",    "        gap = 0.08 * y_max if ax.get_yscale() == 'linear' else 1.2\n",    "        step = 0.06 * y_max if ax.get_yscale() == 'linear' else 1.2\n",    "        level = 0\n",    "        for k, ((i, j), U, p, r_rb) in enumerate(store[grp_idx]):\n",    "            p_adj = adj_p[grp_idx][k]\n",    "            if not np.isfinite(p_adj) or p_adj >= fdr_q:\n",    "                continue\n",    "            if p_adj < 0.001:\n",    "                stars = '***'\n",    "            elif p_adj < 0.01:\n",    "                stars = '**'\n",    "            else:\n",    "                stars = '*'\n",    "            x1, x2 = i+1, j+1\n",    "            y = y_max + gap + level * step\n",    "            ax.plot([x1, x1, x2, x2], [y, y+0.02*y, y+0.02*y, y], lw=1, c='black')\n",    "            ax.text((x1+x2)/2, y+0.02*y, stars, ha='center', va='bottom', fontsize=9)\n",    "            level += 1\n",    "\n",    "    axes[0].set_ylabel(title, fontsize=12)\n",    "    axes[-2].set_ylabel(r\"$L^1$ Distance\", fontsize=12)\n",    "    axes[-1].set_ylabel(\"Cosine Similarity\", fontsize=12)\n",    "    for a in axes:\n",    "        a.tick_params(axis=\"x\", labelsize=10)\n",    "        a.tick_params(axis=\"y\", labelsize=10)\n",    "\n",    "    handles = [Patch(facecolor=c, label=l) for c, l in zip(colors, labels)]\n",    "    fig.legend(handles=handles, loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.33), fontsize=12)\n",    "\n",    "    if path:\n",    "        plt.savefig(path, bbox_inches='tight')\n",    "    plt.show()"   ],   "id": "928fa5711cec8077",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "plot_sumstats_distance_stats_test(obj_func_comparison,\n",    "                             test_sim_dict,\n",    "                             [ps for ps in simulation_sumstats.values()],\n",    "                             labels=labels,\n",    "                             title='Wasserstein Distance',\n",    "                             colors=colors,\n",    "                             ylog_scale=True,\n",    "                             #path=os.path.join(gp, f'{results_path}/synthetic_sumstats_comparison_{test_id}.pdf')\n",    "                             )"   ],   "id": "d8f92cef59c6358d",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "from scipy.stats import mannwhitneyu\n",    "from itertools import combinations\n",    "\n",    "def rank_biserial(x, y, u_stat):\n",    "    \"\"\"\n",    "    Calculate rank-biserial correlation (effect size for Mann-Whitney U).\n",    "    r = 1 - (2U)/(n1*n2)\n",    "    Range: [-1, 1] where 0 = no effect, ±1 = complete separation\n",    "    \"\"\"\n",    "    n1, n2 = len(x), len(y)\n",    "    r = 1 - (2 * u_stat) / (n1 * n2)\n",    "    return r\n",    "\n",    "def interpret_effect_size(r):\n",    "    \"\"\"Interpret rank-biserial correlation.\"\"\"\n",    "    abs_r = abs(r)\n",    "    if abs_r < 0.1:\n",    "        return \"negligible\"\n",    "    elif abs_r < 0.3:\n",    "        return \"small\"\n",    "    elif abs_r < 0.5:\n",    "        return \"medium\"\n",    "    else:\n",    "        return \"large\"\n",    "\n",    "def plot_sumstats_distance_stats_test(obj_func_comparison: callable,\n",    "                                 test_sim_dict: dict,\n",    "                                 sumstats_list: list[list],\n",    "                                 labels: list[str] = None,\n",    "                                 colors: list[str] = None,\n",    "                                 ylog_scale: bool = False,\n",    "                                 title: str = 'Weighted\\nWasserstein Distance',\n",    "                                 path: str = None,\n",    "                                 alpha: float = 0.05,\n",    "                                 correction: str = 'bonferroni',\n",    "                                 min_effect_size: float = 0.3):\n",    "    \"\"\"\n",    "    Plot boxplots of the distance between each simulation method and the test simulation,\n",    "    separately for each summary-stat category. Tests whether the summary statistics\n",    "    themselves differ between methods.\n",    "\n",    "    Parameters\n",    "    ----------\n",    "    alpha : float\n",    "        Significance level for statistical tests (default: 0.05)\n",    "    correction : str\n",    "        Multiple comparison correction method: 'bonferroni' or 'none' (default: 'bonferroni')\n",    "    min_effect_size : float\n",    "        Minimum rank-biserial r to consider meaningful (default: 0.3 for medium effect)\n",    "    \"\"\"\n",    "    # 1) compute marginal distances for each summary-stat group\n",    "    marginal_distances_list = []\n",    "    for sumstats in sumstats_list:\n",    "        md = np.zeros((len(sumstats), len(test_sim_dict)))\n",    "        for i, st in enumerate(sumstats):\n",    "            md[i, :] = obj_func_comparison(test_sim_dict, st, return_marginal=True)\n",    "        marginal_distances_list.append(md)\n",    "    marginal_distances_list = np.array(marginal_distances_list)\n",    "\n",    "    n_stats = len(test_sim_dict)\n",    "    n_methods = len(sumstats_list)\n",    "    name_plots = ['Displacement', 'Turning Angle', 'Velocity', 'Angle Degree', 'NPE Summary', 'UMAP']\n",    "    order = [0, 2, 3, 1, 4, 5]\n",    "\n",    "    # 2) Extract raw summary statistics from each method for direct comparison\n",    "    print(\"\\n\" + \"=\"*110)\n",    "    print(\"TESTING SUMMARY STATISTICS DIRECTLY (Mann-Whitney U)\")\n",    "    print(\"=\"*110)\n",    "    print(\"Testing if methods produce statistically different summary statistics (not distances)\")\n",    "\n",    "    n_comparisons = len(list(combinations(range(n_methods), 2)))\n",    "    alpha_corrected = alpha / n_comparisons if correction == 'bonferroni' else alpha\n",    "\n",    "    if correction == 'bonferroni':\n",    "        print(f\"Significance: α = {alpha} (Bonferroni: α = {alpha_corrected:.6f}) | \"\n",    "              f\"Min meaningful effect: r = {min_effect_size}\")\n",    "    else:\n",    "        print(f\"Significance: α = {alpha} (no correction) | \"\n",    "              f\"Min meaningful effect: r = {min_effect_size}\")\n",    "    print(f\"Comparisons per statistic: {n_comparisons}\\n\")\n",    "\n",    "    # Store results\n",    "    test_results = {}\n",    "    summary_stats = {}\n",    "\n",    "    for grp_idx in order:\n",    "        stat_name = name_plots[grp_idx]\n",    "        print(f\"\\n{stat_name}\")\n",    "        print(\"-\" * 110)\n",    "        print(f\"{'Comparison':<50} {'p-value':>10} {'Sig':>5} {'r (effect)':>12} {'Interpret':>12} \"\n",    "              f\"{'Median A':>10} {'Median B':>10}\")\n",    "        print(\"-\" * 110)\n",    "\n",    "        # Get raw summary statistics for this category from each method\n",    "        # Each method has shape (n_simulations, n_stats_in_category)\n",    "        # We want to test if the distributions of these statistics differ\n",    "        method_summaries = []\n",    "        for method_idx in range(n_methods):\n",    "            # Extract all summary statistics of this type across all simulations\n",    "            stats_values = []\n",    "            for sim_idx, sim_stats in enumerate(sumstats_list[method_idx]):\n",    "                # sim_stats contains all summary statistics for one simulation\n",    "                # We need to extract the ones corresponding to grp_idx\n",    "                if isinstance(sim_stats, (list, np.ndarray)):\n",    "                    stats_values.append(sim_stats[grp_idx])\n",    "            method_summaries.append(np.concatenate(stats_values) if stats_values else np.array([]))\n",    "\n",    "        # Perform pairwise comparisons on the summary statistics themselves\n",    "        sig_pairs = []\n",    "        meaningful_pairs = []\n",    "\n",    "        for i, j in combinations(range(n_methods), 2):\n",    "            stats_i = method_summaries[i]\n",    "            stats_j = method_summaries[j]\n",    "\n",    "            # Remove inf/nan\n",    "            stats_i = stats_i[np.isfinite(stats_i)]\n",    "            stats_j = stats_j[np.isfinite(stats_j)]\n",    "\n",    "            if len(stats_i) == 0 or len(stats_j) == 0:\n",    "                continue\n",    "\n",    "            u_stat, pval = mannwhitneyu(stats_i, stats_j, alternative='two-sided')\n",    "\n",    "            median_i = np.median(stats_i)\n",    "            median_j = np.median(stats_j)\n",    "\n",    "            # Effect size\n",    "            r = rank_biserial(stats_i, stats_j, u_stat)\n",    "            effect_interp = interpret_effect_size(r)\n",    "\n",    "            is_sig = pval < alpha_corrected\n",    "            is_meaningful = abs(r) >= min_effect_size\n",    "\n",    "            sig_marker = \"***\" if is_sig else \"\"\n",    "            effect_marker = \"⚠\" if is_sig and not is_meaningful else \"\"\n",    "\n",    "            comparison = f\"{labels[i]} vs {labels[j]}\"\n",    "            print(f\"{comparison:<50} {pval:>10.6f} {sig_marker:>5} {r:>12.3f} {effect_interp:>12} \"\n",    "                  f\"{median_i:>10.4f} {median_j:>10.4f} {effect_marker}\")\n",    "\n",    "            if is_sig:\n",    "                sig_pairs.append((i+1, j+1, pval, abs(r)))\n",    "                if is_meaningful:\n",    "                    meaningful_pairs.append((i+1, j+1, pval, abs(r)))\n",    "\n",    "        test_results[grp_idx] = meaningful_pairs\n",    "        summary_stats[grp_idx] = {\n",    "            'total_significant': len(sig_pairs),\n",    "            'meaningful': len(meaningful_pairs),\n",    "            'small_effect_but_sig': len(sig_pairs) - len(meaningful_pairs)\n",    "        }\n",    "\n",    "        print(f\"\\nSummary: {len(sig_pairs)}/{n_comparisons} significant, \"\n",    "              f\"{len(meaningful_pairs)} with meaningful effect size (|r| ≥ {min_effect_size})\")\n",    "        if len(sig_pairs) - len(meaningful_pairs) > 0:\n",    "            print(f\"         ⚠ {len(sig_pairs) - len(meaningful_pairs)} comparisons are \"\n",    "                  f\"statistically significant but have small practical effect\")\n",    "\n",    "    print(\"\\n\" + \"=\"*110)\n",    "    print(\"INTERPRETATION GUIDE:\")\n",    "    print(\"  *** = Statistically significant after Bonferroni correction\")\n",    "    print(\"  ⚠   = Statistically significant but small effect size (may not be practically meaningful)\")\n",    "    print(\"  r   = Rank-biserial correlation (effect size for Mann-Whitney U)\")\n",    "    print(\"        |r| < 0.1 (negligible), 0.1-0.3 (small), 0.3-0.5 (medium), > 0.5 (large)\")\n",    "    print(\"=\"*110 + \"\\n\")\n",    "\n",    "    # 3) Create visualization based on distances (as before)\n",    "    fig, axes = plt.subplots(1, n_stats, figsize=(12, 2.5), layout=\"constrained\")\n",    "    if n_stats == 1:\n",    "        axes = [axes]\n",    "\n",    "    for ax, grp_idx in zip(axes, order):\n",    "        data = marginal_distances_list[:, :, grp_idx]\n",    "        cleaned_data = [np.array(group)[~np.isinf(group)] for group in data]\n",    "\n",    "        b = ax.boxplot(cleaned_data, patch_artist=True)\n",    "\n",    "        for patch, col in zip(b['boxes'], colors):\n",    "            patch.set_facecolor(col)\n",    "        for median in b['medians']:\n",    "            median.set_color('black')\n",    "            median.set_linewidth(1.5)\n",    "\n",    "        # Annotate only meaningful differences in summary statistics\n",    "        meaningful_pairs = test_results[grp_idx]\n",    "\n",    "        if meaningful_pairs:\n",    "            y_max = max([np.percentile(d, 95) for d in cleaned_data if len(d) > 0])\n",    "            y_min = min([np.percentile(d, 5) for d in cleaned_data if len(d) > 0])\n",    "            y_range = y_max - y_min\n",    "\n",    "            # Show top 3 most meaningful (largest effect size)\n",    "            top_pairs = sorted(meaningful_pairs, key=lambda x: x[3], reverse=True)[:3]\n",    "\n",    "            for idx, (i, j, pval, effect) in enumerate(top_pairs):\n",    "                y = y_max + y_range * 0.15 * (idx + 1)\n",    "                ax.plot([i, i, j, j], [y, y*1.01, y*1.01, y], 'k-', lw=0.8)\n",    "                ax.text((i+j)/2, y*1.02, '***', ha='center', va='bottom', fontsize=8, weight='bold')\n",    "\n",    "        ax.set_xlabel(name_plots[grp_idx], fontsize=11, weight='bold')\n",    "        ax.set_xticks(np.arange(1, n_methods+1))\n",    "        ax.set_xticklabels([], rotation=45, ha='right')\n",    "\n",    "        # Add count of meaningful differences\n",    "        n_meaningful = summary_stats[grp_idx]['meaningful']\n",    "        ax.text(0.98, 0.98, f'n={n_meaningful}', transform=ax.transAxes,\n",    "                ha='right', va='top', fontsize=8, bbox=dict(boxstyle='round',\n",    "                facecolor='wheat', alpha=0.3))\n",    "\n",    "        if ylog_scale and grp_idx != 5:\n",    "            ax.set_yscale('log')\n",    "        if grp_idx < 4:\n",    "            ax.set_ylim(0.0001, 20)\n",    "            if grp_idx != 0:\n",    "                ax.set_yticks([])\n",    "        if grp_idx == 4:\n",    "            ax.set_ylim(0.1, 20)\n",    "\n",    "    axes[0].set_ylabel(title, fontsize=11)\n",    "    axes[-2].set_ylabel(r\"$L^1$ Distance\", fontsize=11)\n",    "    axes[-1].set_ylabel(\"Cosine Similarity\", fontsize=11)\n",    "\n",    "    for a in axes:\n",    "        a.tick_params(axis=\"x\", labelsize=9)\n",    "        a.tick_params(axis=\"y\", labelsize=9)\n",    "\n",    "    # Legend\n",    "    handles = [Patch(facecolor=c, label=l) for c, l in zip(colors, labels)]\n",    "    fig.legend(handles=handles, loc='lower center', ncol=2,\n",    "               bbox_to_anchor=(0.5, -0.35), fontsize=10)\n",    "\n",    "    if path:\n",    "        plt.savefig(path, bbox_inches='tight', dpi=300)\n",    "    plt.show()\n",    "\n",    "    return"   ],   "id": "28094aff8c45802e",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "plot_sumstats_distance_stats_test(obj_func_comparison,\n",    "                             test_sim_dict,\n",    "                             [ps for ps in simulation_sumstats.values()],\n",    "                             labels=labels,\n",    "                             title='Wasserstein Distance',\n",    "                             colors=colors,\n",    "                             ylog_scale=True,\n",    "                             #path=os.path.join(gp, f'{results_path}/synthetic_sumstats_comparison_{test_id}.pdf')\n",    "                             )"   ],   "id": "7e5838c4c63b37c3",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "from itertools import combinations\n",    "from scipy.stats import mannwhitneyu\n",    "from statsmodels.stats.multitest import multipletests\n",    "def plot_sumstats_distance_stats(obj_func_comparison: callable,\n",    "                                 test_sim_dict: dict,\n",    "                                 sumstats_list: list[list[dict]],\n",    "                                 labels: list[str] = None,\n",    "                                 colors: list[str] = None,\n",    "                                 ylog_scale: bool = False,\n",    "                                 title: str = 'Weighted\\nWasserstein Distance',\n",    "                                 path: str = None):\n",    "    \"\"\"\n",    "    Plot boxplots of the distance between each simulation method and the test simulation,\n",    "    separately for each summary‐stat category.\n",    "    \"\"\"\n",    "    # 1) compute marginal distances for each summary‐stat group\n",    "    marginal_distances_list = []\n",    "    for sumstats in sumstats_list:\n",    "        # shape = (n_methods, n_sim, n_stats_in_group)\n",    "        md = np.zeros((len(sumstats), len(test_sim_dict)))\n",    "        for i, st in enumerate(sumstats):\n",    "            md[i, :] = obj_func_comparison(test_sim_dict, st, return_marginal=True)\n",    "        marginal_distances_list.append(md)\n",    "    marginal_distances_list = np.array(marginal_distances_list)  # (n_methods, n_sim, statistics)\n",    "\n",    "    stats_list = []\n",    "    for method in sumstats_list:\n",    "        stat_method = {k: [] for k in method[0].keys()}\n",    "        for sim in method:\n",    "            for k in method[0].keys():\n",    "                if isinstance(sim[k], np.ndarray):\n",    "                    stat_method[k] += list(sim[k])\n",    "                else:\n",    "                    stat_method[k] += [sim[k]]\n",    "        for k in method[0].keys():\n",    "            stat_method[k] = np.array(stat_method[k])\n",    "            # only keep finite values\n",    "            stat_method[k] = stat_method[k][np.isfinite(stat_method[k])]\n",    "        stats_list.append(stat_method)\n",    "\n",    "    n_stats = len(test_sim_dict)\n",    "    n_methods  = len(sumstats_list)\n",    "    name_plots = ['Displacement', 'Turning Angle', 'Velocity', 'Angle Degree', 'NPE Summary', 'UMAP']\n",    "    order     = [0, 2, 3, 1, 4, 5]  # reorder\n",    "\n",    "    # 2) make one subplot per summary‐stat category\n",    "    fig, axes = plt.subplots(1, n_stats, figsize=(10, 2), layout=\"constrained\")\n",    "    if n_stats == 1:\n",    "        axes = [axes]\n",    "\n",    "    for ax, grp_idx in zip(axes, order):\n",    "        data = marginal_distances_list[:, :, grp_idx]   # (n_methods, n_sim, n_stats)\n",    "        cleaned_data = [np.array(group)[~np.isinf(group)] for group in data]\n",    "        # boxplot: one array per method\n",    "        b = ax.boxplot(cleaned_data, patch_artist=True)\n",    "\n",    "        # color each box\n",    "        for patch, col in zip(b['boxes'], colors):\n",    "            patch.set_facecolor(col)\n",    "        for median in b['medians']:\n",    "            median.set_color('black')\n",    "\n",    "        ax.set_xlabel(name_plots[grp_idx], fontsize=12)\n",    "        ax.set_xticks(np.arange(1, n_methods+1))\n",    "        ax.set_xticklabels([], rotation=45, ha='right')\n",    "        if ylog_scale and not grp_idx == 5:\n",    "            ax.set_yscale('log')\n",    "        if grp_idx < 4:\n",    "            ax.set_ylim(0.0001, 20)\n",    "            if grp_idx != 0:\n",    "                ax.set_yticks([])\n",    "        if grp_idx == 4:\n",    "            ax.set_ylim(0.1, 20)\n",    "\n",    "    axes[0].set_ylabel(title, fontsize=12)\n",    "    axes[-2].set_ylabel(r\"$L^1$ Distance\", fontsize=12)\n",    "    axes[-1].set_ylabel(\"Cosine Similarity\", fontsize=12)\n",    "    for a in axes:\n",    "        a.tick_params(axis=\"x\", labelsize=10)\n",    "        a.tick_params(axis=\"y\", labelsize=10)\n",    "\n",    "    # 3) shared legend\n",    "    handles = [Patch(facecolor=c, label=l) for c, l in zip(colors, labels)]\n",    "    fig.legend(handles=handles, loc='lower center', ncol=2, bbox_to_anchor=(0.5, -0.33), fontsize=12)\n",    "\n",    "    # 4) save & show\n",    "    if path:\n",    "        plt.savefig(path, bbox_inches='tight')\n",    "    plt.show()\n",    "\n",    "    # 5) Statistical tests\n",    "    alpha = 0.05\n",    "    method_names = labels if labels is not None else [f\"Method {i}\" for i in range(len(sumstats_list))]\n",    "\n",    "    for grp_idx, grp_name in enumerate(name_plots):\n",    "        # extract data for this summary‐stat group\n",    "        data = marginal_distances_list[:, :, grp_idx]\n",    "        cleaned_data = [np.array(group)[~np.isinf(group)] for group in data]\n",    "\n",    "        # all pairwise combinations\n",    "        pairs = list(combinations(range(len(cleaned_data)), 2))\n",    "        pvals = []\n",    "        for (i, j) in pairs:\n",    "            _, p = mannwhitneyu(cleaned_data[i], cleaned_data[j], alternative='two-sided')\n",    "            pvals.append(p)\n",    "\n",    "        # FDR correction\n",    "        reject, pvals_corr, _, _ = multipletests(pvals, alpha=alpha, method='fdr_bh')\n",    "\n",    "        print(f\"\\n--- {grp_name} ---\")\n",    "        for (i, j), p, pc, r in zip(pairs, pvals, pvals_corr, reject):\n",    "            signif = \"Significant\" if r else \"NS\"\n",    "            print(f\"{method_names[i]} vs {method_names[j]}: raw p={p:.3e}, corrected p={pc:.3e}, {signif}\")\n",    "\n",    "    # 5) Pairwise Mann Whitney tests on raw summary statistics\n",    "    alpha = 0.05\n",    "    method_names = labels if labels is not None else [f\"Method {i+1}\" for i in range(n_methods)]\n",    "    stat_keys = list(stats_list[0].keys())\n",    "\n",    "    for key in stat_keys:\n",    "        # data[i] holds all values of stat `key` for method i\n",    "        data = [stats_list[i][key] for i in range(n_methods)]\n",    "\n",    "        # all method pairs\n",    "        pairs = list(combinations(range(n_methods), 2))\n",    "        pvals = []\n",    "        for i, j in pairs:\n",    "            if len(data[i]) == 0 or len(data[j]) == 0:\n",    "                p = np.nan\n",    "            else:\n",    "                _, p = mannwhitneyu(data[i], data[j], alternative=\"two-sided\")\n",    "            pvals.append(p)\n",    "\n",    "        # multiple testing correction within this statistic\n",    "        pvals = np.array(pvals, dtype=float)\n",    "        mask = ~np.isnan(pvals)\n",    "        corr_p = np.full_like(pvals, np.nan, dtype=float)\n",    "        reject = np.zeros_like(pvals, dtype=bool)\n",    "        if mask.any():\n",    "            rej, pc, _, _ = multipletests(pvals[mask], alpha=alpha, method=\"fdr_bh\")\n",    "            corr_p[mask] = pc\n",    "            reject[mask] = rej\n",    "\n",    "        print(f\"\\n=== {key} ===\")\n",    "        for (i, j), p, pc, r in zip(pairs, pvals, corr_p, reject):\n",    "            signif = \"Significant\" if r else \"NS\"\n",    "            p_str = \"nan\" if np.isnan(p) else f\"{p:.3e}\"\n",    "            pc_str = \"nan\" if np.isnan(pc) else f\"{pc:.3e}\"\n",    "            print(f\"{method_names[i]} vs {method_names[j]}: raw p={p_str}, corrected p={pc_str}, {signif}\")\n",    "\n",    "    print(\"\\nMann Whitney U tests completed with FDR correction per statistic.\")\n"   ],   "id": "86aa321cdd8fbe17",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": [    "plot_sumstats_distance_stats(obj_func_comparison,\n",    "                             test_sim_dict,\n",    "                             [ps for ps in simulation_sumstats.values()],\n",    "                             labels=labels,\n",    "                             title='Wasserstein Distance',\n",    "                             colors=colors,\n",    "                             ylog_scale=True,\n",    "                             #path=os.path.join(gp, f'{results_path}/synthetic_sumstats_comparison_{test_id}.pdf')\n",    "                             )"   ],   "id": "ab5fb0d97eee26e9",   "outputs": [],   "execution_count": null  },  {   "metadata": {},   "cell_type": "code",   "source": "",   "id": "3e9251eae37e10d2",   "outputs": [],   "execution_count": null  } ], "metadata": {  "kernelspec": {   "display_name": "Python 3",   "language": "python",   "name": "python3"  },  "language_info": {   "codemirror_mode": {    "name": "ipython",    "version": 2   },   "file_extension": ".py",   "mimetype": "text/x-python",   "name": "python",   "nbconvert_exporter": "python",   "pygments_lexer": "ipython2",   "version": "2.7.6"  } }, "nbformat": 4, "nbformat_minor": 5}